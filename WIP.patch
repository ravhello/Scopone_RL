diff --git a/GUIDA.md b/GUIDA.md
index a2ab44d83568d978aa7c6560ab03643742763f09..899620b22c6fde78af15abe2bb1e4baeb0d28001 100644
--- a/GUIDA.md
+++ b/GUIDA.md
@@ -173,6 +173,7 @@ Opzioni CLI e significato:
 - Osservazione compatta: usare `--compact` e regolare `--k-history` per bilanciare informatività e costo computazionale. Valori tipici: 12–39.
 - Entropia: il coefficiente di entropia segue uno schedule (`linear` o `cosine`) per facilitare esplorazione iniziale e stabilizzazione successiva.
 - Belief + IS-MCTS: aumentare `--belief-particles`, `--dets` e `--sims` migliora la qualità della ricerca ma scala i tempi.
+- Gerarchia belief (livelli 1-3): abilita `OBS_INCLUDE_INFERRED_L2`/`OBS_INCLUDE_INFERRED_L3` per avere le feature dei livelli extra in osservazione; MCTS usa automaticamente il blend controllato da `SCOPONE_BELIEF_BLEND_ALPHA` (default 0.65).
 - Riproducibilità: impostare sempre `--seed`. Se si passa un seed negativo, il codice genera un seed casuale non-negativo e lo stampa. I checkpoint includono un `run_config` minimale.
 - Dipendenze: la repo usa `gymnasium` (non `gym`).
 - Dispositivi/AMP: su CUDA, il trainer usa AMP con GradScaler (API unificata); su CPU la mixed precision è disabilitata.
@@ -197,4 +198,4 @@ python /home/rikyravi/Scopone_RL/tools/profile_ppo.py --torch-profiler --iters 1
 Analizza i profiling del mio codice e dimmi dove viene speso più tempo. Poi ottimizza il mio codice in modo da risolvere il problema partendo dalle cose che perdono più tempo. Non usare fallback per gli errori (piuttosto causano raise, ma meglio che fallback).
 
 Attivazione server:
-gcloud compute config-ssh
\ No newline at end of file
+gcloud compute config-ssh
diff --git a/algorithms/ppo_ac.py b/algorithms/ppo_ac.py
index 9759323428c8970af0a85b9c2ef5108b52cc24c1..5e1301c3b67f1c5eb1b4b3ae0761dcb7d036e61b 100644
--- a/algorithms/ppo_ac.py
+++ b/algorithms/ppo_ac.py
@@ -22,7 +22,7 @@ class ActionConditionedPPO:
     (passando solo le azioni legali). Non è CTDE completo, ma prepara la struttura.
     """
     def __init__(self,
-                 obs_dim: int = 10823,
+                 obs_dim: Optional[int] = None,
                  action_dim: int = 80,
                  lr: float = 3e-4,
                  clip_ratio: float = 0.2,
@@ -33,7 +33,9 @@ class ActionConditionedPPO:
                  k_history: int = None):
         shared_enc = StateEncoderCompact(k_history=k_history)
         self.actor = ActionConditionedActor(obs_dim, action_dim, state_encoder=shared_enc)
-        self.critic = CentralValueNet(obs_dim, state_encoder=shared_enc)
+        obs_dim_val = self.actor.obs_dim
+        self.obs_dim = obs_dim_val
+        self.critic = CentralValueNet(obs_dim_val, state_encoder=shared_enc)
 
         # Training device override: keep models on CPU for env/collection, move to GPU only in update if requested
         train_dev_str = _os.environ.get('SCOPONE_TRAIN_DEVICE', 'cpu').strip().lower()
@@ -48,7 +50,7 @@ class ActionConditionedPPO:
         # Warm-up forward to materialize any Lazy modules (e.g., LazyLinear) only when CUDA
         if device.type == 'cuda':
             with torch.enable_grad():
-                _obs_w = torch.zeros((2, obs_dim), dtype=torch.float32, device=device, requires_grad=True)
+                _obs_w = torch.zeros((2, obs_dim_val), dtype=torch.float32, device=device, requires_grad=True)
                 _seat_w = torch.zeros((2, 6), dtype=torch.float32, device=device)
                 _seat_w[0, 0] = 1.0; _seat_w[0, 4] = 1.0
                 _seat_w[1, 1] = 1.0; _seat_w[1, 5] = 1.0
diff --git a/benchmark.py b/benchmark.py
index 7e67ddfcd42dbed35de60909994dc6f1c42d0633..c985b8b2594da67a2128c1d81cbf62d96177492c 100644
--- a/benchmark.py
+++ b/benchmark.py
@@ -17,6 +17,7 @@ os.environ.setdefault('SCOPONE_TORCH_COMPILE', '0')
 os.environ.setdefault('SCOPONE_TORCH_COMPILE_MODE', 'max-autotune')
 os.environ.setdefault('SCOPONE_TORCH_COMPILE_BACKEND', 'inductor')
 os.environ.setdefault('SCOPONE_INDUCTOR_AUTOTUNE', '1')
+os.environ.setdefault('SCOPONE_BELIEF_BLEND_ALPHA', '0.65')
 import argparse
 import time
 import re
@@ -32,6 +33,7 @@ import openpyxl
 
 # Import the required components from the existing code
 from environment import ScoponeEnvMA
+from belief import sample_determinization
 from algorithms.is_mcts import run_is_mcts
 from models.action_conditioned import ActionConditionedActor, CentralValueNet
 from utils.compile import maybe_compile_module
@@ -152,52 +154,9 @@ def play_game(actor1, actor2, starting_player=0, use_mcts=False, sims=128, dets=
                     return critic(o_t.unsqueeze(0), s.unsqueeze(0)).item()
             # Belief sampler neurale: determinizza le mani avversarie dai margini del BeliefNet
             def belief_sampler_neural(_env):
-                cp = _env.current_player
-                obs_cur = _env._get_observation(cp)
-                o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
-                if torch.is_tensor(o_cpu):
-                    o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
-                # seat/team vec: 6-dim
-                s_cpu = torch.zeros(6, dtype=torch.float32)
-                s_cpu[cp] = 1.0
-                s_cpu[4] = 1.0 if cp in [0, 2] else 0.0
-                s_cpu[5] = 1.0 if cp in [1, 3] else 0.0
-                if device.type == 'cuda':
-                    o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                    s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                else:
-                    o_t = o_cpu.unsqueeze(0).to(device=device)
-                    s_t = s_cpu.unsqueeze(0).to(device=device)
-                with torch.no_grad():
-                    state_feat = actor.state_enc(o_t, s_t)
-                    logits = actor.belief_net(state_feat)
-                    hand_table = o_t[:, :83]
-                    hand_mask = hand_table[:, :40] > 0.5
-                    table_mask = hand_table[:, 43:83] > 0.5
-                    captured = o_t[:, 83:165]
-                    cap0_mask = captured[:, :40] > 0.5
-                    cap1_mask = captured[:, 40:80] > 0.5
-                    visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
-                    probs_flat = actor.belief_net.probs(logits, visible_mask)
-                probs = probs_flat.view(3, 40).detach().cpu().numpy()
-                vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
-                unknown_ids = [cid for cid in range(40) if not vis[cid]]
-                others = [(cp + 1) % 4, (cp + 2) % 4, (cp + 3) % 4]
-                det = {pid: [] for pid in others}
-                counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
-                caps = [int(counts.get(pid, 0)) for pid in others]
-                n = len(unknown_ids)
-                if sum(caps) != n:
-                    caps[2] = max(0, n - caps[0] - caps[1])
-                for cid in unknown_ids:
-                    pc = probs[:, cid]
-                    s = pc.sum()
-                    ps = pc / (s if s > 0 else 1e-9)
-                    j = int(torch.argmax(torch.tensor(ps)).item())
-                    if caps[j] > 0:
-                        det[others[j]].append(cid)
-                        caps[j] -= 1
-                return det
+                alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
+                noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
+                return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
             action = run_is_mcts(env, policy_fn, value_fn, num_simulations=sims, c_puct=1.0, belief=None, num_determinization=dets,
                                     belief_sampler=belief_sampler_neural)
         else:
@@ -286,51 +245,9 @@ def main_cli():
                     with torch.no_grad():
                         return critic(o_t.unsqueeze(0), s.unsqueeze(0)).item()
                 def belief_sampler_neural(_env):
-                    cp = _env.current_player
-                    obs_cur = _env._get_observation(cp)
-                    o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
-                    if torch.is_tensor(o_cpu):
-                        o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
-                    s_cpu = torch.zeros(6, dtype=torch.float32)
-                    s_cpu[cp] = 1.0
-                    s_cpu[4] = 1.0 if cp in [0, 2] else 0.0
-                    s_cpu[5] = 1.0 if cp in [1, 3] else 0.0
-                    if device.type == 'cuda':
-                        o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                        s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                    else:
-                        o_t = o_cpu.unsqueeze(0).to(device=device)
-                        s_t = s_cpu.unsqueeze(0).to(device=device)
-                    with torch.no_grad():
-                        state_feat = actor.state_enc(o_t, s_t)
-                        logits = actor.belief_net(state_feat)
-                        hand_table = o_t[:, :83]
-                        hand_mask = hand_table[:, :40] > 0.5
-                        table_mask = hand_table[:, 43:83] > 0.5
-                        captured = o_t[:, 83:165]
-                        cap0_mask = captured[:, :40] > 0.5
-                        cap1_mask = captured[:, 40:80] > 0.5
-                        visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
-                        probs_flat = actor.belief_net.probs(logits, visible_mask)
-                    probs = probs_flat.view(3, 40).detach().cpu().numpy()
-                    vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
-                    unknown_ids = [cid for cid in range(40) if not vis[cid]]
-                    others = [(cp + 1) % 4, (cp + 2) % 4, (cp + 3) % 4]
-                    det = {pid: [] for pid in others}
-                    counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
-                    caps = [int(counts.get(pid, 0)) for pid in others]
-                    n = len(unknown_ids)
-                    if sum(caps) != n:
-                        caps[2] = max(0, n - caps[0] - caps[1])
-                    for cid in unknown_ids:
-                        pc = probs[:, cid]
-                        s = pc.sum()
-                        ps = pc / (s if s > 0 else 1e-9)
-                        j = int(torch.argmax(torch.tensor(ps)).item())
-                        if caps[j] > 0:
-                            det[others[j]].append(cid)
-                            caps[j] -= 1
-                    return det
+                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
+                    noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
+                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
                 action = run_is_mcts(env, policy_fn, value_fn, num_simulations=args.sims, c_puct=1.0, belief=None, num_determinization=args.dets,
                                       belief_sampler=belief_sampler_neural)
             else:
@@ -931,4 +848,4 @@ def main():
     print(f"Excel comparison report saved to {excel_file}")
 
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
diff --git a/environment.py b/environment.py
index 459609e59f77150a2c4f7f18ddab78316a2f0845..0202b881e29541bf82ea48eb04b136f53bcf2fe7 100644
--- a/environment.py
+++ b/environment.py
@@ -12,9 +12,12 @@ from observation import (
     encode_state_compact_for_player_fast as _encode_state_compact_for_player_fast,
     RANK_OF_ID,
     OBS_INCLUDE_INFERRED,
+    OBS_INCLUDE_INFERRED_L2,
+    OBS_INCLUDE_INFERRED_L3,
     OBS_INCLUDE_RANK_PROBS,
     OBS_INCLUDE_SCOPA_PROBS,
     OBS_INCLUDE_DEALER,
+    get_compact_obs_dim,
 )
 from utils.compile import maybe_compile_function
 import torch.nn.functional as F
@@ -54,14 +57,7 @@ class ScoponeEnvMA(gym.Env):
         #            + 120 (inferred) + 8 (primiera) + 2 (denari) + 1 (settebello) + 2 (score)
         #            + 1 (table sum) + 10 (table possible sums) + 2 (scopa counts)
         #            + 30 (rank_presence_from_inferred) + 1 (progress) + 2 (last capturing team)
-        include_rank = bool(OBS_INCLUDE_RANK_PROBS)
-        include_scopa = bool(OBS_INCLUDE_SCOPA_PROBS)
-        include_inferred = bool(OBS_INCLUDE_INFERRED)
-        include_dealer = bool(OBS_INCLUDE_DEALER)
-        # Parti fisse comuni (senza inferred): 43+40+82 + 61*k + 40 + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
-        fixed = 43 + 40 + 82 + 61 * self.k_history + 40 + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
-        base = fixed + (120 if include_inferred else 0)
-        obs_dim = base + (10 if include_scopa else 0) + (150 if include_rank else 0) + (4 if include_dealer else 0)
+        obs_dim = get_compact_obs_dim(self.k_history)
         # Observation space con la rappresentazione selezionata
         self.observation_space = spaces.Box(low=0, high=1, shape=(obs_dim,))
         
@@ -129,6 +125,8 @@ class ScoponeEnvMA(gym.Env):
         self._consec_scopa_team_t = torch.zeros(2, dtype=torch.int32, device=device)
         # Optional mirrors for probabilistic features (populated eagerly when enabled)
         self._inferred_probs_t = torch.zeros(120, dtype=torch.float32, device=device)
+        self._inferred_probs_l2_t = torch.zeros(120, dtype=torch.float32, device=device)
+        self._inferred_probs_l3_t = torch.zeros(120, dtype=torch.float32, device=device)
         self._rank_probs_by_player_t = torch.zeros(150, dtype=torch.float32, device=device)
         self._rank_presence_from_inferred_t = torch.zeros(30, dtype=torch.float32, device=device)
         self._scopa_probs_t = torch.zeros(10, dtype=torch.float32, device=device)
@@ -194,6 +192,8 @@ class ScoponeEnvMA(gym.Env):
         self.game_state['_scopa_counts_t'] = self._scopa_counts_t
         self.game_state['_consec_scopa_team_t'] = self._consec_scopa_team_t
         self.game_state['_inferred_probs_t'] = self._inferred_probs_t
+        self.game_state['_inferred_probs_l2_t'] = self._inferred_probs_l2_t
+        self.game_state['_inferred_probs_l3_t'] = self._inferred_probs_l3_t
         self.game_state['_rank_probs_by_player_t'] = self._rank_probs_by_player_t
         self.game_state['_rank_presence_from_inferred_t'] = self._rank_presence_from_inferred_t
         self.game_state['_scopa_probs_t'] = self._scopa_probs_t
@@ -227,6 +227,8 @@ class ScoponeEnvMA(gym.Env):
             '_scopa_counts_t',
             '_consec_scopa_team_t',
             '_inferred_probs_t',
+            '_inferred_probs_l2_t',
+            '_inferred_probs_l3_t',
             '_rank_probs_by_player_t',
             '_rank_presence_from_inferred_t',
             '_scopa_probs_t',
@@ -800,11 +802,24 @@ class ScoponeEnvMA(gym.Env):
             self.game_state['_played_bits_by_player_t'] = self._played_bits_by_player_t
             self.game_state['_scopa_counts_t'] = self._scopa_counts_t
             # probabilistic feature mirrors (if OBS_* enabled, compute eagerly once here)
-            if bool(OBS_INCLUDE_INFERRED):
-                from observation import compute_inferred_probabilities
-                ip = compute_inferred_probabilities(self.game_state, player_id)
-                ip_t = ip if torch.is_tensor(ip) else torch.as_tensor(ip, dtype=torch.float32, device=device)
-                self._inferred_probs_t.copy_(ip_t.to(dtype=torch.float32, device=device))
+            if bool(OBS_INCLUDE_INFERRED) or bool(OBS_INCLUDE_INFERRED_L2) or bool(OBS_INCLUDE_INFERRED_L3):
+                from observation import (
+                    compute_inferred_probabilities,
+                    compute_inferred_probabilities_level2,
+                    compute_inferred_probabilities_level3,
+                )
+                if bool(OBS_INCLUDE_INFERRED):
+                    ip = compute_inferred_probabilities(self.game_state, player_id)
+                    ip_t = ip if torch.is_tensor(ip) else torch.as_tensor(ip, dtype=torch.float32, device=device)
+                    self._inferred_probs_t.copy_(ip_t.to(dtype=torch.float32, device=device))
+                if bool(OBS_INCLUDE_INFERRED_L2):
+                    ip2 = compute_inferred_probabilities_level2(self.game_state, player_id)
+                    ip2_t = ip2 if torch.is_tensor(ip2) else torch.as_tensor(ip2, dtype=torch.float32, device=device)
+                    self._inferred_probs_l2_t.copy_(ip2_t.to(dtype=torch.float32, device=device))
+                if bool(OBS_INCLUDE_INFERRED_L3):
+                    ip3 = compute_inferred_probabilities_level3(self.game_state, player_id)
+                    ip3_t = ip3 if torch.is_tensor(ip3) else torch.as_tensor(ip3, dtype=torch.float32, device=device)
+                    self._inferred_probs_l3_t.copy_(ip3_t.to(dtype=torch.float32, device=device))
             if bool(OBS_INCLUDE_RANK_PROBS):
                 from observation import compute_rank_probabilities_by_player
                 rpb = compute_rank_probabilities_by_player(self.game_state, player_id).flatten()
diff --git a/evaluation/eval.py b/evaluation/eval.py
index 00865e97871478ea1f0b746caf0276e73ceab54e..21f2a4c57bc1cb3e744310e0ddcd0710ef9572d5 100644
--- a/evaluation/eval.py
+++ b/evaluation/eval.py
@@ -20,6 +20,7 @@ try:
     TB_AVAILABLE = True
 except Exception:
     TB_AVAILABLE = False
+from belief import sample_determinization
 from models.action_conditioned import ActionConditionedActor
 from utils.compile import maybe_compile_module
 from algorithms.is_mcts import run_is_mcts
@@ -348,257 +349,9 @@ def evaluate_pair_actors(ckpt_a: str, ckpt_b: str, games: int = 10,
                     return probs
                 # belief sampler neurale
                 def belief_sampler_neural(_env):
-                    obs_cur = _env._get_observation(_env.current_player)
-                    o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
-                    if torch.is_tensor(o_cpu):
-                        o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
-                    s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
-                    o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                    s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                    with torch.no_grad():
-                        state_feat = actor_model.state_enc(o_t, s_t)
-                        logits = actor_model.belief_net(state_feat)
-                        hand_table = o_t[:, :83]
-                        hand_mask = hand_table[:, :40] > 0.5
-                        table_mask = hand_table[:, 43:83] > 0.5
-                        captured = o_t[:, 83:165]
-                        cap0_mask = captured[:, :40] > 0.5
-                        cap1_mask = captured[:, 40:80] > 0.5
-                        visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
-                        probs_flat = actor_model.belief_net.probs(logits, visible_mask)
-                    probs = probs_flat.view(3, 40).detach().cpu().numpy()
-                    det = {}
-                    others = [(_env.current_player + 1) % 4, (_env.current_player + 2) % 4, (_env.current_player + 3) % 4]
-                    for i, pid in enumerate(others):
-                        det[pid] = []
-                    vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
-                    unknown_ids = [cid for cid in range(40) if not vis[cid]]
-                    counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
-                    caps = [int(counts.get(pid, 0)) for pid in others]
-                    n = len(unknown_ids)
-                    if sum(caps) != n:
-                        caps[2] = max(0, n - caps[0] - caps[1])
-                    # semplice greedy per eval (si può allineare alla DP del trainer se serve)
-                    for cid in unknown_ids:
-                        pc = probs[:, cid]
-                        ps = pc / max(1e-9, pc.sum())
-                        j = int(torch.argmax(torch.tensor(ps)).item())
-                        if caps[j] > 0:
-                            det[others[j]].append(cid)
-                            caps[j] -= 1
-                    return det
-                sims_scaled, root_temp_dyn = _eval_resolve_mcts_params(env, dict(mcts))
-                mc = dict(mcts)
-                mc['sims'] = int(sims_scaled)
-                mc['root_temp'] = float(root_temp_dyn)
-                action = run_is_mcts(
-                    env,
-                    policy_fn=policy_fn,
-                    value_fn=lambda _o, _e: 0.0,  # solo policy-guided in eval rapida
-                    num_simulations=int(mc.get('sims', 128)),
-                    c_puct=float(mc.get('c_puct', 1.0)),
-                    belief=None,
-                    num_determinization=int(mc.get('dets', 1)),
-                    root_temperature=float(mc.get('root_temp', 0.0)),
-                    prior_smooth_eps=float(mc.get('prior_smooth_eps', 0.0)),
-                    robust_child=True,
-                    root_dirichlet_alpha=float(mc.get('root_dirichlet_alpha', 0.25)),
-                    root_dirichlet_eps=float(mc.get('root_dirichlet_eps', 0.25)),
-                    belief_sampler=belief_sampler_neural
-                )
-                return action
-            # Greedy actor selection con belief neurale
-            obs = env._get_observation(cp)
-            o_cpu = obs if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32)
-            if torch.is_tensor(o_cpu):
-                o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
-            leg_cpu = torch.stack([
-                (x.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32))
-            for x in legals], dim=0)
-            s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
-            if device.type == 'cuda':
-                o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                leg_t = leg_cpu.pin_memory().to(device=device, non_blocking=True)
-                s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-            else:
-                o_t = o_cpu.unsqueeze(0).to(device=device)
-                leg_t = leg_cpu.to(device=device)
-                s_t = s_cpu.unsqueeze(0).to(device=device)
-            with torch.no_grad():
-                logits = actor_model(o_t, leg_t, s_t)
-                idx = torch.argmax(logits).to('cpu')
-            return leg_cpu[idx]
-        # If a global line-profiler decorator is provided, wrap the closure so time
-        # is attributed inside the function rather than to its call sites.
-        _LP = globals().get('LINE_PROFILE_DECORATOR', None)
-        if _LP is not None:
-            _select = _LP(_select)
-        return _select
-
-    t_eval_start = time.time()
-    agent_fn_team0 = make_agent_fn(actor_a)
-    agent_fn_team1 = make_agent_fn(actor_b)
-    # Default description uses basename of checkpoints
-    if tqdm_desc is None:
-        a_name = os.path.basename(ckpt_a) if ckpt_a else 'A'
-        b_name = os.path.basename(ckpt_b) if ckpt_b else 'B'
-        tqdm_desc = f"Eval {a_name} vs {b_name}"
-    # Valuta workers paralleli da env: SCOPONE_EVAL_WORKERS>1 abilita versione parallela
-    num_workers_env = int(os.environ.get('SCOPONE_EVAL_WORKERS', '1'))
-    if num_workers_env > 1:
-        _dbg(f"parallel eval requested: workers={num_workers_env} games={int(games)} dets={(mcts or {}).get('dets', 1)} desc={tqdm_desc}")
-        # Distribuisci le determinizzazioni tra i worker: dets_per_worker >= 1
-        dets_total = int((mcts or {}).get('dets', 1))
-        if dets_total > 1:
-            dets_per_worker = max(1, dets_total // num_workers_env)
-            dets_rem = dets_total - dets_per_worker * num_workers_env
-            # Costruisci per-worker mcts con dets distribuiti quasi uniformemente
-            mcts_base = dict(mcts or {})
-            mcts_list = []
-            for i in range(num_workers_env):
-                d = dets_per_worker + (1 if i < dets_rem else 0)
-                mc = dict(mcts_base)
-                mc['dets'] = int(max(1, d))
-                mcts_list.append(mc)
-            _dbg(f"distributing dets across workers: total={dets_total} list={[mc['dets'] for mc in mcts_list]}")
-            diff, bd = evaluate_pair_actors_parallel_dist(ckpt_a, ckpt_b, games=games, k_history=k_history,
-                                                          mcts_list=mcts_list, belief_particles=belief_particles, belief_ess_frac=belief_ess_frac,
-                                                          num_workers=num_workers_env,
-                                                          tqdm_desc=tqdm_desc, tqdm_position=int(tqdm_position or 0), tqdm_disable=bool(tqdm_disable))
-            elapsed = time.time() - t_eval_start
-            try:
-                tqdm.write(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers={num_workers_env}, dist-dets)")
-            except Exception:
-                print(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers={num_workers_env}, dist-dets)", flush=True)
-            return diff, bd
-        # Nessuna multi-dets: usa parallelo standard
-        _dbg("starting parallel eval without multi-dets distribution")
-        diff, bd = evaluate_pair_actors_parallel(ckpt_a, ckpt_b, games=games, k_history=k_history,
-                                             mcts=mcts, belief_particles=belief_particles, belief_ess_frac=belief_ess_frac,
-                                             num_workers=num_workers_env,
-                                             tqdm_desc=tqdm_desc, tqdm_position=int(tqdm_position or 0), tqdm_disable=bool(tqdm_disable))
-        elapsed = time.time() - t_eval_start
-        try:
-            tqdm.write(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers={num_workers_env})")
-        except Exception:
-            print(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers={num_workers_env})", flush=True)
-        return diff, bd
-    else:
-        diff, bd = play_match(agent_fn_team0, agent_fn_team1, games=games, k_history=k_history,
-                            tqdm_desc=tqdm_desc, tqdm_position=int(tqdm_position or 0), tqdm_disable=bool(tqdm_disable))
-        elapsed = time.time() - t_eval_start
-        try:
-            tqdm.write(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers=1)")
-        except Exception:
-            print(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers=1)", flush=True)
-        return diff, bd
-
-
-def _eval_pair_chunk_worker(args):
-    """Worker: esegue un sottoinsieme di partite e ritorna (wins_int, breakdown_sum_dict)."""
-    # Args can be (wid, ckpt_a, ckpt_b, games, ...) or without wid
-    if len(args) == 8:
-        wid, ckpt_a, ckpt_b, games, k_history, mcts, belief_particles, belief_ess_frac = args
-    else:
-        wid = -1
-        (ckpt_a, ckpt_b, games, k_history, mcts, belief_particles, belief_ess_frac) = args
-    _dbg(f"worker[{wid}] start: games={int(games)} k_history={int(k_history)} dets={(mcts or {}).get('dets',1)}")
-    # Limit CPU threads per worker
-    try:
-        wt = int(os.environ.get('SCOPONE_WORKER_THREADS', '1'))
-        os.environ['OMP_NUM_THREADS'] = str(wt)
-        os.environ['MKL_NUM_THREADS'] = str(wt)
-        torch.set_num_threads(wt)
-        torch.set_num_interop_threads(1)
-    except Exception:
-        pass
-    # Ricrea attori in ciascun processo
-    from models.action_conditioned import ActionConditionedActor
-    from utils.compile import maybe_compile_module
-    from environment import ScoponeEnvMA
-    import os as _os
-    _os.environ['TQDM_DISABLE'] = '1'
-    env0 = ScoponeEnvMA(k_history=k_history)
-    obs_dim = env0.observation_space.shape[0]
-    del env0
-    _dbg(f"worker[{wid}] loading actors …")
-    actor_a = maybe_compile_module(ActionConditionedActor(obs_dim=obs_dim), name='ActionConditionedActor[eval_A]').to(device)
-    actor_b = maybe_compile_module(ActionConditionedActor(obs_dim=obs_dim), name='ActionConditionedActor[eval_B]').to(device)
-    if ckpt_a and os.path.isfile(ckpt_a):
-        st_a = torch.load(ckpt_a, map_location=device)
-        if isinstance(st_a, dict) and 'actor' in st_a:
-            actor_a.load_state_dict(st_a['actor'])
-    if ckpt_b and os.path.isfile(ckpt_b):
-        st_b = torch.load(ckpt_b, map_location=device)
-        if isinstance(st_b, dict) and 'actor' in st_b:
-            actor_b.load_state_dict(st_b['actor'])
-    actor_a.eval(); actor_b.eval()
-    _dbg(f"worker[{wid}] actors ready; starting matches …")
-
-    def make_agent_fn(actor_model):
-        def _select(env: ScoponeEnvMA):
-            legals = env.get_valid_actions()
-            cp = env.current_player
-            seat_vec = torch.zeros(6, dtype=torch.float32, device='cpu')
-            seat_vec[cp] = 1.0
-            seat_vec[4] = 1.0 if cp in [0, 2] else 0.0
-            seat_vec[5] = 1.0 if cp in [1, 3] else 0.0
-            if mcts is not None and len(legals) > 1:
-                belief_obj = None
-                def policy_fn(obs, legal_list):
-                    o_cpu = obs if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32)
-                    if torch.is_tensor(o_cpu):
-                        o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
-                    leg_cpu = torch.stack([
-                        (x.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32))
-                    for x in legal_list], dim=0)
-                    s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
-                    o_t = o_cpu.unsqueeze(0).to(device=device)
-                    leg_t = leg_cpu.to(device=device)
-                    s_t = s_cpu.unsqueeze(0).to(device=device)
-                    with torch.no_grad():
-                        logits = actor_model(o_t, leg_t, s_t)
-                        probs = torch.softmax(logits, dim=0).detach().cpu().numpy()
-                    return probs
-                def belief_sampler_neural(_env):
-                    obs_cur = _env._get_observation(_env.current_player)
-                    o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
-                    if torch.is_tensor(o_cpu):
-                        o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
-                    s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
-                    o_t = o_cpu.unsqueeze(0).to(device=device)
-                    s_t = s_cpu.unsqueeze(0).to(device=device)
-                    with torch.no_grad():
-                        state_feat = actor_model.state_enc(o_t, s_t)
-                        logits = actor_model.belief_net(state_feat)
-                        hand_table = o_t[:, :83]
-                        hand_mask = hand_table[:, :40] > 0.5
-                        table_mask = hand_table[:, 43:83] > 0.5
-                        captured = o_t[:, 83:165]
-                        cap0_mask = captured[:, :40] > 0.5
-                        cap1_mask = captured[:, 40:80] > 0.5
-                        visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
-                        probs_flat = actor_model.belief_net.probs(logits, visible_mask)
-                    probs = probs_flat.view(3, 40).detach().cpu().numpy()
-                    det = {}
-                    others = [(_env.current_player + 1) % 4, (_env.current_player + 2) % 4, (_env.current_player + 3) % 4]
-                    for i, pid in enumerate(others):
-                        det[pid] = []
-                    vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
-                    unknown_ids = [cid for cid in range(40) if not vis[cid]]
-                    counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
-                    caps = [int(counts.get(pid, 0)) for pid in others]
-                    n = len(unknown_ids)
-                    if sum(caps) != n:
-                        caps[2] = max(0, n - caps[0] - caps[1])
-                    for cid in unknown_ids:
-                        pc = probs[:, cid]
-                        ps = pc / max(1e-9, pc.sum())
-                        j = int(torch.argmax(torch.tensor(ps)).item())
-                        if caps[j] > 0:
-                            det[others[j]].append(cid)
-                            caps[j] -= 1
-                    return det
+                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
+                    noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
+                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
                 from algorithms.is_mcts import run_is_mcts
                 sims_scaled, root_temp_dyn = _eval_resolve_mcts_params(env, dict(mcts))
                 mc = dict(mcts)
@@ -954,4 +707,3 @@ def league_eval_and_update(league_dir='checkpoints/league', games=20, target_poi
     # Aggiorna Elo usando la differenza media di punti -> mapping lineare a score
     league.update_elo_from_diff(a, b, diff)
     return league.elo
-
diff --git a/main.py b/main.py
index 4e4779ed176a10affcdea7b6bac5bc8abc7e6942..15fa6ba1b645729e1990c81f9298ac5c88fd8146 100644
--- a/main.py
+++ b/main.py
@@ -45,6 +45,8 @@ os.environ.setdefault('TORCHDYNAMO_CACHE_SIZE_LIMIT', '32')
 # Se l'utente li ha già impostati nel proprio run, li rispettiamo (setdefault)
 os.environ.setdefault('OBS_INCLUDE_DEALER', '1')
 os.environ.setdefault('OBS_INCLUDE_INFERRED', '0')
+os.environ.setdefault('OBS_INCLUDE_INFERRED_L2', '0')
+os.environ.setdefault('OBS_INCLUDE_INFERRED_L3', '0')
 os.environ.setdefault('OBS_INCLUDE_RANK_PROBS', '0')
 os.environ.setdefault('OBS_INCLUDE_SCOPA_PROBS', '0')
 
@@ -72,6 +74,7 @@ os.environ.setdefault('SCOPONE_EP_PUT_TIMEOUT_S', '15')  # timeout inserimento e
 os.environ.setdefault('SCOPONE_TORCH_PROF_DIR', 'profiles')  # cartella output per tracce profiler JSON
 os.environ.setdefault('SCOPONE_RAISE_ON_CKPT_FAIL', '0')  # solleva se fallisce il load del checkpoint
 os.environ.setdefault('ENABLE_BELIEF_SUMMARY', '0')  # stampa riassunti belief (diagnostica)
+os.environ.setdefault('SCOPONE_BELIEF_BLEND_ALPHA', '0.65')
 os.environ.setdefault('DET_NOISE', '0.0')  # rumore per determinizzazioni MCTS (IS-MCTS)
 os.environ.setdefault('SCOPONE_COLLECT_MIN_BATCH', '0')  # minima dimensione batch prima di flush del collector
 os.environ.setdefault('SCOPONE_COLLECT_MAX_LATENCY_MS', '3.0')  # latenza massima (ms) prima del flush del collector
diff --git a/models/action_conditioned.py b/models/action_conditioned.py
index 473dd7336293b71ed137cd18860441be5abe3627..db32273116f404df5a6b4b9c00b13dffa6817f04 100644
--- a/models/action_conditioned.py
+++ b/models/action_conditioned.py
@@ -22,9 +22,12 @@ STRICT = (os.environ.get('SCOPONE_STRICT_CHECKS', '0') == '1')
 # Observation flags are now imported from observation.py to ensure consistency.
 from observation import (
     OBS_INCLUDE_INFERRED as _OBS_INCLUDE_INFERRED,
+    OBS_INCLUDE_INFERRED_L2 as _OBS_INCLUDE_INFERRED_L2,
+    OBS_INCLUDE_INFERRED_L3 as _OBS_INCLUDE_INFERRED_L3,
     OBS_INCLUDE_RANK_PROBS as _OBS_INCLUDE_RANK_PROBS,
     OBS_INCLUDE_SCOPA_PROBS as _OBS_INCLUDE_SCOPA_PROBS,
     OBS_INCLUDE_DEALER as _OBS_INCLUDE_DEALER,
+    get_compact_obs_dim as _get_compact_obs_dim,
 )
 
 import torch._dynamo as _dynamo  # type: ignore
@@ -32,6 +35,11 @@ _dynamo_disable = _dynamo.disable  # type: ignore[attr-defined]
 
 
 
+def _default_k_history() -> int:
+    return int(os.environ.get('SCOPONE_K_HISTORY', os.environ.get('SCOPONE_EVAL_K_HISTORY', '39')))
+
+
+
 class StateEncoderCompact(nn.Module):
     """
     Encoder per osservazione compatta con storia-k (blocchi da 61) e stats a
@@ -69,6 +77,8 @@ class StateEncoderCompact(nn.Module):
         # Stats and seat/team: fix input dim deterministically from flags to avoid LazyLinear pitfalls
         self.stats_in_dim = 99 \
             + (120 if _OBS_INCLUDE_INFERRED else 0) \
+            + (120 if _OBS_INCLUDE_INFERRED_L2 else 0) \
+            + (120 if _OBS_INCLUDE_INFERRED_L3 else 0) \
             + (10 if _OBS_INCLUDE_SCOPA_PROBS else 0) \
             + (150 if _OBS_INCLUDE_RANK_PROBS else 0) \
             + (4 if _OBS_INCLUDE_DEALER else 0)
@@ -494,8 +504,10 @@ class ActionConditionedActor(torch.nn.Module):
       - Belief head (120 → 64)
       - Proiezione stato → 64 e scoring via prodotto scalare con embedding azione (80 → 64)
     """
-    def __init__(self, obs_dim=10823, action_dim=80, state_encoder: StateEncoderCompact = None):
+    def __init__(self, obs_dim: Optional[int] = None, action_dim: int = 80, state_encoder: StateEncoderCompact = None):
         super().__init__()
+        if obs_dim is None:
+            obs_dim = _get_compact_obs_dim(_default_k_history())
         self.obs_dim = obs_dim
         self.action_dim = action_dim
         # Encoders
@@ -946,8 +958,10 @@ class CentralValueNet(torch.nn.Module):
     """
     Critico condizionato: usa StateEncoderCompact (256) + belief (120→64).
     """
-    def __init__(self, obs_dim=10823, state_encoder: StateEncoderCompact = None):
+    def __init__(self, obs_dim: Optional[int] = None, state_encoder: StateEncoderCompact = None):
         super().__init__()
+        if obs_dim is None:
+            obs_dim = _get_compact_obs_dim(_default_k_history())
         self.state_enc = state_encoder if state_encoder is not None else StateEncoderCompact()
         self.belief_head = nn.Sequential(nn.Linear(120, 64), nn.ReLU())
         self.belief_net = BeliefNet(in_dim=256, hidden_dim=512)
@@ -1104,4 +1118,3 @@ class CentralValueNet(torch.nn.Module):
     def load_state_dict(self, state_dict, strict=True):  # type: ignore[override]
         _ = strict  # keep arg for external callers; force non-strict loading
         return super().load_state_dict(state_dict, strict=False)
-
diff --git a/observation.py b/observation.py
index 14f5f2d44ecec4ffb859ba3199f6779d8dd54c7f..edb114f3498a4e97f8d4e25661d88bbf22c99828 100644
--- a/observation.py
+++ b/observation.py
@@ -4,6 +4,12 @@ from typing import Optional
 import torch
 import torch.nn.functional as F
 import torch._dynamo as _dynamo  # type: ignore
+from belief.hierarchy import (
+    compute_level1,
+    compute_level2,
+    compute_level3,
+    compute_belief_hierarchy,
+)
 _dynamo_disable = _dynamo.disable  # type: ignore[attr-defined]
 
 SUITS = ['denari', 'coppe', 'spade', 'bastoni']
@@ -20,10 +26,31 @@ import os as _os
 OBS_DEVICE = torch.device(_os.environ.get('OBS_DEVICE', _os.environ.get('SCOPONE_DEVICE', 'cpu')))
 # Disabilita di default le feature probabilistiche per lasciare che la rete impari il belief
 OBS_INCLUDE_INFERRED = os.environ.get('OBS_INCLUDE_INFERRED', '0') == '1'
+OBS_INCLUDE_INFERRED_L2 = os.environ.get('OBS_INCLUDE_INFERRED_L2', '0') == '1'
+OBS_INCLUDE_INFERRED_L3 = os.environ.get('OBS_INCLUDE_INFERRED_L3', '0') == '1'
 OBS_INCLUDE_RANK_PROBS = os.environ.get('OBS_INCLUDE_RANK_PROBS', '0') == '1'
 OBS_INCLUDE_SCOPA_PROBS = os.environ.get('OBS_INCLUDE_SCOPA_PROBS', '0') == '1'
 OBS_INCLUDE_DEALER = os.environ.get('OBS_INCLUDE_DEALER', '1') == '1'
 
+
+def get_compact_obs_dim(k_history: int) -> int:
+    """Return the observation dimensionality for the compact encoder."""
+    fixed = 43 + 40 + 82 + 61 * int(k_history) + 40 + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
+    total = fixed
+    if OBS_INCLUDE_INFERRED:
+        total += 120
+    if OBS_INCLUDE_INFERRED_L2:
+        total += 120
+    if OBS_INCLUDE_INFERRED_L3:
+        total += 120
+    if OBS_INCLUDE_SCOPA_PROBS:
+        total += 10
+    if OBS_INCLUDE_RANK_PROBS:
+        total += 150
+    if OBS_INCLUDE_DEALER:
+        total += 4
+    return total
+
 # ===== ID/Bitset helpers (device = OBS_DEVICE) =====
 RANK_OF_ID = torch.tensor([i // 4 + 1 for i in range(40)], dtype=torch.int16, device=OBS_DEVICE)
 RANK_OF_ID_F32 = RANK_OF_ID.to(torch.float32)
@@ -279,40 +306,75 @@ def compute_missing_cards_matrix(game_state, player_id):
 # Cache per probabilità inferite
 inferred_probs_cache = {}
 
+
+def _card_to_id_obs(card) -> int:
+    if isinstance(card, int):
+        return int(card)
+    rank, suit = card
+    return int((int(rank) - 1) * 4 + suit_to_col[str(suit)])
+
+
+def _played_bits(game_state, player_id: int) -> int:
+    played = game_state.get('_played_bits_by_player_t')
+    if torch.is_tensor(played):
+        return int(played[player_id].item())
+    if isinstance(played, dict) and player_id in played:
+        val = played[player_id]
+        if torch.is_tensor(val):
+            return int(val.item())
+        return int(val)
+    return 0
+
+
+def _hierarchy_cache_key(game_state, player_id: int):
+    hands = game_state.get('hands', {})
+    hand_self = tuple(sorted(_card_to_id_obs(c) for c in hands.get(player_id, [])))
+    table = tuple(sorted(_card_to_id_obs(c) for c in game_state.get('table', [])))
+    captured = game_state.get('captured_squads', {})
+    if isinstance(captured, dict):
+        team0 = tuple(sorted(_card_to_id_obs(c) for c in captured.get(0, [])))
+        team1 = tuple(sorted(_card_to_id_obs(c) for c in captured.get(1, [])))
+    else:
+        team0 = tuple()
+        team1 = tuple()
+    hand_sizes = tuple(len(hands.get(p, [])) for p in range(4))
+    played_bits = tuple(int(_played_bits(game_state, p)) for p in range(4))
+    return (player_id, hand_self, table, team0, team1, hand_sizes, played_bits)
+
+
+def _get_hierarchy_cached(game_state, player_id: int):
+    key = _hierarchy_cache_key(game_state, player_id)
+    cached = inferred_probs_cache.get(key)
+    if cached is None:
+        hier = compute_belief_hierarchy(game_state, player_id)
+        cached = {
+            'level1': hier['level1'].to(device=OBS_DEVICE, dtype=torch.float32).clone(),
+            'level2': hier['level2'].to(device=OBS_DEVICE, dtype=torch.float32).clone(),
+            'level3': hier['level3'].to(device=OBS_DEVICE, dtype=torch.float32).clone(),
+        }
+        inferred_probs_cache[key] = cached
+        if len(inferred_probs_cache) > 128:
+            import random
+            for ck in random.sample(list(inferred_probs_cache.keys()), 64):
+                if ck != key:
+                    inferred_probs_cache.pop(ck, None)
+    return cached
+
+
 def compute_inferred_probabilities(game_state, player_id):
-    """
-    Versione mirror-only e compile-friendly: richiede i mirror tensoriali
-    (_hands_bits_t, _table_bits_t, _captured_bits_t, _played_bits_by_player_t).
-    Output: (3*40,) flatten per i tre avversari (10x4 per ciascuno).
-    """
-    hands_bits_t = game_state.get('_hands_bits_t', None)
-    table_bits_t = game_state.get('_table_bits_t', None)
-    captured_bits_t = game_state.get('_captured_bits_t', None)
-    played_bits_by_player_t = game_state.get('_played_bits_by_player_t', None)
-    torch._assert(torch.is_tensor(hands_bits_t), "_hands_bits_t required")
-    torch._assert(torch.is_tensor(table_bits_t), "_table_bits_t required")
-    torch._assert(torch.is_tensor(captured_bits_t), "_captured_bits_t required")
-    torch._assert(torch.is_tensor(played_bits_by_player_t), "_played_bits_by_player_t required")
-
-    visible_bits = hands_bits_t[player_id] | table_bits_t | captured_bits_t[0] | captured_bits_t[1]
-    vis = (((visible_bits >> IDS_CUDA) & 1).to(torch.bool))
-    invisible = ~vis
-    total_unknown = invisible.to(torch.float32).sum().clamp(min=1.0)
-
-    probs = []
-    other_players = [p for p in range(4) if p != player_id]
-    for p in other_players:
-        hand_size = (((hands_bits_t[p] >> IDS_CUDA) & 1).to(torch.float32).sum())
-        pm = torch.zeros((10, 4), dtype=torch.float32, device=OBS_DEVICE)
-        played_mask = (((played_bits_by_player_t[p] >> IDS_CUDA) & 1).to(torch.bool))
-        possible_mask = invisible & (~played_mask)
-        idx = torch.nonzero(possible_mask, as_tuple=False).flatten()
-        if idx.numel() > 0:
-            rows = (RANK_OF_ID[idx].to(torch.long) - 1).clamp(0, 9)
-            cols = SUITCOL_OF_ID[idx].to(torch.long).clamp(0, 3)
-            pm[rows, cols] = (hand_size / total_unknown)
-        probs.append(pm.reshape(-1))
-    return torch.cat(probs)
+    """Return level-1 belief probabilities flattened as (120,)."""
+    cached = _get_hierarchy_cached(game_state, player_id)
+    return cached['level1'].reshape(-1).clone()
+
+
+def compute_inferred_probabilities_level2(game_state, player_id):
+    cached = _get_hierarchy_cached(game_state, player_id)
+    return cached['level2'].reshape(-1).clone()
+
+
+def compute_inferred_probabilities_level3(game_state, player_id):
+    cached = _get_hierarchy_cached(game_state, player_id)
+    return cached['level3'].reshape(-1).clone()
 
 # Cache per primiera
 primiera_cache = {}
@@ -928,6 +990,10 @@ def encode_state_compact_for_player_fast(game_state, player_id, k_history=12, ou
     # 6) Inferred probs (120) - opzionale
     inferred_probs = (compute_inferred_probabilities(game_state, player_id)
                       if OBS_INCLUDE_INFERRED else None)
+    inferred_probs_l2 = (compute_inferred_probabilities_level2(game_state, player_id)
+                         if OBS_INCLUDE_INFERRED_L2 else None)
+    inferred_probs_l3 = (compute_inferred_probabilities_level3(game_state, player_id)
+                         if OBS_INCLUDE_INFERRED_L3 else None)
 
     suits_oh = SUITS_OH_4x40
     prim_vals = PRIMIERA_PER_ID
@@ -1005,8 +1071,16 @@ def encode_state_compact_for_player_fast(game_state, player_id, k_history=12, ou
     include_scopa = OBS_INCLUDE_SCOPA_PROBS
     include_rank = OBS_INCLUDE_RANK_PROBS
     include_inferred = OBS_INCLUDE_INFERRED
-    expected_dim = (43 + 40 + 82 + 61 * k_history + 40 + (120 if include_inferred else 0) + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
-                    + (10 if include_scopa else 0) + (150 if include_rank else 0) + (4 if OBS_INCLUDE_DEALER else 0))
+    include_inferred_l2 = OBS_INCLUDE_INFERRED_L2
+    include_inferred_l3 = OBS_INCLUDE_INFERRED_L3
+    expected_dim = (43 + 40 + 82 + 61 * k_history + 40
+                    + (120 if include_inferred else 0)
+                    + (120 if include_inferred_l2 else 0)
+                    + (120 if include_inferred_l3 else 0)
+                    + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
+                    + (10 if include_scopa else 0)
+                    + (150 if include_rank else 0)
+                    + (4 if OBS_INCLUDE_DEALER else 0))
     if (out is not None) and torch.is_tensor(out) and out.shape == (expected_dim,) and out.dtype == torch.float32 and out.device == device:
         result = out
     else:
@@ -1025,6 +1099,10 @@ def encode_state_compact_for_player_fast(game_state, player_id, k_history=12, ou
     _w(missing_vec)
     if include_inferred and inferred_probs is not None:
         _w(inferred_probs if torch.is_tensor(inferred_probs) else torch.as_tensor(inferred_probs, dtype=torch.float32, device=device))
+    if include_inferred_l2 and inferred_probs_l2 is not None:
+        _w(inferred_probs_l2 if torch.is_tensor(inferred_probs_l2) else torch.as_tensor(inferred_probs_l2, dtype=torch.float32, device=device))
+    if include_inferred_l3 and inferred_probs_l3 is not None:
+        _w(inferred_probs_l3 if torch.is_tensor(inferred_probs_l3) else torch.as_tensor(inferred_probs_l3, dtype=torch.float32, device=device))
     _w(primiera_status)
     _w(denari_count)
     _w(settebello_status)
diff --git a/tests/test_code.py b/tests/test_code.py
index 881403cacfb5377be8ec39370d64215bbbbc64ae..9e2001ee15ee37523aa610eac5699d9346363978 100644
--- a/tests/test_code.py
+++ b/tests/test_code.py
@@ -1051,7 +1051,7 @@ def test_policy_prefers_optimal_ace_king_sequence_with_checkpoint():
         'SCOPONE_DEVICE',
         ('cuda' if torch.cuda.is_available() and os.environ.get('TESTS_FORCE_CPU') != '1' else 'cpu')
     ))
-    actor = ActionConditionedActor(obs_dim=10823, action_dim=80)
+    actor = ActionConditionedActor(action_dim=80)
     if ckpt_path and os.path.isfile(ckpt_path):
         try:
             state = torch.load(ckpt_path, map_location=device)
@@ -1175,4 +1175,4 @@ def test_policy_prefers_optimal_ace_king_sequence_with_checkpoint():
     idx_k = [i for i,v in enumerate(legals3) if any(is_action(v, kid, []) for kid in [k_den,k_cop,k_spa,k_bas])]
     assert len(idx_k) > 0
     top_idx3 = int(torch.argmax(logits3).item())
-    assert top_idx3 in idx_k, "P3 dovrebbe preferire posare un re"
\ No newline at end of file
+    assert top_idx3 in idx_k, "P3 dovrebbe preferire posare un re"
diff --git a/tools/benchmark_ac.py b/tools/benchmark_ac.py
index b133a168813e80f5552be6cff46be5cd005f7a89..586da8db7da62a028c5b4ca75799393a04228bc5 100644
--- a/tools/benchmark_ac.py
+++ b/tools/benchmark_ac.py
@@ -20,6 +20,7 @@ import torch
 
 from environment import ScoponeEnvMA
 from algorithms.is_mcts import run_is_mcts
+from belief import sample_determinization
 from models.action_conditioned import ActionConditionedActor, CentralValueNet
 
 
@@ -104,50 +105,9 @@ def run_benchmark(games=50, use_mcts=False, sims=128, dets=16, compact=True, k_h
                         return critic(o_t.unsqueeze(0), s.unsqueeze(0)).item()
                 # belief sampler neurale per determinizzazione
                 def belief_sampler_neural(_env):
-                    try:
-                        cp = _env.current_player
-                        obs_cur = _env._get_observation(cp)
-                        o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
-                        if torch.is_tensor(o_cpu):
-                            o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
-                        s_cpu = torch.zeros(6, dtype=torch.float32)
-                        s_cpu[cp] = 1.0
-                        s_cpu[4] = 1.0 if cp in [0, 2] else 0.0
-                        s_cpu[5] = 1.0 if cp in [1, 3] else 0.0
-                        o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                        s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                        with torch.no_grad():
-                            state_feat = actor.state_enc(o_t, s_t)
-                            logits = actor.belief_net(state_feat)
-                            hand_table = o_t[:, :83]
-                            hand_mask = hand_table[:, :40] > 0.5
-                            table_mask = hand_table[:, 43:83] > 0.5
-                            captured = o_t[:, 83:165]
-                            cap0_mask = captured[:, :40] > 0.5
-                            cap1_mask = captured[:, 40:80] > 0.5
-                            visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
-                            probs_flat = actor.belief_net.probs(logits, visible_mask)
-                        probs = probs_flat.view(3, 40).detach().cpu().numpy()
-                        vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
-                        unknown_ids = [cid for cid in range(40) if not vis[cid]]
-                        others = [(cp + 1) % 4, (cp + 2) % 4, (cp + 3) % 4]
-                        det = {pid: [] for pid in others}
-                        counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
-                        caps = [int(counts.get(pid, 0)) for pid in others]
-                        n = len(unknown_ids)
-                        if sum(caps) != n:
-                            caps[2] = max(0, n - caps[0] - caps[1])
-                        for cid in unknown_ids:
-                            pc = probs[:, cid]
-                            s = pc.sum()
-                            ps = pc / (s if s > 0 else 1e-9)
-                            j = int(torch.argmax(torch.tensor(ps)).item())
-                            if caps[j] > 0:
-                                det[others[j]].append(cid)
-                                caps[j] -= 1
-                        return det
-                    except Exception:
-                        return None
+                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
+                    noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
+                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
                 action = run_is_mcts(env, policy_fn, value_fn, num_simulations=sims, c_puct=c_puct,
                                      belief=None, num_determinization=dets, root_temperature=root_temp,
                                      prior_smooth_eps=prior_smooth_eps, robust_child=robust_child,
@@ -232,4 +192,3 @@ def main():
 if __name__ == '__main__':
     main()
 
-
diff --git a/tools/profile_ppo.py b/tools/profile_ppo.py
index 8eb42d3bfafc75faa3624c8a62ff03b4b8e0c594..3b5684582e6a4176ec4f47504c5fbbdc9fed46e1 100644
--- a/tools/profile_ppo.py
+++ b/tools/profile_ppo.py
@@ -52,6 +52,8 @@ os.environ.setdefault('TORCHDYNAMO_CACHE_SIZE_LIMIT', '32')
 # Se l'utente li ha già impostati nel proprio run, li rispettiamo (setdefault)
 os.environ.setdefault('OBS_INCLUDE_DEALER', '1')
 os.environ.setdefault('OBS_INCLUDE_INFERRED', '0')
+os.environ.setdefault('OBS_INCLUDE_INFERRED_L2', '0')
+os.environ.setdefault('OBS_INCLUDE_INFERRED_L3', '0')
 os.environ.setdefault('OBS_INCLUDE_RANK_PROBS', '0')
 os.environ.setdefault('OBS_INCLUDE_SCOPA_PROBS', '0')
 
@@ -78,6 +80,7 @@ os.environ.setdefault('SCOPONE_EP_PUT_TIMEOUT_S', '15')
 os.environ.setdefault('SCOPONE_TORCH_PROF_DIR', 'profiles')
 os.environ.setdefault('SCOPONE_RAISE_ON_CKPT_FAIL', '0')
 os.environ.setdefault('ENABLE_BELIEF_SUMMARY', '0')
+os.environ.setdefault('SCOPONE_BELIEF_BLEND_ALPHA', '0.65')
 os.environ.setdefault('DET_NOISE', '0.0')
 os.environ.setdefault('SCOPONE_COLLECT_MIN_BATCH', '0')
 os.environ.setdefault('SCOPONE_COLLECT_MAX_LATENCY_MS', '3.0')
diff --git a/trainers/train_ppo.py b/trainers/train_ppo.py
index 7744425b9c3fb6db7effdacec9d28d5f518d86c9..350cbef029ce3a9e69bb07801f2d68de25af20cb 100644
--- a/trainers/train_ppo.py
+++ b/trainers/train_ppo.py
@@ -21,6 +21,7 @@ from algorithms.ppo_ac import ActionConditionedPPO
 from utils.device import get_compute_device
  
 from selfplay.league import League
+from belief import sample_determinization
 from models.action_conditioned import ActionConditionedActor
 from utils.compile import maybe_compile_module
 from utils.seed import set_global_seeds, resolve_seed, temporary_seed
@@ -119,6 +120,8 @@ def _serial_seed_exit(token: Optional[Dict[str, Any]]) -> None:
     if cuda_outer is not None and torch.cuda.is_available():
         torch.cuda.set_rng_state_all(cuda_outer)  # type: ignore[arg-type]
 
+
+
 def _dbg(msg: str) -> None:
     if _PAR_DEBUG:
         try:
@@ -514,103 +517,9 @@ def _env_worker(worker_id: int,
                         _wdbg(f"ep {ep} step {step_idx}: MCTS value <- got value")
                     return float(resp.get('value', 0.0))
                 def belief_sampler_neural(_env):
-                    o_cur = _env._get_observation(_env.current_player)
-                    s_vec = _seat_vec_for(_env.current_player)
-                    o_cpu = (o_cur.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(o_cur) else torch.as_tensor(o_cur, dtype=torch.float32))
-                    if step_idx < 3:
-                        _wdbg(f"ep {ep} step {step_idx}: MCTS belief -> send, waiting probs")
-                    request_q.put({
-                        'type': 'score_belief',
-                        'wid': worker_id,
-                        'obs': o_cpu,
-                        'seat': s_vec,
-                    })
-                    try:
-                        resp = action_q.get(timeout=rpc_timeout_s)
-                    except queue.Empty as e:
-                        raise TimeoutError(f"worker {worker_id}: Timeout waiting for score_belief") from e
-                    if step_idx < 3:
-                        _wdbg(f"ep {ep} step {step_idx}: MCTS belief <- got probs")
-                    probs_flat = resp.get('belief_probs', None)
-                    if probs_flat is None:
-                        return None
-                    import numpy as _np
-                    probs = _np.asarray(probs_flat, dtype=_np.float32).reshape(3, 40)
-                    # visible mask from obs on worker side
-                    if torch.is_tensor(o_cur):
-                        o_t = o_cur.detach().to('cpu', dtype=torch.float32).unsqueeze(0)
-                    else:
-                        o_t = torch.as_tensor(o_cur, dtype=torch.float32).unsqueeze(0)
-                    hand_table = o_t[:, :83]
-                    hand_mask = hand_table[:, :40] > 0.5
-                    table_mask = hand_table[:, 43:83] > 0.5
-                    captured = o_t[:, 83:165]
-                    cap0_mask = captured[:, :40] > 0.5
-                    cap1_mask = captured[:, 40:80] > 0.5
-                    vis = (hand_mask | table_mask | cap0_mask | cap1_mask).squeeze(0).numpy().astype(bool)
-                    unknown_ids = [cid for cid in range(40) if not vis[cid]]
-                    others = [(_env.current_player + 1) % 4, (_env.current_player + 2) % 4, (_env.current_player + 3) % 4]
-                    counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
-                    caps = [int(counts.get(pid, 0)) for pid in others]
-                    n = len(unknown_ids)
-                    if sum(caps) != n:
-                        caps[2] = max(0, n - caps[0] - caps[1])
-                        if sum(caps) != n:
-                            base = n // 3
-                            rem = n - 3 * base
-                            caps = [base, base, base]
-                            for i in range(rem):
-                                caps[i] += 1
-                    import numpy as _np
+                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
                     noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
-                    costs = []
-                    for cid in unknown_ids:
-                        pc = probs[:, cid]
-                        ps = pc / max(1e-12, pc.sum())
-                        c = [-_np.log(max(1e-12, ps[i])) for i in range(3)]
-                        if noise_scale > 0:
-                            u = _np.random.uniform(1e-9, 1.0-1e-9, size=3)
-                            g = -_np.log(-_np.log(u)) * noise_scale
-                            c = [c[i] + float(g[i]) for i in range(3)]
-                        costs.append(c)
-                    INF = 1e12
-                    cap0, cap1, cap2 = caps
-                    dp = [[[INF]*(cap1+1) for _ in range(cap0+1)] for __ in range(n+1)]
-                    bk = [[[-1]*(cap1+1) for _ in range(cap0+1)] for __ in range(n+1)]
-                    dp[0][0][0] = 0.0
-                    for t in range(n):
-                        c0, c1, c2 = costs[t]
-                        for a in range(0, min(t, cap0)+1):
-                            for b in range(0, min(t-a, cap1)+1):
-                                cur = dp[t][a][b]
-                                if cur >= INF:
-                                    continue
-                                if a+1 <= cap0 and dp[t+1][a+1][b] > cur + c0:
-                                    dp[t+1][a+1][b] = cur + c0
-                                    bk[t+1][a+1][b] = 0
-                                if b+1 <= cap1 and dp[t+1][a][b+1] > cur + c1:
-                                    dp[t+1][a][b+1] = cur + c1
-                                    bk[t+1][a][b+1] = 1
-                                assigned2 = t - a - b
-                                if assigned2 + 1 <= cap2 and dp[t+1][a][b] > cur + c2:
-                                    dp[t+1][a][b] = cur + c2
-                                    bk[t+1][a][b] = 2
-                    if dp[n][cap0][cap1] >= INF:
-                        return None
-                    det = {pid: [] for pid in others}
-                    a, b = cap0, cap1
-                    for t in range(n, 0, -1):
-                        choice = bk[t][a][b]
-                        cid = unknown_ids[t-1]
-                        if choice == 0:
-                            det[others[0]].append(cid)
-                            a -= 1
-                        elif choice == 1:
-                            det[others[1]].append(cid)
-                            b -= 1
-                        else:
-                            det[others[2]].append(cid)
-                    return det
+                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
                 # Progress-based scaling (uniform with single-env) — optionally gated by env
                 progress = float(min(1.0, max(0.0, len(env.game_state.get('history', [])) / 40.0)))
                 denom = max(1e-6, (mcts_progress_full - mcts_progress_start))
@@ -1685,108 +1594,11 @@ def _collect_trajectory_impl(env: ScoponeEnvMA, agent: ActionConditionedPPO, hor
                         oh = probs_flat.view(1, 3, 40)
                         v = agent.critic(o_t, s_t, oh)
                     return float(v.squeeze(0).detach().cpu().item())
-                # Belief determinization sampler dal BeliefNet: campiona assignment coerenti
+                # Belief determinization sampler gerarchico
                 def belief_sampler_neural(_env):
-                    # Costruisci marginali 3x40 dal BeliefNet
-                    obs_cur = _env._get_observation(_env.current_player)
-                    o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
-                    if torch.is_tensor(o_cpu):
-                        o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
-                    s_cpu = _seat_vec_for(_env.current_player).detach().to('cpu', dtype=torch.float32)
-                    if device.type == 'cuda':
-                        o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                        s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
-                    else:
-                        o_t = o_cpu.unsqueeze(0).to(device=device)
-                        s_t = s_cpu.unsqueeze(0).to(device=device)
-                    with torch.no_grad():
-                        state_feat = agent.actor.state_enc(o_t, s_t)
-                        bn_dtype = agent.actor.belief_net.fc_in.weight.dtype
-                        if state_feat.dtype != bn_dtype:
-                            state_feat = state_feat.to(dtype=bn_dtype)
-                        logits = agent.actor.belief_net(state_feat)  # (1,120)
-                        # Visibilità dall'osservazione
-                        hand_table = o_t[:, :83]
-                        hand_mask = hand_table[:, :40] > 0.5
-                        table_mask = hand_table[:, 43:83] > 0.5
-                        captured = o_t[:, 83:165]
-                        cap0_mask = captured[:, :40] > 0.5
-                        cap1_mask = captured[:, 40:80] > 0.5
-                        visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)  # (1,40)
-                        probs_flat = agent.actor.belief_net.probs(logits, visible_mask)  # (1,120)
-                    probs = probs_flat.view(3, 40).detach().cpu().numpy()  # (3,40)
-                    vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
-                    unknown_ids = [cid for cid in range(40) if not vis[cid]]
-                    # Capacità (conteggi mano) correnti degli altri giocatori
-                    others = [(_env.current_player + 1) % 4, (_env.current_player + 2) % 4, (_env.current_player + 3) % 4]
-                    counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
-                    caps = [int(counts.get(pid, 0)) for pid in others]
-                    n = len(unknown_ids)
-                    # riallinea capacità se necessario
-                    if sum(caps) != n:
-                        caps[2] = max(0, n - caps[0] - caps[1])
-                        if sum(caps) != n:
-                            from utils.fallback import notify_fallback
-                            notify_fallback('trainer.worker.belief_sampler.uniform_caps')
-                    # Costi = -log p con piccolo rumore per diversità
+                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
                     noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
-                    costs = []  # shape (n,3)
-                    for cid in unknown_ids:
-                        pc = probs[:, cid]
-                        ps = pc / max(1e-12, pc.sum())
-                        c = [-_np.log(max(1e-12, ps[i])) for i in range(3)]
-                        if noise_scale > 0:
-                            u = _np.random.uniform(1e-9, 1.0-1e-9, size=3)
-                            g = -_np.log(-_np.log(u)) * noise_scale
-                            c = [c[i] + float(g[i]) for i in range(3)]
-                        costs.append(c)
-                    # DP ottimo per 3 giocatori con capacità note
-                    INF = 1e12
-                    cap0, cap1, cap2 = caps
-                    dp = [[[INF]*(cap1+1) for _ in range(cap0+1)] for __ in range(n+1)]
-                    bk = [[[-1]*(cap1+1) for _ in range(cap0+1)] for __ in range(n+1)]
-                    dp[0][0][0] = 0.0
-                    for t in range(n):
-                        c0, c1, c2 = costs[t]
-                        for a in range(0, min(t, cap0)+1):
-                            for b in range(0, min(t-a, cap1)+1):
-                                cur = dp[t][a][b]
-                                if cur >= INF: 
-                                    continue
-                                # assign to player 0
-                                if a+1 <= cap0:
-                                    if dp[t+1][a+1][b] > cur + c0:
-                                        dp[t+1][a+1][b] = cur + c0
-                                        bk[t+1][a+1][b] = 0
-                                # assign to player 1
-                                if b+1 <= cap1:
-                                    if dp[t+1][a][b+1] > cur + c1:
-                                        dp[t+1][a][b+1] = cur + c1
-                                        bk[t+1][a][b+1] = 1
-                                # assign to player 2 (implicit count)
-                                assigned2 = t - a - b
-                                if assigned2 + 1 <= cap2:
-                                    if dp[t+1][a][b] > cur + c2:
-                                        dp[t+1][a][b] = cur + c2
-                                        bk[t+1][a][b] = 2
-                    if dp[n][cap0][cap1] >= INF:
-                        from utils.fallback import notify_fallback
-                        notify_fallback('trainer.belief_sampler.dp_infeasible')
-                    # Ricostruisci percorso
-                    det = {pid: [] for pid in others}
-                    a, b = cap0, cap1
-                    for t in range(n, 0, -1):
-                        choice = bk[t][a][b]
-                        cid = unknown_ids[t-1]
-                        if choice == 0:
-                            det[others[0]].append(cid)
-                            a -= 1
-                        elif choice == 1:
-                            det[others[1]].append(cid)
-                            b -= 1
-                        else:
-                            det[others[2]].append(cid)
-                    return det
+                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
                 # temperatura radice dinamica: alta a inizio mano, bassa verso la fine (solo se scaling attivo)
                 scaling_on = (str(os.environ.get('SCOPONE_MCTS_SCALING', '1')).strip().lower() in ['1','true','yes','on'])
                 root_temp_dyn = float(mcts_root_temp) if (not scaling_on or float(mcts_root_temp) > 0) else float(max(0.0, 1.0 - alpha))
