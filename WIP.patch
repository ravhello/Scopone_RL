commit 732c62ea870fc44905c02f8321477a527c36083c
Merge: 52ed673 09e5c60 19b6961
Author: Riccardo Ravello <b00792959@essec.edu>
Date:   Mon Oct 6 06:51:31 2025 +0200

    On hybrid-gpu: wip

diff --cc GUIDA.md
index a2ab44d,a2ab44d,0000000..899620b
mode 100644,100644,000000..100644
--- a/GUIDA.md
+++ b/GUIDA.md
@@@@ -1,200 -1,200 -1,0 +1,201 @@@@
  +## Guida all'uso della repository Scopone_RL
  +
  +Questa guida spiega come installare, allenare, fare benchmark e profilare l'agente di Scopone. Elenca tutte le opzioni disponibili a riga di comando per gli script principali e chiarisce i parametri di default scelti nel codice.
  +
  +### Installazione
  +- Requisiti: Python 3.10+, CUDA opzionale per GPU.
  +- Installazione pacchetti:
  +```bash
  +pip install -r requirements.txt
  +```
  +
  +In alternativa, con i target del `Makefile` (richiede l'ambiente conda `gym_tf` già creato):
  +```bash
  +make install
  +```
  +
  +### Modi di esecuzione principali
  +- Allenamento PPO: `python trainers/train_ppo.py` (consigliato)
  +- Benchmark policy/IS-MCTS: `python tools/benchmark_ac.py`
  +- Confronto checkpoint Team 0: `python benchmark.py`
  +- Profiling breve dell'allenamento: `python tools/profile_ppo.py`
  +- Target di comodo del `Makefile`:
  +  - `make run` → `python main.py` (codice sperimentale separato, senza CLI)
  +  - `make benchmark` → `python benchmark.py`
  +  - `make test` → esegue test con pytest
  +
  +Nota: la GUI (`scopone_gui.py`) è WIP e non espone una CLI stabile.
  +
  +---
  +
  +## Allenamento PPO (Action-Conditioned)
  +Script: `trainers/train_ppo.py`
  +
  +Esempio rapido (consigliato):
  +```bash
  +python trainers/train_ppo.py \
  +  --iters 2000 --horizon 256 \
  +  --compact --k-history 12 \
  +  --seed 0 \
  +  --ckpt checkpoints/ppo_ac.pth
  +```
  +
  +Opzioni CLI e significato:
  +- `--iters` (int, default 2000): numero di iterazioni PPO da eseguire.
  +- `--horizon` (int, default 256): numero di step per iterazione (rollout). Con solo reward finale, ~horizon//40 episodi per iter.
  +- `--save-every` (int, default 200): salva un checkpoint ogni N iterazioni.
  +- `--ckpt` (str, default `checkpoints/ppo_ac.pth`): percorso file checkpoint.
  +- `--compact` (flag): usa osservazione compatta (raccomandato in produzione). Se assente, usa la modalità di default non compatta.
  +- `--k-history` (int, default 39): numero di mosse recenti nella storia compatta. Valori più bassi riducono costo computazionale (es. 12).
  +- `--seed` (int, default 0): seed di riproducibilità.
  +- `--entropy-schedule` (str, default `linear`, scelte: `linear`, `cosine`): schedule del coefficiente di entropia durante il training.
  +- `--eval-every` (int, default 0): ogni quante iterazioni eseguire una mini-valutazione (0 = disabilitata).
  +- `--eval-games` (int, default 10): numero di partite nella mini-valutazione.
  +- `--belief-particles` (int, default 512): numero di particelle del filtro di credenze (belief) usato dal trainer.
  +- `--belief-ess-frac` (float, default 0.5): soglia ESS frazionaria per il resampling del belief.
  +- `--mcts-eval` (flag): abilita l'uso dell'IS-MCTS nella mini-valutazione.
  +- `--mcts-train` (flag, default True): presente nella CLI ma attualmente non utilizzato nel trainer. L'allenamento usa sempre IS-MCTS con warmup (disattivo per ~500 iterazioni) e poi attivo; non è disattivabile da CLI.
  +- `--train-both-teams` (flag): presente nella CLI ma attualmente non utilizzato; il trainer alterna i seat principali tra 0/2 e 1/3 e chiama `collect_trajectory(..., train_both_teams=False)`.
  +- `--mcts-sims` (int, default 128): numero di simulazioni per mossa nell'IS-MCTS (per train/eval se attivo).
  +- `--mcts-dets` (int, default 4): numero di determinizzazioni del belief per ricerca MCTS (train/eval se attivo).
  +- `--mcts-c-puct` (float, default 1.0): costante di esplorazione PUCT.
  +- `--mcts-root-temp` (float, default 0.0): temperatura alla radice per campionamento basato su visite (0 = argmax visite).
  +- `--mcts-prior-smooth-eps` (float, default 0.0): smoothing dei prior (1−eps)·p + eps/|A| (default NEUTRO).
  +- `--mcts-dirichlet-alpha` (float, default 0.25): parametro alpha per rumore Dirichlet alla radice (se `--mcts-dirichlet-eps` > 0).
  +- `--mcts-dirichlet-eps` (float, default 0.0): mixing con rumore Dirichlet alla radice (default NEUTRO).
  +
  +Output e logging:
  +- Checkpoint salvato su `--ckpt` ogni `--save-every` iterazioni (contiene pesi actor/critic e ottimizzatori).
  +- Log per TensorBoard in `runs/` se disponibile: eseguire `tensorboard --logdir runs` per visualizzare.
  +
  +Note utili:
  +- È fortemente consigliato `--compact` con un `--k-history` moderato (es. 12) per tenere bassa la dimensione dell'osservazione.
  +
  +Parametri PPO interni (scelte di default nel codice `algorithms/ppo_ac.py`):
  +- `lr=3e-4`, `clip_ratio=0.2`, `value_coef=0.5`, `entropy_coef=0.01`, `value_clip=0.2`, `target_kl=0.02`.
  +- Early stop se KL supera il target (con pazienza interna); riduzione LR automatica se persiste.
  +- Mixed precision CUDA (AMP) e clip del gradiente a 0.5.
  +
  +Regole di gioco (default effettive nel trainer):
  +- Salvo diversa indicazione, tutte le varianti sono disattivate (`asso_piglia_tutto`, `scopa_on_asso_piglia_tutto`, `re_bello`, `napola`, ecc.).
  +- `last_cards_to_dealer`: True (a fine mano le carte rimaste vanno all'ultimo catturante quando la regola è attiva).
  +- `shape_scopa`: False (nessun reward shaping intermedio durante l'episodio).
  +- Altre chiavi supportate ma non impostate dal trainer restano ai default hard-coded del codice (es. `napola_scoring='fixed3'`, `max_consecutive_scope=None`).
  +
  +---
  +
  +## Benchmark policy / IS-MCTS
  +Script: `tools/benchmark_ac.py`
  +
  +Esempi:
  +```bash
  +# Solo policy AC
  +python tools/benchmark_ac.py --games 100 --compact --k-history 12 --ckpt checkpoints/ppo_ac.pth --out-json summary.json
  +
  +# Policy + IS-MCTS (booster)
  +python tools/benchmark_ac.py --mcts --sims 256 --dets 16 --games 50 \
  +  --compact --k-history 12 --ckpt checkpoints/ppo_ac.pth --out-json summary_mcts.json
  +```
  +
  +Opzioni CLI e significato:
  +- `--mcts` (flag): abilita IS-MCTS come booster decisionale.
  +- `--sims` (int, default 128): simulazioni MCTS per mossa.
  +- `--dets` (int, default 16): determinizzazioni del belief per ricerca.
  +- `--compact` (flag): usa osservazione compatta.
  +- `--k-history` (int, default 12): mosse recenti nella storia compatta.
  +- `--ckpt` (str, default vuoto): percorso checkpoint per actor/critic (opzionale). Se vuoto, usa pesi iniziali.
  +- `--games` (int, default 50): numero di partite.
  +- `--seed` (int, default 0): seed.
  +- `--c-puct` (float, default 1.0): costante PUCT.
  +- `--root-temp` (float, default 0.0): temperatura alla radice (0 = scelta deterministica per visite).
  +- `--prior-smooth-eps` (float, default 0.0): smoothing dei prior.
  +- `--belief-particles` (int, default 256): particelle del belief.
  +- `--belief-ess-frac` (float, default 0.5): soglia ESS per resampling del belief.
  +- `--robust-child` (flag): seleziona il figlio con più visite (robust child). Se non impostato, usa max-Q.
  +- `--root-dirichlet-alpha` (float, default 0.0): alpha del Dirichlet alla radice.
  +- `--root-dirichlet-eps` (float, default 0.0): mixing con il rumore Dirichlet alla radice (default NEUTRO).
  +- `--out-csv` (str): salva report per-partita in CSV.
  +- `--out-json` (str): salva sommario aggregato in JSON.
  +
  +Output:
  +- Stampa un sommario con win rate e statistiche di punteggio.
  +- Opzionalmente, salva CSV per partita e JSON riassuntivo.
  +
  +---
  +
  +## Confronto checkpoint Team 0 (pairwise)
  +Script: `benchmark.py` (entrypoint principale del file)
  +
  +Esempio:
  +```bash
  +python benchmark.py --checkpoint_dir checkpoints/ --checkpoint_pattern "*team0*ep*.pth" --games 5000 --excel confronto.xlsx
  +```
  +
  +Opzioni CLI e significato:
  +- `--checkpoints` (lista di path): file o directory di checkpoint da includere (Team 0). Accetta multipli.
  +- `--checkpoint_dir` (str): directory contenente checkpoint (filtrati con `--checkpoint_pattern`).
  +- `--checkpoint_pattern` (str, default `*team0*ep*.pth`): pattern per il match dei file.
  +- `--games` (int, default 10000): partite per ogni scontro testa-a-testa.
  +- `--output` (str): file di testo per i risultati (default auto-generato con timestamp).
  +- `--excel` (str): file Excel per la matrice comparativa (default auto-generato con timestamp).
  +- `--limit` (int): limita il numero di checkpoint valutati (se > numero trovato, riduce con campionamento equispaziato).
  +
  +Output:
  +- Report testuale e file Excel con matrici di win rate, differenza punteggio, vantaggio del primo giocatore e lunghezza media partite.
  +
  +Note:
  +- Lo script filtra automaticamente solo i checkpoint di Team 0 (nel nome deve comparire `team0`).
  +
  +---
  +
  +## Profiling dell'allenamento
  +Script: `tools/profile_ppo.py`
  +
  +Esempi:
  +```bash
  +# Profiling con torch.profiler
  +python tools/profile_ppo.py --iters 50 --horizon 256
  +
  +# Profiling line-by-line (richiede profiler interno)
  +python tools/profile_ppo.py --iters 50 --horizon 256 --line --wrap-update --report
  +```
  +
  +Opzioni CLI e significato:
  +- `--iters` (int, default 50): numero di iterazioni da profilare (run breve).
  +- `--horizon` (int, default 256): horizon per iterazione durante il profiling.
  +- `--line` (flag): abilita il profiler per-linea con tempi per riga.
  +- `--wrap-update` (flag): profila anche `ActionConditionedPPO.update` (più lento).
  +- `--report` (flag): stampa un report esteso del line-profiler.
  +
  +---
  +
  +## Consigli pratici e note progettuali
  +- Osservazione compatta: usare `--compact` e regolare `--k-history` per bilanciare informatività e costo computazionale. Valori tipici: 12–39.
  +- Entropia: il coefficiente di entropia segue uno schedule (`linear` o `cosine`) per facilitare esplorazione iniziale e stabilizzazione successiva.
  +- Belief + IS-MCTS: aumentare `--belief-particles`, `--dets` e `--sims` migliora la qualità della ricerca ma scala i tempi.
+++- Gerarchia belief (livelli 1-3): abilita `OBS_INCLUDE_INFERRED_L2`/`OBS_INCLUDE_INFERRED_L3` per avere le feature dei livelli extra in osservazione; MCTS usa automaticamente il blend controllato da `SCOPONE_BELIEF_BLEND_ALPHA` (default 0.65).
  +- Riproducibilità: impostare sempre `--seed`. Se si passa un seed negativo, il codice genera un seed casuale non-negativo e lo stampa. I checkpoint includono un `run_config` minimale.
  +- Dipendenze: la repo usa `gymnasium` (non `gym`).
  +- Dispositivi/AMP: su CUDA, il trainer usa AMP con GradScaler (API unificata); su CPU la mixed precision è disabilitata.
  +- TensorBoard: avviare `tensorboard --logdir runs` per monitorare loss, KL, entropia, grad norm, ecc.
  +- Dispositivo ambiente: per impostazione predefinita l'ambiente gira su CPU. È possibile forzare il device impostando `ENV_DEVICE` (es. `ENV_DEVICE=cuda`), ma in generale è sconsigliato per via di micro-kernel poco efficienti.
  +
  +---
  +
  +## Riepilogo cartelle output
  +- `checkpoints/`: salvataggi periodici (`--save-every`) del trainer PPO.
  +- `runs/`: log per TensorBoard.
  +- Output benchmark: file `.csv`, `.json`, `.txt`, `.xlsx` a seconda delle opzioni.
  +
  +
  +Comandi comuni:
  +python /home/rikyravi/Scopone_RL/tools/profile_ppo.py --cprofile --iters 1
  +python /home/rikyravi/Scopone_RL/tools/profile_ppo.py --line --profile-all  --line-full --iters 1
  +python /home/rikyravi/Scopone_RL/tools/profile_ppo.py --scalene --scalene-out html --iters 1
  +python /home/rikyravi/Scopone_RL/tools/profile_ppo.py --scalene --iters 1 --no-scalene-cpu-only --scalene-gpu-modes --scalene-out html
  +python /home/rikyravi/Scopone_RL/tools/profile_ppo.py --torch-profiler --iters 1 --horizon 2048
  +
  +Analizza i profiling del mio codice e dimmi dove viene speso più tempo. Poi ottimizza il mio codice in modo da risolvere il problema partendo dalle cose che perdono più tempo. Non usare fallback per gli errori (piuttosto causano raise, ma meglio che fallback).
  +
  +Attivazione server:
-- gcloud compute config-ssh
+++gcloud compute config-ssh
diff --cc algorithms/ppo_ac.py
index 9759323,9759323,0000000..5e1301c
mode 100644,100644,000000..100644
--- a/algorithms/ppo_ac.py
+++ b/algorithms/ppo_ac.py
@@@@ -1,1205 -1,1205 -1,0 +1,1207 @@@@
  +import torch
  +import torch.nn as nn
  +import torch.optim as optim
  +import time
  +from typing import List, Tuple, Optional, Dict
  +from contextlib import nullcontext
  +
  +from models.action_conditioned import ActionConditionedActor, CentralValueNet, StateEncoderCompact
  +from utils.device import get_compute_device, get_amp_dtype
  +from utils.compile import maybe_compile_module, maybe_compile_function
  +import os as _os
  +device = get_compute_device()
  +autocast_device = device.type
  +autocast_dtype = get_amp_dtype()
  +import os as _os
  +STRICT_CHECKS = (_os.environ.get('SCOPONE_STRICT_CHECKS', '0') == '1')
  +
  +
  +class ActionConditionedPPO:
  +    """
  +    Skeleton PPO per policy action-conditioned con mask implicito
  +    (passando solo le azioni legali). Non è CTDE completo, ma prepara la struttura.
  +    """
  +    def __init__(self,
--                  obs_dim: int = 10823,
+++                 obs_dim: Optional[int] = None,
  +                 action_dim: int = 80,
  +                 lr: float = 3e-4,
  +                 clip_ratio: float = 0.2,
  +                 value_coef: float = 0.5,
  +                 entropy_coef: float = 0.01,
  +                 value_clip: float = 0.2,
  +                 target_kl: float = 0.02,
  +                 k_history: int = None):
  +        shared_enc = StateEncoderCompact(k_history=k_history)
  +        self.actor = ActionConditionedActor(obs_dim, action_dim, state_encoder=shared_enc)
--         self.critic = CentralValueNet(obs_dim, state_encoder=shared_enc)
+++        obs_dim_val = self.actor.obs_dim
+++        self.obs_dim = obs_dim_val
+++        self.critic = CentralValueNet(obs_dim_val, state_encoder=shared_enc)
  +
  +        # Training device override: keep models on CPU for env/collection, move to GPU only in update if requested
  +        train_dev_str = _os.environ.get('SCOPONE_TRAIN_DEVICE', 'cpu').strip().lower()
  +        if train_dev_str.startswith('cuda') and (not torch.cuda.is_available()):
  +            train_dev_str = 'cpu'
  +        self.train_device = torch.device(train_dev_str)
  +        # AMP GradScaler only when training on CUDA
  +        self.scaler = None
  +        if self.train_device.type == 'cuda':
  +            self.scaler = torch.amp.GradScaler(device='cuda')
  +
  +        # Warm-up forward to materialize any Lazy modules (e.g., LazyLinear) only when CUDA
  +        if device.type == 'cuda':
  +            with torch.enable_grad():
--                 _obs_w = torch.zeros((2, obs_dim), dtype=torch.float32, device=device, requires_grad=True)
+++                _obs_w = torch.zeros((2, obs_dim_val), dtype=torch.float32, device=device, requires_grad=True)
  +                _seat_w = torch.zeros((2, 6), dtype=torch.float32, device=device)
  +                _seat_w[0, 0] = 1.0; _seat_w[0, 4] = 1.0
  +                _seat_w[1, 1] = 1.0; _seat_w[1, 5] = 1.0
  +                cm = torch.autocast(device_type=autocast_device, dtype=autocast_dtype)
  +                with cm:
  +                    _ = self.actor.compute_state_proj(_obs_w, _seat_w)
  +                    _ = self.critic(_obs_w, _seat_w)
  +
  +        # Unified compile: allow CPU and CUDA. Avoid compiling backward-heavy compute_loss on CPU.
  +        self.actor = maybe_compile_module(self.actor, name='ActionConditionedActor')
  +        self.critic = maybe_compile_module(self.critic, name='CentralValueNet')
  +        # Forward hotspots (safe on CPU/CUDA)
  +        try:
  +            self.actor.compute_state_proj = maybe_compile_function(self.actor.compute_state_proj, name='ActionConditionedActor.compute_state_proj')
  +        except Exception as e:
  +            raise RuntimeError('ppo.init.compile_compute_state_proj_failed') from e
  +        try:
  +            self._select_action_core = maybe_compile_function(self._select_action_core, name='ActionConditionedPPO._select_action_core')
  +        except Exception as e:
  +            raise RuntimeError('ppo.init.compile_select_action_core_failed') from e
  +        # Additional forward hotspots
  +        try:
  +            self.actor.compute_state_features = maybe_compile_function(self.actor.compute_state_features, name='ActionConditionedActor.compute_state_features')
  +        except Exception as e:
  +            raise RuntimeError('ppo.init.compile_compute_state_features_failed') from e
  +        try:
  +            self.actor.compute_state_proj_from_state = maybe_compile_function(self.actor.compute_state_proj_from_state, name='ActionConditionedActor.compute_state_proj_from_state')
  +        except Exception as e:
  +            raise RuntimeError('ppo.init.compile_compute_state_proj_from_state_failed') from e
  +        try:
  +            self.critic.forward_from_state = maybe_compile_function(self.critic.forward_from_state, name='CentralValueNet.forward_from_state')
  +        except Exception as e:
  +            raise RuntimeError('ppo.init.compile_critic_forward_from_state_failed') from e
  +        # Compile compute_loss as well (CPU/CUDA). We ensure stable strides/shapes in update() to support CPU.
  +        try:
  +            self.compute_loss = maybe_compile_function(self.compute_loss, name='ActionConditionedPPO.compute_loss')
  +        except Exception as e:
  +            raise RuntimeError('ppo.init.compile_compute_loss_failed') from e
  +
  +        # Ensure parameters start on the intended device for optimizer state placement
  +        self.actor.to('cpu')
  +        self.critic.to('cpu')
  +        # Optimizers with deduplicated shared encoder params to avoid double updates
  +        shared_ids = {id(p) for p in self.actor.state_enc.parameters()}
  +        actor_params = list(self.actor.parameters())
  +        critic_params = [p for p in self.critic.parameters() if id(p) not in shared_ids]
  +        self.opt_actor = optim.Adam(actor_params, lr=lr, fused=(self.train_device.type == 'cuda'))
  +        self.opt_critic = optim.Adam(critic_params, lr=lr, fused=(self.train_device.type == 'cuda'))
  +        self.clip_ratio = clip_ratio
  +        self.value_coef = value_coef
  +        self.entropy_coef = entropy_coef
  +        self.value_clip = value_clip
  +
  +        # Optional schedulers (can be set from trainer)
  +        self._lr_schedulers = []
  +        self._entropy_schedule = None
  +        self.update_steps = 0
  +        self.target_kl = target_kl
  +        self._high_kl_count = 0
  +        self._high_kl_patience = 5
  +        self._lr_decay_factor = 0.5
  +        # save config for reproducibility
  +        self.run_config = {
  +            'obs_dim': obs_dim,
  +            'action_dim': action_dim,
  +            'lr': lr,
  +            'clip_ratio': clip_ratio,
  +            'value_coef': value_coef,
  +            'entropy_coef': entropy_coef,
  +            'value_clip': value_clip,
  +            'target_kl': target_kl,
  +        }
  +
  +        # Perf flags
  +        try:
  +            torch.backends.cudnn.benchmark = True
  +            torch.backends.cudnn.enabled = True
  +            torch.backends.cuda.matmul.allow_tf32 = True
  +            torch.set_float32_matmul_precision('high')
  +        except Exception as e:
  +            raise RuntimeError('ppo.init.set_matmul_precision_failed') from e
  +
  +        # (train_device and scaler already set above)
  +
  +        # Pre-allocate pinned CPU buffers to cut repeated pin_memory/allocations in select_action
  +        # These are sized for single-step inference (batch size 1) and dynamic actions buffer
  +        if device.type == 'cuda':
  +            try:
  +                self._obs_cpu_pinned = torch.empty((1, obs_dim), dtype=torch.float32, pin_memory=True)
  +                self._seat_cpu_pinned = torch.empty((1, 6), dtype=torch.float32, pin_memory=True)
  +            except Exception as e:
  +                raise RuntimeError('ppo.init.pin_memory_unavailable') from e
  +            self._actions_cpu_pinned = None
  +            self._actions_cpu_capacity = 0
  +        else:
  +            self._obs_cpu_pinned = None
  +            self._seat_cpu_pinned = None
  +            self._actions_cpu_pinned = None
  +            self._actions_cpu_capacity = 0
  +
  +    def _ensure_actions_pinned_capacity(self, capacity: int):
  +        # Lazily allocate or grow the pinned actions buffer
  +        if (self._actions_cpu_pinned is None) or (capacity > self._actions_cpu_capacity):
  +            new_cap = max(capacity, int(self._actions_cpu_capacity * 1.5) if self._actions_cpu_capacity > 0 else 64)
  +            try:
  +                if device.type == 'cuda':
  +                    self._actions_cpu_pinned = torch.empty((new_cap, self.run_config['action_dim']), dtype=torch.float32, pin_memory=True)
  +                else:
  +                    self._actions_cpu_pinned = torch.empty((new_cap, self.run_config['action_dim']), dtype=torch.float32)
  +                self._actions_cpu_capacity = new_cap
  +            except Exception as e:
  +                raise RuntimeError('ppo.ensure_actions_pinned_capacity.pin_memory_unavailable') from e
  +
  +    def _ensure_actions_cpu_capacity(self, capacity: int):
  +        # Allocate or grow a CPU tensor (A,80) buffer for legal actions on CPU path
  +        if not hasattr(self, '_actions_cpu_buf_capacity'):
  +            self._actions_cpu_buf_capacity = 0
  +            self._actions_cpu_buf = None
  +        if (self._actions_cpu_buf is None) or (capacity > self._actions_cpu_buf_capacity):
  +            new_cap = max(capacity, int(self._actions_cpu_buf_capacity * 2) if self._actions_cpu_buf_capacity > 0 else 64)
  +            self._actions_cpu_buf = torch.empty((new_cap, self.run_config['action_dim']), dtype=torch.float32)
  +            self._actions_cpu_buf_capacity = new_cap
  +
  +    def select_action(self, obs, legal_actions: List, seat_team_vec = None, profiling_bins: Optional[Dict[str, float]] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
  +        if len(legal_actions) == 0:
  +            raise ValueError("No legal actions")
  +        # Validate legal actions dimensionality (accept list of (80,) or tensor (A,80))
  +        if torch.is_tensor(legal_actions):
  +            if legal_actions.dim() != 2 or legal_actions.size(1) != self.run_config['action_dim']:
  +                raise ValueError(f"legal actions tensor must be shape (A, 80), got {tuple(legal_actions.shape)}")
  +            if STRICT_CHECKS:
  +                # Validate binary structure of action encoding (hot-path disabled unless STRICT)
  +                la = legal_actions
  +                if (la.size(0) > 0) and (not torch.allclose(la[:, :40].sum(dim=1), torch.ones((la.size(0),), dtype=la.dtype, device=la.device))):
  +                    raise RuntimeError("select_action: each legal action must have exactly one played bit in [:40]")
  +                cap = la[:, 40:]
  +                if bool((((cap > 0.0 + 1e-6) & (cap < 1.0 - 1e-6)) | (cap < -1e-6) | (cap > 1.0 + 1e-6)).any().item() if torch.is_tensor(cap) else False):
  +                    raise RuntimeError("select_action: captured section must be binary (0/1)")
  +        elif isinstance(legal_actions, list) and len(legal_actions) > 0:
  +            la0 = legal_actions[0]
  +            if STRICT_CHECKS:
  +                if torch.is_tensor(la0):
  +                    if la0.dim() != 1 or la0.numel() != self.run_config['action_dim']:
  +                        raise ValueError(f"each legal action vector must be shape (80,), got {tuple(la0.shape)}")
  +                    if not torch.allclose(la0[:40].sum(), torch.tensor(1.0, dtype=la0.dtype, device=la0.device)):
  +                        raise RuntimeError("select_action: first legal action must have exactly one played bit in [:40]")
  +                else:
  +                    import numpy as _np
  +                    a0 = _np.asarray(la0)
  +                    if a0.ndim != 1 or a0.shape[0] != self.run_config['action_dim']:
  +                        raise ValueError(f"each legal action vector must be shape (80,), got {a0.shape}")
  +                    s = float(a0[:40].sum())
  +                    if abs(s - 1.0) > 1e-6:
  +                        raise RuntimeError("select_action: first legal action must have exactly one played bit in [:40]")
  +        # Accept obs on CPU by staging through pinned memory when needed
  +        if torch.is_tensor(obs):
  +            if obs.device.type == device.type:
  +                obs_t = obs.to(dtype=torch.float32)
  +                if obs_t.dim() == 1:
  +                    obs_t = obs_t.unsqueeze(0)
  +            elif obs.device.type == 'cpu':
  +                o_cpu = obs.detach().to('cpu', dtype=torch.float32)
  +                if o_cpu.dim() == 1:
  +                    o_cpu = o_cpu.unsqueeze(0)
  +                if device.type == 'cuda':
  +                    self._obs_cpu_pinned.copy_(o_cpu, non_blocking=True)
  +                    obs_t = self._obs_cpu_pinned.to(device=device, non_blocking=True)
  +                else:
  +                    obs_t = o_cpu
  +            else:
  +                raise RuntimeError('select_action expects obs tensor on compute device or CPU tensor')
  +        else:
  +            import numpy as _np
  +            o_cpu = torch.as_tensor(obs, dtype=torch.float32, device='cpu')
  +            if o_cpu.dim() == 1:
  +                o_cpu = o_cpu.unsqueeze(0)
  +            obs_t = (o_cpu.pin_memory().to(device=device, non_blocking=True) if device.type == 'cuda' else o_cpu)
  +
  +        # Legal actions: accept CPU and list inputs, stage via pinned memory when needed
  +        if torch.is_tensor(legal_actions):
  +            if legal_actions.device.type == device.type:
  +                actions_t = legal_actions.to(dtype=torch.float32)
  +            elif legal_actions.device.type == 'cpu':
  +                la_cpu = legal_actions.detach().to('cpu', dtype=torch.float32)
  +                A = int(la_cpu.size(0)) if la_cpu.dim() > 0 else 0
  +                if A > 0:
  +                    if device.type == 'cuda':
  +                        self._ensure_actions_pinned_capacity(A)
  +                        self._actions_cpu_pinned[:A].copy_(la_cpu, non_blocking=True)
  +                        actions_t = self._actions_cpu_pinned[:A].to(device=device, non_blocking=True)
  +                    else:
  +                        actions_t = la_cpu
  +                else:
  +                    actions_t = la_cpu.to(device=device)
  +            else:
  +                raise RuntimeError('select_action expects legal_actions tensor on compute device or CPU tensor')
  +        elif isinstance(legal_actions, list) and len(legal_actions) > 0:
  +            if torch.is_tensor(legal_actions[0]):
  +                # Prefer buffer reuse on CPU to avoid per-step stack allocations
  +                if device.type == 'cuda':
  +                    # GPU path: keep existing pinned+stack pipeline
  +                    la_cpu = torch.stack([x.detach().to('cpu', dtype=torch.float32) for x in legal_actions], dim=0)
  +                    A = int(la_cpu.size(0)) if la_cpu.dim() > 0 else 0
  +                    if A > 0:
  +                        self._ensure_actions_pinned_capacity(A)
  +                        self._actions_cpu_pinned[:A].copy_(la_cpu, non_blocking=True)
  +                        actions_t = self._actions_cpu_pinned[:A].to(device=device, non_blocking=True)
  +                    else:
  +                        actions_t = la_cpu.to(device=device)
  +                else:
  +                    # CPU path: copy into reusable CPU buffer directly
  +                    A = len(legal_actions)
  +                    self._ensure_actions_cpu_capacity(A)
  +                    buf = self._actions_cpu_buf[:A]
  +                    for i, a in enumerate(legal_actions):
  +                        buf[i].copy_(a.detach().to('cpu', dtype=torch.float32))
  +                    actions_t = buf
  +            else:
  +                import numpy as _np
  +                la_cpu = torch.as_tensor(_np.asarray(legal_actions, dtype=_np.float32), dtype=torch.float32, device='cpu')
  +                A = int(la_cpu.size(0)) if la_cpu.dim() > 0 else 0
  +                if A > 0:
  +                    if device.type == 'cuda':
  +                        self._ensure_actions_pinned_capacity(A)
  +                        self._actions_cpu_pinned[:A].copy_(la_cpu, non_blocking=True)
  +                        actions_t = self._actions_cpu_pinned[:A].to(device=device, non_blocking=True)
  +                    else:
  +                        actions_t = la_cpu
  +                else:
  +                    actions_t = la_cpu.to(device=device)
  +        else:
  +            raise RuntimeError('select_action expects non-empty legal_actions')
  +        # Validate each legal row has exactly one played bit
  +        ones_per_row = actions_t[:, :40].sum(dim=1)
  +        if STRICT_CHECKS:
  +            if not torch.allclose(ones_per_row, torch.ones_like(ones_per_row)):
  +                raise RuntimeError("select_action: each legal action must have exactly one played bit in [:40]")
  +        # Validate captured is binary only in STRICT mode
  +        if STRICT_CHECKS:
  +            cap = actions_t[:, 40:]
  +            cap_bad = ((cap > 0.0 + 1e-6) & (cap < 1.0 - 1e-6)) | (cap < -1e-6) | (cap > 1.0 + 1e-6)
  +            if bool(cap_bad.any().item() if torch.is_tensor(cap_bad) else cap_bad.any()):
  +                raise RuntimeError("select_action: captured section must be binary (0/1)")
  +
  +        st = None
  +        if seat_team_vec is not None:
  +            if torch.is_tensor(seat_team_vec) and (seat_team_vec.device.type == device.type):
  +                st = seat_team_vec.to(dtype=torch.float32)
  +                if st.dim() == 1:
  +                    st = st.unsqueeze(0)
  +            else:
  +                if torch.is_tensor(seat_team_vec):
  +                    st_cpu = seat_team_vec.detach().to('cpu', dtype=torch.float32)
  +                else:
  +                    st_cpu = torch.as_tensor(seat_team_vec, dtype=torch.float32, device='cpu')
  +                if device.type == 'cuda':
  +                    if (self._seat_cpu_pinned is not None) and (st_cpu.numel() == self._seat_cpu_pinned.size(1)):
  +                        self._seat_cpu_pinned[0].copy_(st_cpu, non_blocking=True)
  +                        st = self._seat_cpu_pinned.to(device=device, non_blocking=True)
  +                    else:
  +                        raise RuntimeError('select_action expects seat_team_vec on compute device or as CUDA-ready tensor')
  +                else:
  +                    st = st_cpu.unsqueeze(0) if st_cpu.dim() == 1 else st_cpu
  +            # Validate seat one-hot + team flags (after staging)
  +            if st is not None:
  +                if st.size(1) != 6:
  +                    raise ValueError("select_action: seat_team_vec must have shape (B,6)")
  +                if not (st[:, :4].sum(dim=1) == 1).all():
  +                    raise RuntimeError("select_action: seat one-hot invalid (sum != 1)")
  +                if ((st[:, 4:6] < 0) | (st[:, 4:6] > 1)).any():
  +                    raise RuntimeError("select_action: team flags out of [0,1]")
  +        # belief handled internally by the actor
  +
  +        # inference_mode disables autograd and some dispatcher overhead vs no_grad
  +        with torch.inference_mode():
  +            if profiling_bins is not None:
  +                chosen_act_d, logp_total_d, idx_t_d = self._select_action_core_profiled(obs_t, actions_t, st, profiling_bins=profiling_bins)
  +            else:
  +                chosen_act_d, logp_total_d, idx_t_d = self._select_action_core(obs_t, actions_t, st)
  +        # Move chosen action and metadata back to CPU for env.step
  +        chosen_act = chosen_act_d.detach().to('cpu', non_blocking=True)
  +        return chosen_act, logp_total_d.detach().to('cpu'), idx_t_d.detach().to('cpu')
  +
  +    def _select_action_core(self, obs_t: torch.Tensor, actions_t: torch.Tensor, seat_team_t: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
  +        """Pure compute core for select_action. Expects inputs already on device.
  +        Returns (chosen_action_tensor_on_device, logp_total_on_device, idx_on_device).
  +        """
  +        # Strict shape/device checks
  +        if obs_t.dim() != 2 or obs_t.size(0) != 1:
  +            raise ValueError(f"_select_action_core expects obs_t with shape (1, D), got {tuple(obs_t.shape)}")
  +        if actions_t.dim() != 2 or actions_t.size(1) != self.run_config['action_dim']:
  +            raise ValueError(f"_select_action_core expects actions_t with shape (A, {self.run_config['action_dim']}), got {tuple(actions_t.shape)}")
  +        if seat_team_t is not None and (seat_team_t.dim() != 2 or seat_team_t.size(1) != 6):
  +            raise ValueError(f"_select_action_core expects seat_team_t with shape (1, 6), got {None if seat_team_t is None else tuple(seat_team_t.shape)}")
  +        cm = torch.autocast(device_type=autocast_device, dtype=autocast_dtype) if device.type == 'cuda' else nullcontext()
  +        with cm:
  +            # Scoring simultaneo via fattorizzazione: proiezione di stato e logits carta
  +            state_proj = self.actor.compute_state_proj(obs_t, seat_team_t)  # (1,64)
  +            # Logits carta su tutte le 40 carte (mantieni dentro autocast per allineamento dtype)
  +            card_logits_all = torch.matmul(state_proj, self.actor.card_emb_play.t()).squeeze(0)  # (40)
  +        # Early diagnostics on non-finite values
  +        if STRICT_CHECKS and (not torch.isfinite(state_proj).all()):
  +            bad = state_proj[~torch.isfinite(state_proj)]
  +            raise RuntimeError(f"Actor state_proj contains non-finite values (count={int(bad.numel())})")
  +        if STRICT_CHECKS and (not torch.isfinite(card_logits_all).all()):
  +            bad = card_logits_all[~torch.isfinite(card_logits_all)]
  +            raise RuntimeError(f"Actor card_logits_all contains non-finite values (count={int(bad.numel())})")
  +        # Estrai card id per ciascuna azione legale
  +        played_ids_all = torch.argmax(actions_t[:, :40], dim=1)  # (A)
  +        # Validate legal actions have exactly one played bit per row
  +        ones_per_row = actions_t[:, :40].sum(dim=1)
  +        if STRICT_CHECKS:
  +            if not torch.allclose(ones_per_row, torch.ones_like(ones_per_row)):
  +                raise RuntimeError(f"Invalid legal actions: expected exactly one played card per row, got sums={ones_per_row.tolist()}")
  +
  +        # Two-stage sampling (equivalente alla softmax su logp_totals):
  +        # 1) campiona la carta tra quelle presenti nelle legali usando p_card ristretto alle carte ammissibili
  +        logp_cards_all = torch.log_softmax(card_logits_all, dim=0)  # (40)
  +        unique_cards, inv_idx = torch.unique(played_ids_all, sorted=False, return_inverse=True)
  +        logp_cards_allowed = logp_cards_all[unique_cards]  # (G)
  +        probs_card_allowed = torch.softmax(logp_cards_allowed, dim=0)  # rinormalizza su carte consentite
  +        # sample card
  +        cdf_c = torch.cumsum(probs_card_allowed, dim=0)
  +        # Avoid scalarization: clamp last element to valid positive value
  +        last_c = torch.clamp(cdf_c[-1], min=torch.finfo(cdf_c.dtype).eps)
  +        if STRICT_CHECKS:
  +            torch._assert(torch.isfinite(last_c), "Invalid card CDF: non-finite last element")
  +        u_c = torch.rand((), device=cdf_c.device, dtype=cdf_c.dtype) * last_c
  +        sel_card_pos = torch.searchsorted(cdf_c, u_c, right=True)
  +        sel_card_pos = torch.clamp(sel_card_pos, max=cdf_c.numel() - 1)
  +        sel_card_id = unique_cards[sel_card_pos]
  +
  +        # 2) campiona la presa condizionata alla carta scelta, usando solo le azioni del gruppo
  +        group_mask = (played_ids_all == sel_card_id)
  +        group_idx = torch.nonzero(group_mask, as_tuple=False).flatten()
  +        if group_idx.numel() <= 0:
  +            # Non dovrebbe accadere: carta scelta deve comparire tra le legali
  +            raise RuntimeError("Two-stage sampling: empty group for selected card")
  +        actions_grp = actions_t[group_idx]
  +        a_tbl = self.actor.get_action_emb_table_cached(device=actions_t.device, dtype=state_proj.dtype)  # (80,64)
  +        a_emb_grp = actions_grp.to(dtype=a_tbl.dtype) @ a_tbl  # (Gk,64)
  +        if STRICT_CHECKS and (not torch.isfinite(a_emb_grp).all()):
  +            bad = a_emb_grp[~torch.isfinite(a_emb_grp)]
  +            raise RuntimeError(f"Action embeddings (group) contain non-finite values (count={int(bad.numel())})")
  +        cap_logits_grp = torch.matmul(a_emb_grp, state_proj.squeeze(0).to(dtype=a_emb_grp.dtype))  # (Gk)
  +        if STRICT_CHECKS and (not torch.isfinite(cap_logits_grp).all()):
  +            bad = cap_logits_grp[~torch.isfinite(cap_logits_grp)]
  +            raise RuntimeError(f"Capture logits (group) contain non-finite values (count={int(bad.numel())})")
  +        # softmax nel gruppo
  +        probs_cap_grp = torch.softmax(cap_logits_grp, dim=0)
  +        probs_cap_grp = probs_cap_grp.nan_to_num(0.0)
  +        s_g = probs_cap_grp.sum()
  +        if STRICT_CHECKS and ((not torch.isfinite(s_g)) or (s_g <= 0)):
  +            raise RuntimeError("Invalid group probabilities for capture selection")
  +        cdf_g = torch.cumsum(probs_cap_grp, dim=0)
  +        u_g = torch.rand((), device=cdf_g.device, dtype=cdf_g.dtype) * cdf_g[-1]
  +        idx_in_group = torch.searchsorted(cdf_g, u_g, right=True)
  +        idx_in_group = torch.clamp(idx_in_group, max=cdf_g.numel() - 1)
  +
  +        # Indice assoluto e log-prob totale coerente con training (log-softmax su 40 carte + log-softmax nel gruppo)
  +        idx_t = group_idx[idx_in_group]
  +        logp_card_sel = logp_cards_all[sel_card_id]
  +        logp_cap_sel = torch.log_softmax(cap_logits_grp, dim=0)[idx_in_group]
  +        logp_total = (logp_card_sel + logp_cap_sel).detach()
  +        chosen_act = actions_t[idx_t].detach()
  +        return chosen_act, logp_total, idx_t
  +
  +    def _select_action_core_profiled(self, obs_t: torch.Tensor, actions_t: torch.Tensor, seat_team_t: torch.Tensor = None,
  +                                     profiling_bins: Dict[str, float] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
  +        if profiling_bins is None:
  +            return self._select_action_core(obs_t, actions_t, seat_team_t)
  +        profiling_bins.setdefault('state_proj', 0.0)
  +        profiling_bins.setdefault('action_enc', 0.0)
  +        profiling_bins.setdefault('sampling', 0.0)
  +
  +        if obs_t.dim() != 2 or obs_t.size(0) != 1:
  +            raise ValueError(f"_select_action_core expects obs_t with shape (1, D), got {tuple(obs_t.shape)}")
  +        if actions_t.dim() != 2 or actions_t.size(1) != self.run_config['action_dim']:
  +            raise ValueError(f"_select_action_core expects actions_t with shape (A, {self.run_config['action_dim']}), got {tuple(actions_t.shape)}")
  +        if seat_team_t is not None and (seat_team_t.dim() != 2 or seat_team_t.size(1) != 6):
  +            raise ValueError(f"_select_action_core expects seat_team_t with shape (1, 6), got {None if seat_team_t is None else tuple(seat_team_t.shape)}")
  +
  +        cm = torch.autocast(device_type=autocast_device, dtype=autocast_dtype) if device.type == 'cuda' else nullcontext()
  +        t_state = time.perf_counter()
  +        with cm:
  +            state_proj = self.actor.compute_state_proj(obs_t, seat_team_t)  # (1,64)
  +            card_logits_all = torch.matmul(state_proj, self.actor.card_emb_play.t()).squeeze(0)  # (40)
  +        profiling_bins['state_proj'] += (time.perf_counter() - t_state)
  +
  +        if STRICT_CHECKS and (not torch.isfinite(state_proj).all()):
  +            bad = state_proj[~torch.isfinite(state_proj)]
  +            raise RuntimeError(f"Actor state_proj contains non-finite values (count={int(bad.numel())})")
  +        if STRICT_CHECKS and (not torch.isfinite(card_logits_all).all()):
  +            bad = card_logits_all[~torch.isfinite(card_logits_all)]
  +            raise RuntimeError(f"Actor card_logits_all contains non-finite values (count={int(bad.numel())})")
  +
  +        played_ids_all = torch.argmax(actions_t[:, :40], dim=1)
  +        ones_per_row = actions_t[:, :40].sum(dim=1)
  +        if STRICT_CHECKS:
  +            if not torch.allclose(ones_per_row, torch.ones_like(ones_per_row)):
  +                raise RuntimeError(f"Invalid legal actions: expected exactly one played card per row, got sums={ones_per_row.tolist()}")
  +
  +        logp_cards_all = torch.log_softmax(card_logits_all, dim=0)  # (40)
  +        unique_cards, inv_idx = torch.unique(played_ids_all, sorted=False, return_inverse=True)
  +        logp_cards_allowed = logp_cards_all[unique_cards]
  +        probs_card_allowed = torch.softmax(logp_cards_allowed, dim=0)
  +        cdf_c = torch.cumsum(probs_card_allowed, dim=0)
  +        last_c = torch.clamp(cdf_c[-1], min=torch.finfo(cdf_c.dtype).eps)
  +        if STRICT_CHECKS:
  +            torch._assert(torch.isfinite(last_c), "Invalid card CDF: non-finite last element")
  +        u_c = torch.rand((), device=cdf_c.device, dtype=cdf_c.dtype) * last_c
  +        sel_card_pos = torch.searchsorted(cdf_c, u_c, right=True)
  +        sel_card_pos = torch.clamp(sel_card_pos, max=cdf_c.numel() - 1)
  +        sel_card_id = unique_cards[sel_card_pos]
  +
  +        group_mask = (played_ids_all == sel_card_id)
  +        group_idx = torch.nonzero(group_mask, as_tuple=False).flatten()
  +        if group_idx.numel() <= 0:
  +            raise RuntimeError("Two-stage sampling: empty group for selected card")
  +        actions_grp = actions_t[group_idx]
  +
  +        t_action = time.perf_counter()
  +        a_tbl = self.actor.get_action_emb_table_cached(device=actions_t.device, dtype=state_proj.dtype)
  +        a_emb_grp = actions_grp.to(dtype=a_tbl.dtype) @ a_tbl  # (Gk,64)
  +        profiling_bins['action_enc'] += (time.perf_counter() - t_action)
  +
  +        if STRICT_CHECKS and (not torch.isfinite(a_emb_grp).all()):
  +            bad = a_emb_grp[~torch.isfinite(a_emb_grp)]
  +            raise RuntimeError(f"Action embeddings (group) contain non-finite values (count={int(bad.numel())})")
  +
  +        t_sampling = time.perf_counter()
  +        cap_logits_grp = torch.matmul(a_emb_grp, state_proj.squeeze(0).to(dtype=a_emb_grp.dtype))  # (Gk)
  +        if STRICT_CHECKS and (not torch.isfinite(cap_logits_grp).all()):
  +            bad = cap_logits_grp[~torch.isfinite(cap_logits_grp)]
  +            raise RuntimeError(f"Capture logits (group) contain non-finite values (count={int(bad.numel())})")
  +        probs_cap_grp = torch.softmax(cap_logits_grp, dim=0)
  +        probs_cap_grp = probs_cap_grp.nan_to_num(0.0)
  +        s_g = probs_cap_grp.sum()
  +        if STRICT_CHECKS and ((not torch.isfinite(s_g)) or (s_g <= 0)):
  +            raise RuntimeError("Invalid group probabilities for capture selection")
  +        cdf_g = torch.cumsum(probs_cap_grp, dim=0)
  +        u_g = torch.rand((), device=cdf_g.device, dtype=cdf_g.dtype) * cdf_g[-1]
  +        idx_in_group = torch.searchsorted(cdf_g, u_g, right=True)
  +        idx_in_group = torch.clamp(idx_in_group, max=cdf_g.numel() - 1)
  +
  +        idx_t = group_idx[idx_in_group]
  +        logp_card_sel = logp_cards_all[sel_card_id]
  +        logp_cap_sel = torch.log_softmax(cap_logits_grp, dim=0)[idx_in_group]
  +        logp_total = (logp_card_sel + logp_cap_sel).detach()
  +        chosen_act = actions_t[idx_t].detach()
  +        profiling_bins['sampling'] += (time.perf_counter() - t_sampling)
  +        return chosen_act, logp_total, idx_t
  +
  +    def compute_loss(self, batch):
  +        """
  +        batch:
  +          - obs: (B, obs_dim)
  +          - act: (B, 80)
  +          - old_logp: (B)
  +          - ret: (B)
  +          - adv: (B)
  +          - legals: (M, 80) stack di tutte le azioni legali in ordine
  +          - legals_offset: (B) offset in legals per ciascun sample
  +          - legals_count: (B) numero di azioni legali per sample
  +          - chosen_index: (B) indice della scelta nel proprio sottoinsieme legale
  +        """
  +        def to_f32(x):
  +            if torch.is_tensor(x):
  +                # Evita copie inutili se già su device e dtype corretto
  +                if (x.device.type == device.type) and (x.dtype == torch.float32):
  +                    return x
  +                # Se è già su device ma dtype diverso, cambia solo dtype
  +                if x.device.type == device.type:
  +                    return x.to(dtype=torch.float32, non_blocking=True)
  +                # Altrimenti porta su CPU con dtype corretto (pin in to_cuda_nb se serve)
  +                return x.detach().to('cpu', dtype=torch.float32)
  +            return torch.as_tensor(x, dtype=torch.float32, device=device)
  +        def to_long(x):
  +            if torch.is_tensor(x):
  +                if (x.device.type == device.type) and (x.dtype == torch.long):
  +                    return x
  +                if x.device.type == device.type:
  +                    return x.to(dtype=torch.long, non_blocking=True)
  +                return x.detach().to('cpu', dtype=torch.long)
  +            return torch.as_tensor(x, dtype=torch.long, device=device)
  +
  +        # Accept CPU inputs; move to CUDA once in a pinned, non_blocking way
  +        # Route all compute in this function to the training device (can be CUDA while env stays on CPU)
  +        device = self.train_device
  +
  +        def to_cuda_nb(x, dtype):
  +            # Se il tensore è già su device/dtype corretti, restituisci direttamente
  +            if torch.is_tensor(x):
  +                if (x.device.type == device.type) and (x.dtype == dtype):
  +                    return x
  +                if x.device.type == device.type:
  +                    return x.to(dtype=dtype, non_blocking=True)
  +                x_cpu = x.detach().to('cpu', dtype=dtype)
  +            else:
  +                x_cpu = torch.as_tensor(x, dtype=dtype, device='cpu')
  +            if device.type == 'cuda':
  +                import torch._dynamo as _dyn  # type: ignore
  +                is_compiling = bool(getattr(_dyn, 'is_compiling', lambda: False)())
  +                if is_compiling or (x_cpu.numel() == 0):
  +                    return x_cpu.to(device=device, dtype=dtype, non_blocking=True)
  +                return x_cpu.pin_memory().to(device=device, dtype=dtype, non_blocking=True)
  +            else:
  +                return x_cpu
  +
  +        # Required keys validation
  +        for key in ('obs','act','old_logp','ret','adv','legals','legals_offset','legals_count','chosen_index','seat_team'):
  +            if key not in batch:
  +                raise KeyError(f"compute_loss: missing batch key '{key}'")
  +        obs = to_cuda_nb(batch['obs'], torch.float32)
  +        seat = to_cuda_nb(batch['seat_team'], torch.float32)
  +        act = to_cuda_nb(batch['act'], torch.float32)
  +        # Move core scalars to training device to avoid CPU↔CUDA mismatches
  +        old_logp = to_cuda_nb(batch['old_logp'], torch.float32)
  +        ret = to_cuda_nb(batch['ret'], torch.float32)
  +        adv = to_cuda_nb(batch['adv'], torch.float32)
  +        legals = to_cuda_nb(batch['legals'], torch.float32)
  +        offs = to_cuda_nb(batch['legals_offset'], torch.long)
  +        cnts = to_cuda_nb(batch['legals_count'], torch.long)
  +        chosen_idx = to_cuda_nb(batch['chosen_index'], torch.long)
  +        # Plausibility checks on old_logp early to catch bad batches
  +        if STRICT_CHECKS and (not torch.isfinite(old_logp).all()):
  +            raise RuntimeError("compute_loss: old_logp contains non-finite values")
  +        if STRICT_CHECKS and old_logp.numel() > 0:
  +            pos_any = (old_logp > 1.0e-6).any()
  +            small_any = (old_logp < -120.0).any()
  +            if bool(pos_any.detach().cpu().item()):
  +                mx = float(old_logp.max().detach().cpu().item())
  +                raise RuntimeError(f"compute_loss: old_logp contains positive values (max={mx})")
  +            if bool(small_any.detach().cpu().item()):
  +                mn = float(old_logp.min().detach().cpu().item())
  +                raise RuntimeError(f"compute_loss: old_logp too small (min={mn})")
  +        # Sanity on ragged indices
  +        if obs.size(0) != seat.size(0) or obs.size(0) != act.size(0):
  +            raise RuntimeError("compute_loss: batch size mismatch among obs/seat/act")
  +        if obs.size(0) != offs.size(0) or obs.size(0) != cnts.size(0) or obs.size(0) != chosen_idx.size(0):
  +            raise RuntimeError("compute_loss: ragged indices sizes mismatch with batch size")
  +        # Optional precomputed global action embeddings to avoid recomputation per minibatch
  +        a_emb_global = batch.get('a_emb_global', None)
  +        # distillazione MCTS (targets raggruppati per sample): policy piatta e peso per-sample
  +        mcts_policy_flat = to_cuda_nb(batch.get('mcts_policy', torch.zeros((0,), dtype=torch.float32, device=device)), torch.float32)
  +        mcts_weight = to_cuda_nb(batch.get('mcts_weight', torch.zeros((0,), dtype=torch.float32, device=device)), torch.float32)
  +
  +        # Filtra eventuali sample senza azioni legali per evitare NaN
  +        valid_mask = cnts > 0
  +        if not bool(valid_mask.all()):
  +            if not bool(valid_mask.any()):
  +                zero = torch.tensor(0.0, device=device)
  +                return zero, {'loss_pi': 0.0, 'loss_v': 0.0, 'entropy': 0.0, 'approx_kl': 0.0, 'clip_frac': 0.0}
  +            obs = obs[valid_mask]
  +            seat = seat[valid_mask]
  +            act = act[valid_mask]
  +            old_logp = old_logp[valid_mask]
  +            ret = ret[valid_mask]
  +            adv = adv[valid_mask]
  +            offs = offs[valid_mask]
  +            cnts = cnts[valid_mask]
  +            chosen_idx = chosen_idx[valid_mask]
  +        B = obs.size(0)
  +        # Prepara indici legal per minibatch usando offs/cnts contro legals globali
  +        max_cnt = int(cnts.max().item()) if B > 0 else 0
  +        # Validate ragged structure before use
  +        if STRICT_CHECKS:
  +            neg_offs = (offs < 0).any()
  +            neg_cnts = (cnts < 0).any()
  +            if bool((neg_offs | neg_cnts).detach().cpu().item()):
  +                raise RuntimeError("compute_loss: negative offsets or counts in legals structure")
  +        if STRICT_CHECKS:
  +            if (B > 0) and (legals.size(0) > 0):
  +                last_sum = int((offs[-1] + cnts[-1]).detach().cpu().item())
  +                if last_sum > int(legals.size(0)):
  +                    raise RuntimeError("compute_loss: last window exceeds legals length")
  +        if STRICT_CHECKS:
  +            bad_low = (chosen_idx < 0).any()
  +            bad_high = ((chosen_idx >= cnts) & (cnts > 0)).any()
  +            if bool((bad_low | bad_high).detach().cpu().item()):
  +                raise RuntimeError("compute_loss: chosen_index out of range for some rows")
  +        row_idx = torch.arange(B, device=device, dtype=torch.long)
  +        # Compute state features once; reuse for actor and critic
  +        state_feat = self.actor.compute_state_features(obs, seat)  # (B,256)
  +        if STRICT_CHECKS and (not torch.isfinite(state_feat).all()):
  +            bad = state_feat[~torch.isfinite(state_feat)]
  +            raise RuntimeError(f"compute_loss: state_feat non-finite (count={int(bad.numel())})")
  +        # Precompute visible mask once and reuse
  +        hand_table = obs[:, :83]
  +        hand_mask = hand_table[:, :40] > 0.5
  +        table_mask = hand_table[:, 43:83] > 0.5
  +        captured = obs[:, 83:165]
  +        cap0_mask = captured[:, :40] > 0.5
  +        cap1_mask = captured[:, 40:80] > 0.5
  +        visible_mask_40 = (hand_mask | table_mask | cap0_mask | cap1_mask)
  +        # State projection e logits per carta
  +        state_proj = self.actor.compute_state_proj_from_state(state_feat, obs, visible_mask_40=visible_mask_40)  # (B,64)
  +        if STRICT_CHECKS and (not torch.isfinite(state_proj).all()):
  +            bad = state_proj[~torch.isfinite(state_proj)]
  +            raise RuntimeError(f"compute_loss: state_proj non-finite (count={int(bad.numel())})")
  +        card_logits_all = torch.matmul(state_proj, self.actor.card_emb_play.t())       # (B,40)
  +        # Guard extreme magnitudes before exp/softmax usage
  +        if STRICT_CHECKS and (not torch.isfinite(card_logits_all).all()):
  +            bad = card_logits_all[~torch.isfinite(card_logits_all)]
  +            raise RuntimeError(f"compute_loss: card_logits_all non-finite (count={int(bad.numel())})")
  +        max_abs_cl = float(card_logits_all.abs().max().item()) if card_logits_all.numel() > 0 else 0.0
  +        if max_abs_cl > 1e3:
  +            raise RuntimeError(f"compute_loss: card_logits_all magnitude too large (max_abs={max_abs_cl})")
  +        if max_cnt > 0:
  +            pos = torch.arange(max_cnt, device=device, dtype=torch.long)
  +            rel_pos_2d = pos.unsqueeze(0).expand(B, max_cnt)
  +            mask = rel_pos_2d < cnts.unsqueeze(1)
  +            abs_idx_2d = offs.unsqueeze(1) + rel_pos_2d
  +            abs_idx = abs_idx_2d[mask]
  +            sample_idx_per_legal = torch.arange(B, device=device, dtype=torch.long).unsqueeze(1).expand(B, max_cnt)[mask]
  +            legals_mb = legals[abs_idx].contiguous()                   # (M_mb,80)
  +            ones_per_row = legals_mb[:, :40].sum(dim=1)
  +            if STRICT_CHECKS:
  +                if not torch.allclose(ones_per_row, torch.ones_like(ones_per_row)):
  +                    raise RuntimeError("compute_loss: legals_mb must have exactly one played bit per row in [:40]")
  +            played_ids_mb = torch.argmax(legals_mb[:, :40], dim=1)     # (M_mb)
  +            # Card log-prob restricted to allowed set per sample (two-stage policy)
  +            # Build allowed mask per sample over the 40 cards
  +            allowed_mask = torch.zeros((B, 40), dtype=torch.bool, device=device)
  +            allowed_mask[sample_idx_per_legal, played_ids_mb] = True
  +            # LSE over allowed set per sample (out-of-place; avoid in-place masking + exp)
  +            neg_inf = torch.full_like(card_logits_all, float('-inf'))
  +            masked_logits = torch.where(allowed_mask, card_logits_all, neg_inf)
  +            lse_allowed = torch.logsumexp(masked_logits, dim=1)
  +            # chosen abs indices e played ids (evita pos_map grande)
  +            chosen_clamped = torch.minimum(chosen_idx, (cnts - 1).clamp_min(0))
  +            chosen_abs_idx = (offs + chosen_clamped)                                 # (B)
  +            # posizioni relative nella maschera (per-legal all'interno del proprio sample)
  +            pos_in_sample = rel_pos_2d[mask]                                         # (M_mb)
  +            chosen_abs_idx_per_legal = chosen_abs_idx[sample_idx_per_legal]          # (M_mb)
  +            match = (abs_idx == chosen_abs_idx_per_legal)                            # (M_mb)
  +            # Raccogli posizioni scelte per ciascun sample tramite index_copy
  +            chosen_pos = torch.full((B,), -1, dtype=torch.long, device=device)
  +            if bool(match.any()):
  +                chosen_pos_vals = pos_in_sample[match]
  +                chosen_pos_idx = sample_idx_per_legal[match]
  +                chosen_pos.index_copy_(0, chosen_pos_idx, chosen_pos_vals)
  +            if STRICT_CHECKS:
  +                has_neg = (chosen_pos < 0).any() if chosen_pos.numel() > 0 else torch.tensor(False, device=device)
  +                if bool(has_neg.detach().cpu().item()):
  +                    bad_rows = torch.nonzero(chosen_pos < 0, as_tuple=False).flatten().tolist()
  +                    raise RuntimeError(f"compute_loss: chosen_pos mapping failed for rows {bad_rows}")
  +            played_ids_all = torch.argmax(legals[:, :40], dim=1)
  +            chosen_card_ids = played_ids_all[chosen_abs_idx]
  +            logp_card = card_logits_all[row_idx, chosen_card_ids] - lse_allowed[row_idx]
  +            # capture logits per-legal via action embedding
  +            # Prefer precomputed global embeddings to avoid recomputation per minibatch
  +            if a_emb_global is not None:
  +                a_emb_mb = a_emb_global[abs_idx]
  +            else:
  +                # In training, avoid cached table to keep gradients flowing
  +                if self.actor.training:
  +                    a_emb_mb = self.actor.action_enc(legals_mb)
  +                else:
  +                    a_tbl = self.actor.get_action_emb_table_cached(device=legals_mb.device, dtype=state_proj.dtype)
  +                    a_emb_mb = torch.matmul(legals_mb, a_tbl)             # (M_mb,64)
  +            if STRICT_CHECKS and (not torch.isfinite(a_emb_mb).all()):
  +                bad = a_emb_mb[~torch.isfinite(a_emb_mb)]
  +                raise RuntimeError(f"compute_loss: a_emb_mb non-finite (count={int(bad.numel())})")
  +            cap_logits = (a_emb_mb * state_proj[sample_idx_per_legal]).sum(dim=1)
  +            if STRICT_CHECKS and (not torch.isfinite(cap_logits).all()):
  +                bad = cap_logits[~torch.isfinite(cap_logits)]
  +                raise RuntimeError(f"compute_loss: cap_logits non-finite (count={int(bad.numel())})")
  +            # segment logsumexp per gruppo (sample, card)
  +            group_ids = sample_idx_per_legal * 40 + played_ids_mb
  +            num_groups = B * 40
  +            group_max = torch.full((num_groups,), float('-inf'), dtype=cap_logits.dtype, device=device)
  +            group_max.scatter_reduce_(0, group_ids, cap_logits, reduce='amax', include_self=True)
  +            gmax_per_legal = group_max[group_ids]
  +            # Ensure dtype consistency under autocast (exp may return float32)
  +            exp_shifted = torch.exp(cap_logits - gmax_per_legal).to(cap_logits.dtype)
  +            group_sum = torch.zeros((num_groups,), dtype=cap_logits.dtype, device=device)
  +            group_sum.index_add_(0, group_ids, exp_shifted)
  +            lse_per_legal = gmax_per_legal + torch.log(torch.clamp_min(group_sum[group_ids], 1e-12))
  +            logp_cap_per_legal = cap_logits - lse_per_legal
  +            logp_cap = logp_cap_per_legal[chosen_pos]
  +            if STRICT_CHECKS and (not torch.isfinite(logp_card).all()):
  +                raise RuntimeError("compute_loss: logp_card non-finite")
  +            if STRICT_CHECKS and (not torch.isfinite(logp_cap).all()):
  +                raise RuntimeError("compute_loss: logp_cap non-finite")
  +            logp_new = logp_card + logp_cap
  +            if not torch.isfinite(logp_new).all():
  +                raise RuntimeError("compute_loss: logp_new non-finite")
  +            # Distribuzione completa sui legali per entropia/KL (computata solo se serve)
  +            need_entropy = (float(self.entropy_coef) > 0.0)
  +            if need_entropy:
  +                # Entropia per-sample sui soli sample con legali: H = -Σ p * log p
  +                logp_cards_allowed_per_legal = (card_logits_all[sample_idx_per_legal, played_ids_mb] - lse_allowed[sample_idx_per_legal])
  +                logp_total_per_legal = logp_cards_allowed_per_legal + logp_cap_per_legal
  +                probs = torch.exp(logp_total_per_legal)
  +                neg_p_logp = -(probs * logp_total_per_legal)
  +                ent_per_row = torch.zeros((B,), dtype=neg_p_logp.dtype, device=device)
  +                ent_per_row.index_add_(0, sample_idx_per_legal, neg_p_logp)
  +                valid_rows = torch.zeros((B,), dtype=torch.bool, device=device)
  +                valid_rows.index_fill_(0, sample_idx_per_legal.unique(), True)
  +                denom = valid_rows.to(ent_per_row.dtype).sum().clamp_min(1.0)
  +                entropy = (ent_per_row[valid_rows].sum() / denom)
  +            else:
  +                entropy = torch.tensor(0.0, device=device)
  +            if not torch.isfinite(entropy).all():
  +                raise RuntimeError("compute_loss: entropy non-finite")
  +        else:
  +            logp_new = torch.zeros((B,), device=device, dtype=state_proj.dtype)
  +            entropy = torch.tensor(0.0, device=device)
  +
  +        # Ratio: assert inputs finite and gaps not extreme; raise early with diagnostics
  +        if STRICT_CHECKS and (not torch.isfinite(logp_new).all()):
  +            raise RuntimeError("compute_loss: logp_new non-finite before ratio")
  +        if STRICT_CHECKS and (not torch.isfinite(old_logp).all()):
  +            raise RuntimeError("compute_loss: old_logp non-finite before ratio")
  +        diff = (logp_new - old_logp)
  +        if STRICT_CHECKS and (not torch.isfinite(diff).all()):
  +            raise RuntimeError("compute_loss: non-finite (logp_new - old_logp)")
  +        if diff.numel() > 0:
  +            max_gap = diff.abs().amax()
  +            torch._assert((max_gap <= 80.0), "compute_loss: log-prob gap too large; likely to overflow exp")
  +        ratio = torch.exp(diff)
  +        if STRICT_CHECKS and (not torch.isfinite(ratio).all()):
  +            raise RuntimeError("compute_loss: ratio non-finite (exp overflow)")
  +        clipped = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * adv
  +        # Maschera off-policy per step MCTS: quando presente mcts_weight>0, escludi la loss di policy
  +        mcts_w = batch.get('mcts_weight', None)
  +        if mcts_w is not None:
  +            if not torch.is_tensor(mcts_w):
  +                mcts_w = torch.as_tensor(mcts_w, dtype=torch.float32, device=device)
  +            else:
  +                mcts_w = mcts_w.to(device=device, dtype=torch.float32)
  +            # costruiamo un mask 0/1 allineato al minibatch corrente (B,)
  +            if mcts_w.dim() > 1:
  +                mcts_w = mcts_w.view(-1)
  +            # clamp and invert: 1.0 per step senza MCTS, 0.0 per step con MCTS
  +            mask_no_mcts = (mcts_w <= 0.0).to(torch.float32)
  +            # applica la maschera alla parte di policy; somma eps per evitare divisione 0
  +            ppo_term = torch.min(ratio * adv, clipped) * mask_no_mcts
  +            denom = mask_no_mcts.sum().clamp_min(1.0)
  +            loss_pi = -(ppo_term.sum() / denom)
  +        else:
  +            loss_pi = -(torch.min(ratio * adv, clipped)).mean()
  +        # Guard against non-finite intermediates spilling into the loss
  +        for name, tensor in (("ratio", ratio), ("adv", adv), ("ret", ret)):
  +            if STRICT_CHECKS and (not torch.isfinite(tensor).all()):
  +                raise RuntimeError(f"compute_loss: non-finite {name}")
  +
  +        # Passa others_hands (se disponibile) al critico per percorso CTDE opzionale
  +        v = self.critic.forward_from_state(state_feat, obs, batch.get('others_hands', None), visible_mask_40=visible_mask_40)
  +        if STRICT_CHECKS and (not torch.isfinite(v).all()):
  +            bad = v[~torch.isfinite(v)]
  +            raise RuntimeError(f"compute_loss: critic value non-finite (count={int(bad.numel())})")
  +        if self.value_clip is not None and self.value_clip > 0:
  +            v_clipped = torch.clamp(v, ret - self.value_clip, ret + self.value_clip)
  +            loss_v = torch.max((v - ret) ** 2, (v_clipped - ret) ** 2).mean()
  +        else:
  +            loss_v = nn.MSELoss()(v, ret)
  +
  +        # Schedula coefficienti (prima per evitare calcoli inutili)
  +        distill_coef_base = float(_os.environ.get('DISTILL_COEF', '0.1'))
  +        warm = int(_os.environ.get('DISTILL_WARMUP', '100'))
  +        coef = 0.0 if self.update_steps < warm else distill_coef_base
  +        belief_coef = float(_os.environ.get('BELIEF_AUX_COEF', '0.1'))
  +
  +        # Distillazione MCTS: costruisci target per-gruppo solo se necessario
  +        distill_loss = torch.tensor(0.0, device=device)
  +        # Loss ausiliaria per BeliefNet solo se necessario
  +        belief_aux = torch.tensor(0.0, device=device)
  +        if (coef > 0.0) and max_cnt > 0 and mcts_weight.numel() == B and (mcts_weight.sum() > 0) and mcts_policy_flat.numel() >= int(cnts.sum().item()):
  +            if STRICT_CHECKS and (not torch.isfinite(mcts_policy_flat).all()):
  +                raise RuntimeError("compute_loss: mcts_policy_flat non-finite")
  +            # Ricostruisci (B, max_cnt) target evitando loop Python: usa masked_scatter
  +            target = torch.zeros((B, max_cnt), device=device, dtype=torch.float32)
  +            valid_mask = torch.arange(max_cnt, device=device).unsqueeze(0).expand(B, max_cnt) < cnts.unsqueeze(1)
  +            flat_len = int(cnts.sum().item())
  +            target.masked_scatter_(valid_mask, mcts_policy_flat[:flat_len])
  +            # KL(pi_target || pi_actor) solo per posizioni con target>0 → niente 0 * (-inf)
  +            eps = 1e-8
  +            mask_pos = (target > 0)
  +            safe_log_t = torch.zeros_like(target)
  +            safe_log_t[mask_pos] = torch.log(torch.clamp(target[mask_pos], min=eps))
  +            # Costruisci sempre i log-prob totali per-legal paddati (usati dalla KL MCTS)
  +            logp_cards_allowed_per_legal = (card_logits_all[sample_idx_per_legal, played_ids_mb] - lse_allowed[sample_idx_per_legal])
  +            logp_total_per_legal = logp_cards_allowed_per_legal + logp_cap_per_legal
  +            logp_total_padded = torch.full((B, max_cnt), float('-inf'), dtype=cap_logits.dtype, device=device)
  +            logp_total_padded[mask] = logp_total_per_legal
  +            diff = safe_log_t - logp_total_padded
  +            # Avoid 0 * inf -> NaN: only accumulate where target > 0
  +            diff = torch.where(mask_pos, diff, torch.zeros_like(diff))
  +            kl_per_row = (target * diff).sum(dim=1)
  +            # Peso per incertezza calcolato sulle posizioni valide
  +            ent_row = torch.zeros_like(kl_per_row)
  +            if int(mask_pos.sum().item()) > 0:
  +                log_t_pos = torch.zeros_like(target)
  +                log_t_pos[mask_pos] = torch.log(torch.clamp(target[mask_pos], min=eps))
  +                ent_row = (-(target * log_t_pos).sum(dim=1))
  +            denom_h = torch.log(torch.clamp_min(cnts.to(torch.float32), 1.0))
  +            ent_norm = torch.where(denom_h > 0, ent_row / denom_h, torch.zeros_like(ent_row))
  +            w_unc = torch.clamp(ent_norm, 0.0, 1.0)
  +            # Pesa solo i sample con MCTS attivo
  +            w = (mcts_weight.clamp(0.0, 1.0) * w_unc)
  +            if w.sum() > 0:
  +                distill_loss = (kl_per_row * w).sum() / torch.clamp_min(w.sum(), 1.0)
  +            if STRICT_CHECKS and (not torch.isfinite(distill_loss).all()):
  +                raise RuntimeError("compute_loss: distill_loss non-finite")
  +        # Prepara target belief supervision (se batch fornisce mani reali degli altri)
  +        real_hands = batch.get('others_hands', None)  # shape (B,3,40) one-hot o multi-hot per altri giocatori
  +        if (belief_coef > 0.0) and (real_hands is not None):
  +            rh = to_cuda_nb(real_hands, torch.float32)
  +            # riusa state_feat già calcolato e la visible_mask_40 già costruita
  +            logits_b = self.actor.belief_net(state_feat)  # (B,120)
  +            if STRICT_CHECKS and (not torch.isfinite(logits_b).all()):
  +                raise RuntimeError("compute_loss: belief logits non-finite")
  +            visible_mask = visible_mask_40  # (B,40)
  +            Bsz = logits_b.size(0)
  +            logits_3x40 = logits_b.view(Bsz, 3, 40)
  +            # softmax over players dim
  +            log_probs = torch.log_softmax(logits_3x40, dim=1)
  +            # mask visible cards: zero their contribution, then average over unknown only
  +            m = (~visible_mask).to(log_probs.dtype).unsqueeze(1)  # True on unknown
  +            ce_per_card = -(rh * log_probs).sum(dim=1)  # (B,40)
  +            ce_per_card = ce_per_card * m.squeeze(1)
  +            denom = m.sum(dim=(1,2)).clamp_min(1.0)
  +            belief_aux = (ce_per_card.sum(dim=1) / denom).mean()
  +
  +        loss = loss_pi + self.value_coef * loss_v - self.entropy_coef * entropy + coef * distill_loss + belief_coef * belief_aux
  +        approx_kl = (old_logp - logp_new).mean()
  +        # Finite checks on core scalars
  +        for name, tensor in (
  +            ('loss', loss),
  +            ('loss_pi', loss_pi),
  +            ('loss_v', loss_v),
  +            ('entropy', entropy),
  +            ('approx_kl', approx_kl),
  +        ):
  +            if STRICT_CHECKS and (not torch.isfinite(tensor).all()):
  +                raise RuntimeError(f"compute_loss: non-finite {name}")
  +        # clip fraction reale: frazione di sample con |ratio-1| > clip_ratio
  +        if B > 0:
  +            clip_frac = (torch.abs(ratio - 1.0) > self.clip_ratio).float().mean()
  +        else:
  +            clip_frac = torch.tensor(0.0, device=device)
  +        # Restituisci TENSORS su device; conversione a float avverrà nel trainer in un'unica sync
  +        return loss, {
  +            'loss_pi': loss_pi.detach(),
  +            'loss_v': loss_v.detach(),
  +            'entropy': entropy.detach(),
  +            'approx_kl': approx_kl.detach(),
  +            'clip_frac': clip_frac.detach(),
  +            'distill_kl': distill_loss.detach(),
  +            'belief_aux': belief_aux.detach()
  +        }
  +
  +    def update(self, batch, epochs: int = 4, minibatch_size: int = 256):
  +        """
  +        Esegue update PPO con più epoche e minibatch sul batch corrente.
  +        Le azioni legali restano passate come array globale (ragged via offset/len per sample).
  +        """
  +        # Move models to training device (GPU for big compute if requested) just for the update
  +        models_were_cpu = (next(self.actor.parameters()).device.type == 'cpu') and (self.train_device.type != 'cpu')
  +        if models_were_cpu:
  +            self.actor.to(self.train_device)
  +            self.critic.to(self.train_device)
  +            # Move optimizer state to the same device as parameters to avoid CPU/GPU mismatch under fused Adam
  +            if self.train_device.type == 'cuda':
  +                for opt in (self.opt_actor, self.opt_critic):
  +                    for group in opt.param_groups:
  +                        for p in group['params']:
  +                            if p is None:
  +                                continue
  +                            state = opt.state.get(p)
  +                            if not state:
  +                                continue
  +                            for key, val in list(state.items()):
  +                                if torch.is_tensor(val) and val.device.type != self.train_device.type:
  +                                    state[key] = val.to(self.train_device)
  +        num_samples = len(batch['obs'])
  +        last_info = {}
  +        avg_kl_acc, avg_clip_acc, count_mb = 0.0, 0.0, 0
  +        early_stop = False
  +
  +        def _grad_norm(params):
  +            # Deprecated: we now use the value returned by clip_grad_norm_ to cut kernel launches
  +            total_sq = torch.zeros((), device=device)
  +            for p in params:
  +                if p.grad is not None:
  +                    total_sq = total_sq + p.grad.data.norm(2).pow(2)
  +            return total_sq.sqrt()
  +
  +        check_every = 8  # reduce CPU syncs for early-stop
  +        # Stage intero batch su device una sola volta (CPU-friendly: evita .to no-op)
  +        batch_cuda = {}
  +        for k, v in batch.items():
  +            if k in ('routing_log',):
  +                continue
  +            if torch.is_tensor(v):
  +                # mappa tensori principali su device
  +                dtype = v.dtype
  +                if k in ('obs', 'act', 'ret', 'adv'):
  +                    dtype = torch.float32
  +                if v.device.type == device.type and v.dtype == dtype:
  +                    batch_cuda[k] = v.detach()
  +                else:
  +                    batch_cuda[k] = v.detach().to(device=device, dtype=dtype, non_blocking=(device.type == 'cuda'))
  +            else:
  +                batch_cuda[k] = v
  +        # 'legals' è globale: porta su device una volta
  +        if 'legals' in batch_cuda:
  +            lv = batch_cuda['legals']
  +            if not (torch.is_tensor(lv) and lv.device.type == device.type and lv.dtype == torch.float32):
  +                batch_cuda['legals'] = lv.to(device=device, dtype=torch.float32, non_blocking=(device.type == 'cuda'))
  +            # Precompute global embeddings ONLY when not training to avoid graph reuse across backward
  +            # which leads to "Trying to backward through the graph a second time" errors.
  +            if not self.actor.training:
  +                try:
  +                    batch_cuda['a_emb_global'] = self.actor.action_enc(batch_cuda['legals']).detach()
  +                except Exception as e:
  +                    raise RuntimeError("Failed to precompute global action embeddings (a_emb_global)") from e
  +            else:
  +                batch_cuda['a_emb_global'] = None
  +
  +        # Stabilizza forme tra minibatch: usa massimo globale di legali e minibatch costante che divide il totale
  +        global_max_cnt = int(batch_cuda['legals_count'].max().item()) if num_samples > 0 else 0
  +        pos_global = (torch.arange(global_max_cnt, device=device, dtype=torch.long) if global_max_cnt > 0 else torch.zeros((0,), device=device, dtype=torch.long))
  +
  +        for ep in range(epochs):
  +            # Indici su device; scegli un minibatch_size effettivo che divida num_samples
  +            perm = torch.randperm(num_samples, device=device)
  +            mb_eff = int(minibatch_size)
  +            if num_samples > 0 and mb_eff > 0 and (num_samples % mb_eff) != 0:
  +                d = min(mb_eff, num_samples)
  +                found = False
  +                for k in range(d, 0, -1):
  +                    if (num_samples % k) == 0:
  +                        mb_eff = k
  +                        found = True
  +                        break
  +                if not found:
  +                    mb_eff = max(1, num_samples)
  +            for start in range(0, num_samples, mb_eff):
  +                idx_t = perm[start:start+mb_eff]
  +                # Slice direttamente su CUDA
  +                def sel_cuda(x):
  +                    return torch.index_select(x, 0, idx_t)
  +                # Seleziona sottovettori offs/cnts del minibatch (no fallback)
  +                offs_mb = sel_cuda(batch_cuda['legals_offset'])
  +                cnts_mb = sel_cuda(batch_cuda['legals_count'])
  +                B_mb = int(cnts_mb.size(0))
  +                # Fissa la dimensione al massimo globale per stabilità delle forme compilate
  +                max_cnt_mb = int(global_max_cnt)
  +                # Verifica presenza mcts_weight
  +                if batch_cuda.get('mcts_weight', None) is None:
  +                    raise RuntimeError('update: missing mcts_weight in batch')
  +                mcts_weight_mb = sel_cuda(batch_cuda['mcts_weight'])
  +                # Costruisci indice assoluto sui legali globali per estrarre la porzione di mcts_policy, solo se necessario
  +                if max_cnt_mb > 0 and bool((mcts_weight_mb > 0).any().item()):
  +                    if batch_cuda.get('mcts_policy', None) is None:
  +                        raise RuntimeError("update: 'mcts_policy' is required when mcts_weight>0 in minibatch")
  +                    if batch_cuda['mcts_policy'].dim() != 1:
  +                        raise RuntimeError('update: mcts_policy must be 1D flat vector')
  +                    if int(batch_cuda['mcts_policy'].numel()) < int(batch_cuda['legals'].size(0)):
  +                        raise RuntimeError('update: mcts_policy length smaller than total legals')
  +                    pos = pos_global
  +                    rel_pos_2d = (pos.unsqueeze(0).expand(B_mb, max_cnt_mb) if max_cnt_mb > 0 else torch.zeros((B_mb, 0), device=device, dtype=torch.long))
  +                    mask = rel_pos_2d < cnts_mb.unsqueeze(1)
  +                    abs_idx = (offs_mb.unsqueeze(1) + rel_pos_2d)[mask]
  +                    mcts_policy_mb = batch_cuda['mcts_policy'][abs_idx].contiguous()
  +                else:
  +                    mcts_policy_mb = torch.zeros((0,), dtype=torch.float32, device=device)
  +
  +                mini = {
  +                    'obs': sel_cuda(batch_cuda['obs']),
  +                    'act': sel_cuda(batch_cuda['act']),
  +                    'old_logp': sel_cuda(batch_cuda['old_logp']),
  +                    'ret': sel_cuda(batch_cuda['ret']),
  +                    'adv': sel_cuda(batch_cuda['adv']),
  +                    'legals': batch_cuda['legals'],  # globale su device
  +                    'legals_offset': offs_mb,
  +                    'legals_count': cnts_mb,
  +                    'chosen_index': sel_cuda(batch_cuda['chosen_index']),
  +                    'seat_team': sel_cuda(batch_cuda['seat_team']) if batch_cuda.get('seat_team', None) is not None else None,
  +                    'others_hands': sel_cuda(batch_cuda['others_hands']) if batch_cuda.get('others_hands', None) is not None else None,
  +                    'a_emb_global': batch_cuda.get('a_emb_global', None),
  +                    'mcts_policy': mcts_policy_mb,
  +                    'mcts_weight': mcts_weight_mb,
  +                }
  +                self.opt_actor.zero_grad(set_to_none=True)
  +                self.opt_critic.zero_grad(set_to_none=True)
  +                if self.scaler is not None:
  +                    with torch.autocast(device_type=self.train_device.type, dtype=autocast_dtype):
  +                        loss, info = self.compute_loss(mini)
  +                    self.scaler.scale(loss).backward()
  +                    # Unscale prima del grad clip
  +                    self.scaler.unscale_(self.opt_actor)
  +                    self.scaler.unscale_(self.opt_critic)
  +                    # grad norm (usa il valore restituito da clip_grad_norm_ per evitare un secondo pass)
  +                    gn_actor = nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
  +                    gn_critic = nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
  +                    if STRICT_CHECKS and ((not torch.isfinite(gn_actor)) or (not torch.isfinite(gn_critic))):
  +                        raise RuntimeError("update: non-finite gradients detected (norm)")
  +                    # garanzia tensor per logging coerente
  +                    gn_actor = torch.as_tensor(gn_actor, device=device, dtype=torch.float32)
  +                    gn_critic = torch.as_tensor(gn_critic, device=device, dtype=torch.float32)
  +                    self.scaler.step(self.opt_actor)
  +                    self.scaler.step(self.opt_critic)
  +                    self.scaler.update()
  +                    # Post-step: parametri finiti (cattura divergenze immediate)
  +
  +                    if hasattr(self.actor, 'state_enc') and hasattr(self.actor.state_enc, 'card_emb'):
  +                        if STRICT_CHECKS and (not torch.isfinite(self.actor.state_enc.card_emb).all()):
  +                            raise RuntimeError("update: state_enc.card_emb became non-finite after step")
  +                    # invalidate any inference caches after params changed
  +                    try:
  +                        self.actor.invalidate_action_cache()
  +                    except Exception as e:
  +                        raise RuntimeError('ppo.update.invalidate_action_cache_failed') from e
  +                else:
  +                    loss, info = self.compute_loss(mini)
  +                    loss.backward()
  +                    # grad norm (usa il valore restituito da clip_grad_norm_ per evitare un secondo pass)
  +                    gn_actor = nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)
  +                    gn_critic = nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)
  +                    if STRICT_CHECKS and ((not torch.isfinite(gn_actor)) or (not torch.isfinite(gn_critic))):
  +                        raise RuntimeError("update: non-finite gradients detected (norm)")
  +                    gn_actor = torch.as_tensor(gn_actor, device=device, dtype=torch.float32)
  +                    gn_critic = torch.as_tensor(gn_critic, device=device, dtype=torch.float32)
  +                    self.opt_actor.step()
  +                    self.opt_critic.step()
  +                    # Post-step: parametri finiti
  +                    try:
  +                        if hasattr(self.actor, 'state_enc') and hasattr(self.actor.state_enc, 'card_emb'):
  +                            if STRICT_CHECKS and (not torch.isfinite(self.actor.state_enc.card_emb).all()):
  +                                raise RuntimeError("update: state_enc.card_emb became non-finite after step")
  +                    except Exception:
  +                        raise
  +                    try:
  +                        self.actor.invalidate_action_cache()
  +                    except Exception as e:
  +                        raise RuntimeError('ppo.update.invalidate_action_cache_failed') from e
  +                last_info = info
  +                self.update_steps += 1
  +                avg_kl_acc += info.get('approx_kl', torch.tensor(0.0, device=device))
  +                avg_clip_acc += info.get('clip_frac', torch.tensor(0.0, device=device))
  +                count_mb += 1
  +                last_info['grad_norm_actor'] = gn_actor.detach()
  +                last_info['grad_norm_critic'] = gn_critic.detach()
  +                last_info['lr_actor'] = self.opt_actor.param_groups[0]['lr']
  +                last_info['lr_critic'] = self.opt_critic.param_groups[0]['lr']
  +                # early stop per target KL (controlla meno spesso per ridurre sync CPU)
  +                if (count_mb % check_every) == 0:
  +                    # Use averaged KL so far to decide early stop (smoother)
  +                    _kl = (avg_kl_acc / max(1, count_mb)).detach()
  +                    if bool((_kl > self.target_kl).item()):
  +                        self._high_kl_count += 1
  +                        early_stop = True
  +                        break
  +                    else:
  +                        self._high_kl_count = max(0, self._high_kl_count - 1)
  +            # Step any schedulers
  +            for sch in self._lr_schedulers:
  +                sch.step()
  +            # entropy schedule opzionale (clamp in range sicuro)
  +            if self._entropy_schedule is not None:
  +                try:
  +                    new_coef = float(self._entropy_schedule(self.update_steps))
  +                    self.entropy_coef = float(max(0.0, min(0.1, new_coef)))
  +                except Exception as e:
  +                    raise RuntimeError('ppo.update.entropy_schedule_failed') from e
  +            if early_stop:
  +                break
  +        # riduzione LR automatica quando KL alto ripetuto
  +        if self._high_kl_count >= self._high_kl_patience:
  +            for opt in (self.opt_actor, self.opt_critic):
  +                for g in opt.param_groups:
  +                    g['lr'] = g['lr'] * self._lr_decay_factor
  +            self._high_kl_count = 0
  +            last_info['lr_reduced'] = True
  +        # medie su minibatch
  +        if count_mb > 0:
  +            last_info['avg_kl'] = (avg_kl_acc / count_mb).detach()
  +            last_info['avg_clip_frac'] = (avg_clip_acc / count_mb).detach()
  +            last_info['early_stop'] = torch.tensor(1.0 if early_stop else 0.0, device=device)
  +        # Move back to CPU for collection if we temporarily moved to CUDA
  +        if models_were_cpu:
  +            self.actor.to('cpu')
  +            self.critic.to('cpu')
  +            # Move optimizer state back to CPU to match parameters for next collection iteration
  +            if self.train_device.type == 'cuda':
  +                for opt in (self.opt_actor, self.opt_critic):
  +                    for group in opt.param_groups:
  +                        for p in group['params']:
  +                            if p is None:
  +                                continue
  +                            state = opt.state.get(p)
  +                            if not state:
  +                                continue
  +                            for key, val in list(state.items()):
  +                                if torch.is_tensor(val) and val.device.type != 'cpu':
  +                                    state[key] = val.to('cpu')
  +        return last_info
  +
  +    def add_lr_schedulers(self, actor_scheduler, critic_scheduler):
  +        self._lr_schedulers = [sch for sch in [actor_scheduler, critic_scheduler] if sch is not None]
  +
  +    def set_entropy_schedule(self, schedule_fn):
  +        """Imposta una funzione schedule_fn(step)->entropy_coef."""
  +        self._entropy_schedule = schedule_fn
  +
  +    def save(self, path: str):
  +        torch.save({
  +            'actor': self.actor.state_dict(),
  +            'critic': self.critic.state_dict(),
  +            'opt_actor': self.opt_actor.state_dict(),
  +            'opt_critic': self.opt_critic.state_dict(),
  +            'run_config': self.run_config,
  +            'update_steps': self.update_steps,
  +        }, path)
  +
  +    def load(self, path: str, map_location=None):
  +        ckpt = torch.load(path, map_location=map_location or device)
  +        # Support multiple checkpoint formats:
  +        # 1) Full PPO checkpoint: {'actor','critic','opt_actor','opt_critic',...}
  +        # 2) Actor-only checkpoint: {'actor', ...}
  +        # 3) Raw actor state_dict (plain mapping)
  +        if isinstance(ckpt, dict) and ('actor' in ckpt or 'critic' in ckpt or 'opt_actor' in ckpt or 'run_config' in ckpt):
  +            # Load actor (required in this branch)
  +            if 'actor' in ckpt:
  +                self.actor.load_state_dict(ckpt['actor'])
  +            else:
  +                # Some older actor-only checkpoints may have saved the actor dict at top-level (handled below)
  +                pass
  +            # Load critic if present; otherwise keep randomly initialized critic
  +            if 'critic' in ckpt:
  +                self.critic.load_state_dict(ckpt['critic'])
  +            # Load optimizers only if both are present (skip safely otherwise)
  +            if ('opt_actor' in ckpt) and ('opt_critic' in ckpt):
  +                try:
  +                    self.opt_actor.load_state_dict(ckpt['opt_actor'])
  +                    self.opt_critic.load_state_dict(ckpt['opt_critic'])
  +                except Exception as e:
  +                    raise RuntimeError('ppo.load.optim_state_load_failed') from e
  +            # Restore metadata if present
  +            self.run_config = ckpt.get('run_config', self.run_config)
  +            self.update_steps = ckpt.get('update_steps', 0)
  +        else:
  +            # Fallback: attempt to treat checkpoint as a raw actor state_dict
  +            # This covers files like bootstrap_random.pth with only actor weights.
  +            try:
  +                self.actor.load_state_dict(ckpt)
  +            except Exception as e:
  +                raise RuntimeError('ppo.load.unsupported_checkpoint_format') from e
  +            # Keep critic/optimizers as freshly initialized
  +            # Metadata not available in this format
diff --cc benchmark.py
index 7e67ddf,7e67ddf,0000000..c985b8b
mode 100644,100644,000000..100644
--- a/benchmark.py
+++ b/benchmark.py
@@@@ -1,934 -1,934 -1,0 +1,851 @@@@
  +#!/usr/bin/env python3
  +"""
  +Checkpoint Benchmark Script for Scopone AI
  +
  +This script compares different Team 0 checkpoints of the Scopone AI model by having them
  +play against each other to benchmark their performance.
  +
  +Usage:
  +  python benchmark_team0.py --checkpoint_dir checkpoints/ --games 1000
  +  python benchmark_team0.py --checkpoints checkpoints/model_team0_ep5000.pth checkpoints/model_team0_ep10000.pth
  +"""
  +
  +import torch
  +import os
  +# Default compile-friendly settings for CLI run
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE', '0')
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE_MODE', 'max-autotune')
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE_BACKEND', 'inductor')
  +os.environ.setdefault('SCOPONE_INDUCTOR_AUTOTUNE', '1')
+++os.environ.setdefault('SCOPONE_BELIEF_BLEND_ALPHA', '0.65')
  +import argparse
  +import time
  +import re
  +from tqdm import tqdm
  +from collections import defaultdict
  +import itertools
  +import glob
  +import pandas as pd
  +from openpyxl import Workbook
  +from openpyxl.styles import PatternFill, Border, Side, Alignment, Font
  +from openpyxl.utils.dataframe import dataframe_to_rows
  +import openpyxl
  +
  +# Import the required components from the existing code
  +from environment import ScoponeEnvMA
+++from belief import sample_determinization
  +from algorithms.is_mcts import run_is_mcts
  +from models.action_conditioned import ActionConditionedActor, CentralValueNet
  +from utils.compile import maybe_compile_module
  +import torch
  + 
  +
  +# Device selection with overrides
  +device = torch.device(os.environ.get(
  +    'SCOPONE_DEVICE',
  +    ('cuda' if torch.cuda.is_available() and os.environ.get('TESTS_FORCE_CPU') != '1' else 'cpu')
  +))
  +print(f"Using device: {device}")
  +
  +# Regole di default per l'ambiente (modalità standard senza varianti)
  +# Nota: la variante "asso_piglia_tutto" è disattivata e quindi ignorata in questo script di benchmark.
  +DEFAULT_RULES = {
  +    'start_with_4_on_table': False,
  +    'asso_piglia_tutto': False,
  +    'scopa_on_asso_piglia_tutto': False,
  +    'asso_piglia_tutto_posabile': False,
  +    'asso_piglia_tutto_posabile_only_empty': False,
  +    'scopa_on_last_capture': False,
  +    're_bello': False,
  +    'napola': False,
  +    'napola_scoring': 'fixed3',
  +    'max_consecutive_scope': None,
  +    'last_cards_to_dealer': True,
  +}
  +
  +def load_actor_critic(ckpt_path: str):
  +    actor = ActionConditionedActor()
  +    critic = CentralValueNet()
  +    # Enable compiled versions when requested (works on CPU with inductor)
  +    actor = maybe_compile_module(actor, name='ActionConditionedActor[benchmark]')
  +    critic = maybe_compile_module(critic, name='CentralValueNet[benchmark]')
  +    ckpt = torch.load(ckpt_path, map_location=device)
  +    # se si usa algorithms/ppo_ac save
  +    if 'actor' in ckpt and 'critic' in ckpt:
  +        actor.load_state_dict(ckpt['actor'])
  +        critic.load_state_dict(ckpt['critic'])
  +    else:
  +        raise RuntimeError('No weights in checkpoint')
  +    return actor, critic
  +
  +def play_game(actor1, actor2, starting_player=0, use_mcts=False, sims=128, dets=16):
  +    """
  +    Play a single game between two Team 0 agents.
  +    
  +    Args:
  +        agent1: First agent
  +        agent2: Second agent
  +        starting_player: Which player starts (0-3)
  +    
  +    Returns:
  +        winner: 0 if agent1 won, 1 if agent2 won, -1 if draw
  +        team_scores: Scores for each agent [agent1_score, agent2_score]
  +        game_length: Number of moves in the game
  +    """
  +    env = ScoponeEnvMA(rules=DEFAULT_RULES)
  +    env.current_player = starting_player
  +    
  +    done = False
  +    info = {}
  +    
  +    # Track which actor controls which seats
  +    actor1_positions = [0, 2]  # Team 0 positions
  +    actor2_positions = [1, 3]  # Team 1 positions
  +    
  +    # Game loop
  +    while not done:
  +        current_player = env.current_player
  +        
  +        # Get valid actions
  +        valid_actions = env.get_valid_actions()
  +        
  +        if not valid_actions:
  +            print("\n[ERROR] No valid actions available!")
  +            break
  +        
  +        # Get observation for current player
  +        obs = env._get_observation(current_player)
  +        
  +        # Select action (with optional IS-MCTS if available)
  +        if use_mcts:
  +            # Use the corresponding loaded actor; critic can be a lightweight instance for value
  +            actor = actor1 if current_player in actor1_positions else actor2
  +            critic = maybe_compile_module(CentralValueNet(), name='CentralValueNet[bench_mcts]')
  +            def policy_fn(o, leg):
  +                o_t = torch.tensor(o, dtype=torch.float32, device=device)
  +                if len(leg) > 0 and torch.is_tensor(leg[0]):
  +                    leg_t = torch.stack(leg).to(device=device, dtype=torch.float32)
  +                else:
  +                    leg_t = torch.stack([
  +                        x if torch.is_tensor(x) else torch.tensor(x, dtype=torch.float32, device=device)
  +                    for x in leg], dim=0)
  +                with torch.no_grad():
  +                    # seat/team vec aligned to current env seat
  +                    s = torch.zeros(6, dtype=torch.float32, device=device)
  +                    cp = env.current_player
  +                    s[cp] = 1.0
  +                    s[4] = 1.0 if cp in [0, 2] else 0.0
  +                    s[5] = 1.0 if cp in [1, 3] else 0.0
  +                    logits = actor(o_t.unsqueeze(0), leg_t, s.unsqueeze(0))
  +                probs = torch.softmax(logits, dim=0)
  +                return probs
  +            def value_fn(o, _env=None):
  +                o_t = o.clone().detach().to(device=device, dtype=torch.float32) if torch.is_tensor(o) else torch.tensor(o, dtype=torch.float32, device=device)
  +                # build seat vector if env available
  +                if _env is not None:
  +                    cp = _env.current_player
  +                    s = torch.zeros(6, dtype=torch.float32, device=device)
  +                    s[cp] = 1.0
  +                    s[4] = 1.0 if cp in [0, 2] else 0.0
  +                    s[5] = 1.0 if cp in [1, 3] else 0.0
  +                else:
  +                    s = torch.zeros(6, dtype=torch.float32, device=device)
  +                with torch.no_grad():
  +                    return critic(o_t.unsqueeze(0), s.unsqueeze(0)).item()
  +            # Belief sampler neurale: determinizza le mani avversarie dai margini del BeliefNet
  +            def belief_sampler_neural(_env):
--                 cp = _env.current_player
--                 obs_cur = _env._get_observation(cp)
--                 o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
--                 if torch.is_tensor(o_cpu):
--                     o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
--                 # seat/team vec: 6-dim
--                 s_cpu = torch.zeros(6, dtype=torch.float32)
--                 s_cpu[cp] = 1.0
--                 s_cpu[4] = 1.0 if cp in [0, 2] else 0.0
--                 s_cpu[5] = 1.0 if cp in [1, 3] else 0.0
--                 if device.type == 'cuda':
--                     o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                     s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                 else:
--                     o_t = o_cpu.unsqueeze(0).to(device=device)
--                     s_t = s_cpu.unsqueeze(0).to(device=device)
--                 with torch.no_grad():
--                     state_feat = actor.state_enc(o_t, s_t)
--                     logits = actor.belief_net(state_feat)
--                     hand_table = o_t[:, :83]
--                     hand_mask = hand_table[:, :40] > 0.5
--                     table_mask = hand_table[:, 43:83] > 0.5
--                     captured = o_t[:, 83:165]
--                     cap0_mask = captured[:, :40] > 0.5
--                     cap1_mask = captured[:, 40:80] > 0.5
--                     visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
--                     probs_flat = actor.belief_net.probs(logits, visible_mask)
--                 probs = probs_flat.view(3, 40).detach().cpu().numpy()
--                 vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
--                 unknown_ids = [cid for cid in range(40) if not vis[cid]]
--                 others = [(cp + 1) % 4, (cp + 2) % 4, (cp + 3) % 4]
--                 det = {pid: [] for pid in others}
--                 counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
--                 caps = [int(counts.get(pid, 0)) for pid in others]
--                 n = len(unknown_ids)
--                 if sum(caps) != n:
--                     caps[2] = max(0, n - caps[0] - caps[1])
--                 for cid in unknown_ids:
--                     pc = probs[:, cid]
--                     s = pc.sum()
--                     ps = pc / (s if s > 0 else 1e-9)
--                     j = int(torch.argmax(torch.tensor(ps)).item())
--                     if caps[j] > 0:
--                         det[others[j]].append(cid)
--                         caps[j] -= 1
--                 return det
+++                alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
+++                noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
+++                return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
  +            action = run_is_mcts(env, policy_fn, value_fn, num_simulations=sims, c_puct=1.0, belief=None, num_determinization=dets,
  +                                    belief_sampler=belief_sampler_neural)
  +        else:
  +            # Greedy selection by actor scoring
  +            with torch.no_grad():
  +                o_t = obs.clone().detach().to(device=device, dtype=torch.float32) if torch.is_tensor(obs) else torch.tensor(obs, dtype=torch.float32, device=device)
  +                leg_t = torch.stack([
  +                    x if torch.is_tensor(x) else torch.tensor(x, dtype=torch.float32, device=device)
  +                for x in valid_actions], dim=0)
  +                s = torch.zeros(6, dtype=torch.float32, device=device)
  +                s[current_player] = 1.0
  +                s[4] = 1.0 if current_player in [0, 2] else 0.0
  +                s[5] = 1.0 if current_player in [1, 3] else 0.0
  +                actor = actor1 if current_player in actor1_positions else actor2
  +                logits = actor(o_t.unsqueeze(0), leg_t, s.unsqueeze(0))
  +                idx = torch.argmax(logits).item()
  +            action = valid_actions[idx]
  +
  +        # Take step in environment
  +        next_obs, reward, done, info = env.step(action)
  +
  +    # Extract final score information from team_rewards
  +    agent1_score = 0.0
  +    agent2_score = 0.0
  +    if "team_rewards" in info:
  +        team_rewards = info["team_rewards"]
  +        agent1_score = team_rewards[0]  # Team 0 score for agent1
  +        agent2_score = team_rewards[1]  # Team 1 score for agent2
  +    
  +    # Determine winner based on team scores
  +    winner = 0 if agent1_score > agent2_score else 1 if agent2_score > agent1_score else -1
  +    
  +    return winner, [agent1_score, agent2_score], len(env.game_state["history"])
  +
  +def load_actor_critic(ckpt_path: str):
  +    actor = ActionConditionedActor()
  +    critic = CentralValueNet()
  +    # Enable compiled versions when requested (works on CPU with inductor)
  +    actor = maybe_compile_module(actor, name='ActionConditionedActor[benchmark]')
  +    critic = maybe_compile_module(critic, name='CentralValueNet[benchmark]')
  +    ckpt = torch.load(ckpt_path, map_location=device)
  +    # se si usa algorithms/ppo_ac save
  +    if 'actor' in ckpt and 'critic' in ckpt:
  +        actor.load_state_dict(ckpt['actor'])
  +        critic.load_state_dict(ckpt['critic'])
  +    else:
  +        raise RuntimeError('No weights in checkpoint')
  +    return actor, critic
  +
  +
  +def main_cli():
  +    import argparse
  +    parser = argparse.ArgumentParser(description='Benchmark Scopone with optional IS-MCTS')
  +    parser.add_argument('--mcts', action='store_true', help='Use IS-MCTS booster')
  +    parser.add_argument('--sims', type=int, default=128, help='Number of MCTS simulations')
  +    parser.add_argument('--dets', type=int, default=16, help='Number of belief determinisations per search')
  +    parser.add_argument('--compact', action='store_true', help='Use compact observation')
  +    parser.add_argument('--k-history', type=int, default=12, help='Recent moves for compact observation')
  +    parser.add_argument('--ckpt', type=str, default='', help='Checkpoint path for actor/critic (optional)')
  +    parser.add_argument('--games', type=int, default=10, help='Number of games to play')
  +    args = parser.parse_args()
  +    # Placeholder for agent loading and running a quick game
  +    actor, critic = load_actor_critic(args.ckpt) if args.ckpt else (maybe_compile_module(ActionConditionedActor(), name='ActionConditionedActor[benchmark]'),
  +                                                                     maybe_compile_module(CentralValueNet(), name='CentralValueNet[benchmark]'))
  +    for g in range(args.games):
  +        env = ScoponeEnvMA(k_history=args.k_history)
  +        done = False
  +        while not done:
  +            obs = env._get_observation(env.current_player)
  +            legals = env.get_valid_actions()
  +            if not legals:
  +                break
  +            if args.mcts:
  +                def policy_fn(o, leg):
  +                    o_t = o.clone().detach().to(device=device, dtype=torch.float32) if torch.is_tensor(o) else torch.tensor(o, dtype=torch.float32, device=device)
  +                    leg_t = torch.stack([
  +                        x if torch.is_tensor(x) else torch.tensor(x, dtype=torch.float32, device=device)
  +                    for x in legals], dim=0)
  +                    with torch.no_grad():
  +                        logits = actor(o_t.unsqueeze(0), leg_t)
  +                    return torch.softmax(logits, dim=0)
  +                def value_fn(o):
  +                    o_t = torch.tensor(o, dtype=torch.float32, device=device)
  +                    # derive a simple seat vector from env state is not available here; use zeros
  +                    s = torch.zeros(6, dtype=torch.float32, device=device)
  +                    with torch.no_grad():
  +                        return critic(o_t.unsqueeze(0), s.unsqueeze(0)).item()
  +                def belief_sampler_neural(_env):
--                     cp = _env.current_player
--                     obs_cur = _env._get_observation(cp)
--                     o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
--                     if torch.is_tensor(o_cpu):
--                         o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
--                     s_cpu = torch.zeros(6, dtype=torch.float32)
--                     s_cpu[cp] = 1.0
--                     s_cpu[4] = 1.0 if cp in [0, 2] else 0.0
--                     s_cpu[5] = 1.0 if cp in [1, 3] else 0.0
--                     if device.type == 'cuda':
--                         o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                         s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                     else:
--                         o_t = o_cpu.unsqueeze(0).to(device=device)
--                         s_t = s_cpu.unsqueeze(0).to(device=device)
--                     with torch.no_grad():
--                         state_feat = actor.state_enc(o_t, s_t)
--                         logits = actor.belief_net(state_feat)
--                         hand_table = o_t[:, :83]
--                         hand_mask = hand_table[:, :40] > 0.5
--                         table_mask = hand_table[:, 43:83] > 0.5
--                         captured = o_t[:, 83:165]
--                         cap0_mask = captured[:, :40] > 0.5
--                         cap1_mask = captured[:, 40:80] > 0.5
--                         visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
--                         probs_flat = actor.belief_net.probs(logits, visible_mask)
--                     probs = probs_flat.view(3, 40).detach().cpu().numpy()
--                     vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
--                     unknown_ids = [cid for cid in range(40) if not vis[cid]]
--                     others = [(cp + 1) % 4, (cp + 2) % 4, (cp + 3) % 4]
--                     det = {pid: [] for pid in others}
--                     counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
--                     caps = [int(counts.get(pid, 0)) for pid in others]
--                     n = len(unknown_ids)
--                     if sum(caps) != n:
--                         caps[2] = max(0, n - caps[0] - caps[1])
--                     for cid in unknown_ids:
--                         pc = probs[:, cid]
--                         s = pc.sum()
--                         ps = pc / (s if s > 0 else 1e-9)
--                         j = int(torch.argmax(torch.tensor(ps)).item())
--                         if caps[j] > 0:
--                             det[others[j]].append(cid)
--                             caps[j] -= 1
--                     return det
+++                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
+++                    noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
+++                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
  +                action = run_is_mcts(env, policy_fn, value_fn, num_simulations=args.sims, c_puct=1.0, belief=None, num_determinization=args.dets,
  +                                      belief_sampler=belief_sampler_neural)
  +            else:
  +                # pick best by actor scoring
  +                with torch.no_grad():
  +                    o_t = obs.clone().detach().to(device=device, dtype=torch.float32) if torch.is_tensor(obs) else torch.tensor(obs, dtype=torch.float32, device=device)
  +                    leg_t = torch.stack([
  +                        x if torch.is_tensor(x) else torch.tensor(x, dtype=torch.float32, device=device)
  +                    for x in legals], dim=0)
  +                    logits = actor(o_t.unsqueeze(0), leg_t)
  +                    idx = torch.argmax(logits).item()
  +                action = legals[idx]
  +            _, _, done, _ = env.step(action)
  +    print('Benchmark completed.')
  +
  +def find_checkpoints(checkpoint_dir, pattern="*team0*ep*.pth"):
  +    """Find Team 0 checkpoint files in the specified directory."""
  +    if os.path.isfile(checkpoint_dir):
  +        return [checkpoint_dir]
  +    
  +    checkpoint_pattern = os.path.join(checkpoint_dir, pattern)
  +    checkpoint_files = glob.glob(checkpoint_pattern)
  +    
  +    # Sort checkpoints by episode number when possible
  +    try:
  +        checkpoint_files.sort(key=lambda x: int(x.split('_ep')[1].split('.pth')[0]) 
  +                             if '_ep' in x else float('inf'))
  +    except Exception as e:
  +        raise RuntimeError('Failed to sort checkpoint files by episode number') from e
  +    
  +    return checkpoint_files
  +
  +def extract_episode_number(checkpoint_path):
  +    """Extract episode number from checkpoint filename."""
  +    # Try to find episode number using regex
  +    match = re.search(r'_ep(\d+)', os.path.basename(checkpoint_path))
  +    if match:
  +        return int(match.group(1))
  +    
  +    # If not found, try to extract any number from the filename
  +    match = re.search(r'(\d+)', os.path.basename(checkpoint_path))
  +    if match:
  +        return int(match.group(1))
  +    
  +    # If still no number, return a very large number to place at the end
  +    return float('inf')
  +
  +def evaluate_checkpoints(checkpoint_paths, num_games=10000):
  +    """Evaluate Team 0 checkpoints against each other in head-to-head matches."""
  +    results = {}
  +    
  +    # Load all models (actor/critic)
  +    models = {}
  +    for path in checkpoint_paths:
  +        # Extract episode number for better naming
  +        episode_num = extract_episode_number(path)
  +        name = f"ep{episode_num}" if episode_num != float('inf') else os.path.basename(path).replace(".pth", "")
  +        
  +        print(f"Loading actor/critic from checkpoint: {path} as {name}")
  +        models[name] = load_actor_critic(path)
  +    
  +    # Play games between all pairs of agents (each pair plays only once)
  +    matchups = list(itertools.combinations(models.keys(), 2))
  +    
  +    for agent1_name, agent2_name in matchups:
  +        print(f"\nEvaluating: {agent1_name} vs {agent2_name}")
  +        actor1, critic1 = models[agent1_name]
  +        actor2, critic2 = models[agent2_name]
  +        
  +        matchup_key = f"{agent1_name}_vs_{agent2_name}"
  +        
  +        results[matchup_key] = {
  +            "games": num_games,
  +            "agent1_wins": 0,
  +            "agent2_wins": 0,
  +            "draws": 0,
  +            "agent1_score_total": 0,
  +            "agent2_score_total": 0,
  +            "agent1_score_distribution": defaultdict(int),
  +            "agent2_score_distribution": defaultdict(int),
  +            "first_starter_wins": 0,
  +            "game_lengths": []
  +        }
  +        
  +        # Progress bar
  +        pbar = tqdm(total=num_games)
  +        
  +        # Play num_games games with rotation of starting player
  +        for game_idx in range(num_games):
  +            # Rotate starting player every game
  +            starting_player = game_idx % 4
  +            
  +            # Play game
  +            winner, scores, game_length = play_game(actor1, actor2, starting_player)
  +            
  +            # Record results
  +            agent1_score, agent2_score = scores
  +            
  +            if winner == 0:  # Agent 1 won
  +                results[matchup_key]["agent1_wins"] += 1
  +                # Check if the starting team won
  +                if starting_player in [0, 2]:  # Team 0 started
  +                    results[matchup_key]["first_starter_wins"] += 1
  +            elif winner == 1:  # Agent 2 won
  +                results[matchup_key]["agent2_wins"] += 1
  +                # Check if the starting team won
  +                if starting_player in [1, 3]:  # Team 1 started
  +                    results[matchup_key]["first_starter_wins"] += 1
  +            else:
  +                results[matchup_key]["draws"] += 1
  +            
  +            # Record scores
  +            results[matchup_key]["agent1_score_total"] += agent1_score
  +            results[matchup_key]["agent2_score_total"] += agent2_score
  +            
  +            # Update score distributions
  +            results[matchup_key]["agent1_score_distribution"][agent1_score] += 1
  +            results[matchup_key]["agent2_score_distribution"][agent2_score] += 1
  +            
  +            # Record game length
  +            results[matchup_key]["game_lengths"].append(game_length)
  +            
  +            # Calculate win rates for progress bar
  +            agent1_wins = results[matchup_key]["agent1_wins"]
  +            agent2_wins = results[matchup_key]["agent2_wins"]
  +            win_percentage1 = (agent1_wins / (game_idx + 1)) * 100
  +            win_percentage2 = (agent2_wins / (game_idx + 1)) * 100
  +            
  +            pbar.update(1)
  +            pbar.set_description(
  +                f"{agent1_name}: {agent1_wins} ({win_percentage1:.1f}%), "
  +                f"{agent2_name}: {agent2_wins} ({win_percentage2:.1f}%)"
  +            )
  +        
  +        pbar.close()
  +        
  +        # Calculate average scores
  +        results[matchup_key]["agent1_avg_score"] = results[matchup_key]["agent1_score_total"] / num_games
  +        results[matchup_key]["agent2_avg_score"] = results[matchup_key]["agent2_score_total"] / num_games
  +        
  +        # Calculate detailed win percentages
  +        agent1_total_wins = results[matchup_key]["agent1_wins"]
  +        agent2_total_wins = results[matchup_key]["agent2_wins"]
  +        total_decided_games = agent1_total_wins + agent2_total_wins
  +        
  +        # Calculate first-starter advantage
  +        first_starter_wins = results[matchup_key]["first_starter_wins"] 
  +        first_starter_advantage = first_starter_wins / total_decided_games if total_decided_games > 0 else 0.5
  +        
  +        agent1_win_pct = agent1_total_wins / num_games * 100
  +        agent2_win_pct = agent2_total_wins / num_games * 100
  +        draw_pct = results[matchup_key]["draws"] / num_games * 100
  +        
  +        # Print detailed summary
  +        print(f"\nResults for {agent1_name} vs {agent2_name}:")
  +        print(f"  {agent1_name} wins: {agent1_total_wins} ({agent1_win_pct:.1f}%)")
  +        print(f"  {agent2_name} wins: {agent2_total_wins} ({agent2_win_pct:.1f}%)")
  +        print(f"  Draws: {results[matchup_key]['draws']} ({draw_pct:.1f}%)")
  +        print(f"  First-starter advantage: {first_starter_advantage:.2f} (1.0 = 100% advantage, 0.5 = no advantage)")
  +        print(f"  Average score {agent1_name}: {results[matchup_key]['agent1_avg_score']:.2f}")
  +        print(f"  Average score {agent2_name}: {results[matchup_key]['agent2_avg_score']:.2f}")
  +        print(f"  Average game length: {sum(results[matchup_key]['game_lengths'])/len(results[matchup_key]['game_lengths']):.1f} moves")
  +    
  +    return results
  +
  +def generate_excel_comparison(checkpoint_paths, results, output_file):
  +    """
  +    Generate an Excel file with comparative results between models.
  +    Each metric has its own dedicated sheet.
  +    """
  +    # Extract checkpoint names and episode numbers
  +    checkpoints_info = []
  +    for path in checkpoint_paths:
  +        episode = extract_episode_number(path)
  +        name = f"ep{episode}" if episode != float('inf') else os.path.basename(path).replace(".pth", "")
  +        checkpoints_info.append((name, episode, path))
  +    
  +    # Sort checkpoints by episode number
  +    checkpoints_info.sort(key=lambda x: x[1])
  +    
  +    # Get model names
  +    model_names = [info[0] for info in checkpoints_info]
  +    
  +    # Create DataFrames for different metrics
  +    win_rate_data = pd.DataFrame(index=model_names, columns=model_names)
  +    score_diff_data = pd.DataFrame(index=model_names, columns=model_names)
  +    starter_advantage_data = pd.DataFrame(index=model_names, columns=model_names)
  +    game_length_data = pd.DataFrame(index=model_names, columns=model_names)
  +    draws_data = pd.DataFrame(index=model_names, columns=model_names)
  +    
  +    # Initialize DataFrames with dash in diagonal
  +    for i, model1 in enumerate(model_names):
  +        for j, model2 in enumerate(model_names):
  +            if i == j:
  +                win_rate_data.loc[model1, model2] = "—"
  +                score_diff_data.loc[model1, model2] = "—"
  +                starter_advantage_data.loc[model1, model2] = "—"
  +                game_length_data.loc[model1, model2] = "—"
  +                draws_data.loc[model1, model2] = "—"
  +            else:
  +                win_rate_data.loc[model1, model2] = "N/A"
  +                score_diff_data.loc[model1, model2] = "N/A"
  +                starter_advantage_data.loc[model1, model2] = "N/A"
  +                game_length_data.loc[model1, model2] = "N/A"
  +                draws_data.loc[model1, model2] = "N/A"
  +    
  +    # Fill in the metric matrices
  +    for matchup_key, data in results.items():
  +        agents = matchup_key.split("_vs_")
  +        model1, model2 = agents
  +        
  +        # Win rates
  +        model1_win_rate = data['agent1_wins'] / data['games'] * 100
  +        model2_win_rate = data['agent2_wins'] / data['games'] * 100
  +        win_rate_data.loc[model1, model2] = f"{model1_win_rate:.1f}%"
  +        win_rate_data.loc[model2, model1] = f"{model2_win_rate:.1f}%"
  +        
  +        # Score differences
  +        score_diff = data['agent1_avg_score'] - data['agent2_avg_score']
  +        score_diff_data.loc[model1, model2] = f"{score_diff:.2f}"
  +        score_diff_data.loc[model2, model1] = f"{-score_diff:.2f}"
  +        
  +        # First-starter advantage
  +        total_decided_games = data['agent1_wins'] + data['agent2_wins']
  +        if total_decided_games > 0:
  +            first_starter_advantage = data['first_starter_wins'] / total_decided_games
  +            starter_advantage_data.loc[model1, model2] = f"{first_starter_advantage:.2f}"
  +            starter_advantage_data.loc[model2, model1] = f"{first_starter_advantage:.2f}"  # Same value for both directions
  +        else:
  +            starter_advantage_data.loc[model1, model2] = "N/A"
  +            starter_advantage_data.loc[model2, model1] = "N/A"
  +        
  +        # Game length
  +        avg_game_length = sum(data['game_lengths']) / len(data['game_lengths'])
  +        game_length_data.loc[model1, model2] = f"{avg_game_length:.1f}"
  +        game_length_data.loc[model2, model1] = f"{avg_game_length:.1f}"  # Same value for both directions
  +        
  +        # Draws percentage
  +        draw_pct = data['draws'] / data['games'] * 100
  +        draws_data.loc[model1, model2] = f"{draw_pct:.1f}%"
  +        draws_data.loc[model2, model1] = f"{draw_pct:.1f}%"  # Same value for both directions
  +    
  +    # Create Excel workbook
  +    wb = Workbook()
  +    
  +    # Create Summary sheet
  +    summary_sheet = wb.active
  +    summary_sheet.title = "Summary"
  +    
  +    # Add headers
  +    summary_sheet['A1'] = "Team 0 Agent Comparison Summary"
  +    summary_sheet['A1'].font = Font(bold=True, size=14)
  +    summary_sheet.merge_cells('A1:F1')
  +    
  +    summary_sheet['A3'] = "Model"
  +    summary_sheet['B3'] = "Episodes"
  +    summary_sheet['C3'] = "Average Win Rate (%)"
  +    summary_sheet['D3'] = "Average Score Diff"
  +    summary_sheet['E3'] = "First-Starter Advantage"
  +    summary_sheet['F3'] = "Path"
  +    
  +    # Bold the headers
  +    for cell in summary_sheet['A3:F3'][0]:
  +        cell.font = Font(bold=True)
  +    
  +    # Fill summary data
  +    for row_idx, (name, episode, path) in enumerate(checkpoints_info, start=4):
  +        # Calculate average metrics against all other models
  +        win_rates = []
  +        score_diffs = []
  +        starter_advantages = []
  +        
  +        for other_name in model_names:
  +            if other_name != name:
  +                # Win rates
  +                win_rate_str = win_rate_data.loc[name, other_name]
  +                if win_rate_str != "N/A" and win_rate_str != "—":
  +                    win_rates.append(float(win_rate_str.strip('%')))
  +                
  +                # Score diffs
  +                score_diff_str = score_diff_data.loc[name, other_name]
  +                if score_diff_str != "N/A" and score_diff_str != "—":
  +                    score_diffs.append(float(score_diff_str))
  +                
  +                # Starter advantage
  +                starter_adv_str = starter_advantage_data.loc[name, other_name]
  +                if starter_adv_str != "N/A" and starter_adv_str != "—":
  +                    starter_advantages.append(float(starter_adv_str))
  +        
  +        avg_win_rate = sum(win_rates) / len(win_rates) if win_rates else float('nan')
  +        avg_score_diff = sum(score_diffs) / len(score_diffs) if score_diffs else float('nan')
  +        avg_starter_adv = sum(starter_advantages) / len(starter_advantages) if starter_advantages else float('nan')
  +        
  +        summary_sheet[f'A{row_idx}'] = name
  +        summary_sheet[f'B{row_idx}'] = episode if episode != float('inf') else "Unknown"
  +        summary_sheet[f'C{row_idx}'] = f"{avg_win_rate:.1f}%" if not pd.isna(avg_win_rate) else "N/A"
  +        summary_sheet[f'D{row_idx}'] = f"{avg_score_diff:.2f}" if not pd.isna(avg_score_diff) else "N/A"
  +        summary_sheet[f'E{row_idx}'] = f"{avg_starter_adv:.2f}" if not pd.isna(avg_starter_adv) else "N/A"
  +        summary_sheet[f'F{row_idx}'] = path
  +    
  +    # Auto-adjust column widths for Summary sheet
  +    for col in range(1, 7):  # Columns A-F
  +        column_letter = openpyxl.utils.get_column_letter(col)
  +        max_length = 0
  +        for row in range(1, summary_sheet.max_row + 1):
  +            cell = summary_sheet.cell(row=row, column=col)
  +            if cell.value:
  +                max_length = max(max_length, len(str(cell.value)))
  +        adjusted_width = max_length + 2
  +        summary_sheet.column_dimensions[column_letter].width = adjusted_width
  +    
  +    # Create all metric-specific sheets
  +    metrics_to_create = [
  +        ("Win Rates", win_rate_data, "Win Rates (Row vs Column)", 
  +         "Higher percentage means the row model wins more often against the column model"),
  +        ("Score Differences", score_diff_data, "Score Differences (Row minus Column)",
  +         "Positive values mean the row model scores higher than the column model"),
  +        ("First-Starter Advantage", starter_advantage_data, "First-Starter Advantage",
  +         "Values close to 0.5 indicate no advantage, higher values indicate advantage (1.0 = 100% advantage)"),
  +        ("Game Length", game_length_data, "Average Game Length (moves)",
  +         "Average number of moves per game for each matchup"),
  +        ("Draws", draws_data, "Draw Percentage", 
  +         "Percentage of games that ended in a draw")
  +    ]
  +    
  +    for sheet_name, data_frame, title, description in metrics_to_create:
  +        sheet = wb.create_sheet(title=sheet_name)
  +        
  +        # Add title and description
  +        sheet['A1'] = title
  +        sheet['A1'].font = Font(bold=True, size=14)
  +        sheet.merge_cells(f'A1:{chr(65 + len(model_names) + 1)}1')
  +        
  +        sheet['A2'] = description
  +        sheet.merge_cells(f'A2:{chr(65 + len(model_names) + 1)}2')
  +        
  +        # Add data from DataFrame
  +        for r_idx, row in enumerate(dataframe_to_rows(data_frame, index=True, header=True), start=4):
  +            for c_idx, value in enumerate(row, start=1):
  +                sheet.cell(row=r_idx, column=c_idx, value=value)
  +                
  +                # Apply conditional formatting based on sheet type
  +                cell = sheet.cell(row=r_idx, column=c_idx)
  +                
  +                if sheet_name == "Win Rates" and isinstance(value, str) and '%' in value and value != "—":
  +                    # Apply color gradient to win rates (green for high, red for low)
  +                    win_rate = float(value.strip('%'))
  +                    if win_rate >= 50:
  +                        intensity = min(255, int(155 + (win_rate - 50) * 2))
  +                        green_hex = format(intensity, '02x')
  +                        red_hex = format(255 - intensity // 3, '02x')
  +                        cell.fill = PatternFill(start_color=f"{red_hex}{green_hex}55", 
  +                                                end_color=f"{red_hex}{green_hex}55", 
  +                                                fill_type="solid")
  +                    else:
  +                        intensity = min(255, int(155 + (50 - win_rate) * 2))
  +                        red_hex = format(intensity, '02x')
  +                        green_hex = format(255 - intensity // 3, '02x')
  +                        cell.fill = PatternFill(start_color=f"{red_hex}{green_hex}55", 
  +                                                end_color=f"{red_hex}{green_hex}55", 
  +                                                fill_type="solid")
  +                
  +                elif sheet_name == "Score Differences" and isinstance(value, str) and value not in ["N/A", "—"]:
  +                # Apply color gradient to score differences
  +                    score_diff = float(value)
  +                    if score_diff > 0:
  +                        intensity = min(255, int(155 + min(score_diff * 25, 100)))
  +                        green_hex = format(intensity, '02x')
  +                        red_hex = format(255 - intensity // 3, '02x')
  +                        cell.fill = PatternFill(start_color=f"{red_hex}{green_hex}55", 
  +                                                end_color=f"{red_hex}{green_hex}55", 
  +                                                fill_type="solid")
  +                    elif score_diff < 0:
  +                        intensity = min(255, int(155 + min(abs(score_diff) * 25, 100)))
  +                        red_hex = format(intensity, '02x')
  +                        green_hex = format(255 - intensity // 3, '02x')
  +                        cell.fill = PatternFill(start_color=f"{red_hex}{green_hex}55", 
  +                                                end_color=f"{red_hex}{green_hex}55", 
  +                                                fill_type="solid")
  +                
  +                elif sheet_name == "First-Starter Advantage" and isinstance(value, str) and value not in ["N/A", "—"]:
  +                    # Apply color gradient to starter advantage (neutral at 0.5)
  +                    advantage = float(value)
  +                    # Distance from 0.5 (neutral)
  +                    distance = abs(advantage - 0.5)
  +                    intensity = min(255, int(155 + min(distance * 200, 100)))
  +                    blue_hex = format(intensity, '02x')
  +                    cell.fill = PatternFill(start_color=f"55{blue_hex}{blue_hex}", 
  +                                            end_color=f"55{blue_hex}{blue_hex}", 
  +                                            fill_type="solid")
  +        
  +        # Apply formatting to header row and column
  +        for row in sheet.iter_rows(min_row=4, max_row=4, min_col=2):
  +            for cell in row:
  +                cell.font = Font(bold=True)
  +        
  +        for col in sheet.iter_cols(min_col=1, max_col=1, min_row=5):
  +            for cell in col:
  +                cell.font = Font(bold=True)
  +        
  +        # Auto-adjust column widths
  +        max_col = len(model_names) + 1
  +        for col in range(1, max_col + 1):
  +            column_letter = openpyxl.utils.get_column_letter(col)
  +            max_length = 0
  +            for row in range(1, sheet.max_row + 1):
  +                cell = sheet.cell(row=row, column=col)
  +                if cell.value:
  +                    max_length = max(max_length, len(str(cell.value)))
  +            adjusted_width = max_length + 2
  +            sheet.column_dimensions[column_letter].width = adjusted_width
  +    
  +    # Create a Detailed Results sheet
  +    detailed_sheet = wb.create_sheet(title="Detailed Results")
  +    
  +    # Add headers
  +    detailed_sheet['A1'] = "Detailed Matchup Results"
  +    detailed_sheet['A1'].font = Font(bold=True, size=14)
  +    detailed_sheet.merge_cells('A1:J1')
  +    
  +    row_idx = 3
  +    detailed_sheet[f'A{row_idx}'] = "Model A"
  +    detailed_sheet[f'B{row_idx}'] = "Model B"
  +    detailed_sheet[f'C{row_idx}'] = "A Wins"
  +    detailed_sheet[f'D{row_idx}'] = "B Wins"
  +    detailed_sheet[f'E{row_idx}'] = "Draws"
  +    detailed_sheet[f'F{row_idx}'] = "A Win %"
  +    detailed_sheet[f'G{row_idx}'] = "B Win %"
  +    detailed_sheet[f'H{row_idx}'] = "A Avg Score"
  +    detailed_sheet[f'I{row_idx}'] = "B Avg Score"
  +    detailed_sheet[f'J{row_idx}'] = "First-Starter Advantage"
  +    
  +    # Bold the headers
  +    for cell in detailed_sheet[f'A{row_idx}:J{row_idx}'][0]:
  +        cell.font = Font(bold=True)
  +    
  +    # Add detailed results
  +    row_idx = 4
  +    for matchup, data in results.items():
  +        agents = matchup.split("_vs_")
  +        agent1_name, agent2_name = agents
  +        
  +        # Calculate metrics
  +        total_games = data['games']
  +        agent1_wins = data['agent1_wins']
  +        agent2_wins = data['agent2_wins']
  +        draws = data['draws']
  +        
  +        agent1_win_pct = (agent1_wins / total_games) * 100
  +        agent2_win_pct = (agent2_wins / total_games) * 100
  +        
  +        total_decided_games = agent1_wins + agent2_wins
  +        if total_decided_games > 0:
  +            first_starter_advantage = data['first_starter_wins'] / total_decided_games
  +        else:
  +            first_starter_advantage = 0.5  # Default to no advantage
  +        
  +        # Add data to sheet
  +        detailed_sheet[f'A{row_idx}'] = agent1_name
  +        detailed_sheet[f'B{row_idx}'] = agent2_name
  +        detailed_sheet[f'C{row_idx}'] = agent1_wins
  +        detailed_sheet[f'D{row_idx}'] = agent2_wins
  +        detailed_sheet[f'E{row_idx}'] = draws
  +        detailed_sheet[f'F{row_idx}'] = f"{agent1_win_pct:.1f}%"
  +        detailed_sheet[f'G{row_idx}'] = f"{agent2_win_pct:.1f}%"
  +        detailed_sheet[f'H{row_idx}'] = data['agent1_avg_score']
  +        detailed_sheet[f'I{row_idx}'] = data['agent2_avg_score']
  +        detailed_sheet[f'J{row_idx}'] = f"{first_starter_advantage:.2f}"
  +        
  +        row_idx += 1
  +    
  +    # Auto-adjust column widths for Detailed Results
  +    for col in range(1, 11):  # Columns A-J
  +        column_letter = openpyxl.utils.get_column_letter(col)
  +        max_length = 0
  +        for row in range(1, detailed_sheet.max_row + 1):
  +            cell = detailed_sheet.cell(row=row, column=col)
  +            if cell.value:
  +                max_length = max(max_length, len(str(cell.value)))
  +        adjusted_width = max_length + 2
  +        detailed_sheet.column_dimensions[column_letter].width = adjusted_width
  +    
  +    # Save the workbook
  +    wb.save(output_file)
  +    print(f"\nEnhanced Excel comparison report saved to {output_file}")
  +    
  +    return output_file
  +
  +def main():
  +    parser = argparse.ArgumentParser(description="Evaluate Team 0 Scopone AI checkpoints")
  +    parser.add_argument("--checkpoints", nargs="+", help="Paths to Team 0 checkpoint files or directories")
  +    parser.add_argument("--checkpoint_dir", help="Directory containing Team 0 checkpoints")
  +    parser.add_argument("--checkpoint_pattern", default="*team0*ep*.pth", help="Pattern to match Team 0 checkpoint files")
  +    parser.add_argument("--games", type=int, default=10000, help="Number of games to play for each matchup")
  +    parser.add_argument("--output", help="Output file path (default: auto-generated)")
  +    parser.add_argument("--excel", help="Excel output file path (default: auto-generated)")
  +    parser.add_argument("--limit", type=int, help="Limit the number of checkpoints to evaluate")
  +    args = parser.parse_args()
  +    
  +    # Get checkpoint paths
  +    checkpoint_paths = []
  +    
  +    if args.checkpoints:
  +        for cp in args.checkpoints:
  +            if os.path.isfile(cp):
  +                checkpoint_paths.append(cp)
  +            elif os.path.isdir(cp):
  +                checkpoint_paths.extend(find_checkpoints(cp, args.checkpoint_pattern))
  +            else:
  +                print(f"Warning: Checkpoint not found: {cp}")
  +    
  +    if args.checkpoint_dir:
  +        checkpoint_paths.extend(find_checkpoints(args.checkpoint_dir, args.checkpoint_pattern))
  +    
  +    # Remove duplicates
  +    checkpoint_paths = list(set(checkpoint_paths))
  +    
  +    # Ensure we only have Team 0 checkpoints
  +    team0_checkpoints = []
  +    for cp in checkpoint_paths:
  +        if "team0" in os.path.basename(cp):
  +            team0_checkpoints.append(cp)
  +        else:
  +            print(f"Skipping non-Team 0 checkpoint: {cp}")
  +    
  +    checkpoint_paths = team0_checkpoints
  +    
  +    # Limit number of checkpoints if requested
  +    if args.limit and len(checkpoint_paths) > args.limit:
  +        # Try to pick evenly spaced checkpoints
  +        step = len(checkpoint_paths) // args.limit
  +        limited_paths = [checkpoint_paths[i] for i in range(0, len(checkpoint_paths), step)][:args.limit]
  +        checkpoint_paths = limited_paths
  +    
  +    if not checkpoint_paths:
  +        print("Error: No Team 0 checkpoint files found. Please provide valid checkpoint paths.")
  +        return
  +    
  +    # Report found checkpoints
  +    print(f"Found {len(checkpoint_paths)} Team 0 checkpoint files:")
  +    for i, cp in enumerate(checkpoint_paths):
  +        print(f"{i+1}. {cp}")
  +    
  +    # Configure GPU if available
  +    if torch.cuda.is_available():
  +        torch.backends.cudnn.benchmark = True
  +        torch.backends.cudnn.enabled = True
  +        if hasattr(torch.backends.cuda, 'matmul'):
  +            torch.backends.cuda.matmul.allow_tf32 = True
  +        torch.backends.cudnn.allow_tf32 = True
  +        torch.cuda.empty_cache()
  +    
  +    # Save results to file
  +    timestamp = time.strftime("%Y%m%d-%H%M%S")
  +    output_file = args.output if args.output else f"team0_comparison_{timestamp}.txt"
  +    excel_file = args.excel if args.excel else f"team0_comparison_{timestamp}.xlsx"
  +    
  +    with open(output_file, "w") as f:
  +        f.write(f"Scopone AI Team 0 Checkpoint Comparison Results ({time.strftime('%Y-%m-%d %H:%M:%S')})\n")
  +        f.write(f"Number of games per evaluation: {args.games}\n\n")
  +        f.write(f"Checkpoints evaluated ({len(checkpoint_paths)}):\n")
  +        for i, cp in enumerate(checkpoint_paths):
  +            episode = extract_episode_number(cp)
  +            episode_str = f" (Episode {episode})" if episode != float('inf') else ""
  +            f.write(f"{i+1}. {cp}{episode_str}\n")
  +        f.write("\n")
  +        
  +        # Only do comparison - no self-play needed since all agents are Team 0
  +        if len(checkpoint_paths) > 1:
  +            f.write("=== COMPARISON RESULTS ===\n\n")
  +            comparison_results = evaluate_checkpoints(checkpoint_paths, args.games)
  +            
  +            for matchup, data in comparison_results.items():
  +                agents = matchup.split("_vs_")
  +                
  +                # Calculate first-mover advantage
  +                first_starter_advantage = 0
  +                total_decided_games = data['agent1_wins'] + data['agent2_wins']
  +                if total_decided_games > 0:
  +                    first_starter_advantage = data['first_starter_wins'] / total_decided_games
  +                
  +                f.write(f"{agents[0]} vs {agents[1]}:\n")
  +                f.write(f"  {agents[0]} wins: {data['agent1_wins']} ({data['agent1_wins']/args.games*100:.1f}%)\n")
  +                f.write(f"  {agents[1]} wins: {data['agent2_wins']} ({data['agent2_wins']/args.games*100:.1f}%)\n")
  +                f.write(f"  First-starter advantage: {first_starter_advantage:.2f} (1.0 = 100% advantage, 0.5 = no advantage)\n")
  +                f.write(f"  Draws: {data['draws']} ({data['draws']/args.games*100:.1f}%)\n")
  +                f.write(f"  Average score {agents[0]}: {data['agent1_avg_score']:.2f}\n")
  +                f.write(f"  Average score {agents[1]}: {data['agent2_avg_score']:.2f}\n")
  +                f.write(f"  Average game length: {sum(data['game_lengths'])/len(data['game_lengths']):.1f} moves\n")
  +                f.write(f"  Score distributions:\n")
  +                f.write(f"    {agents[0]}: {dict(data['agent1_score_distribution'])}\n")
  +                f.write(f"    {agents[1]}: {dict(data['agent2_score_distribution'])}\n\n")
  +            
  +            # Generate Excel report with comparative matrix
  +            generate_excel_comparison(checkpoint_paths, comparison_results, excel_file)
  +    
  +    print(f"\nResults saved to {output_file}")
  +    print(f"Excel comparison report saved to {excel_file}")
  +
  +if __name__ == "__main__":
--     main()
+++    main()
diff --cc environment.py
index 459609e,459609e,0000000..0202b88
mode 100644,100644,000000..100644
--- a/environment.py
+++ b/environment.py
@@@@ -1,943 -1,943 -1,0 +1,958 @@@@
  +import torch
  +import os
  +from utils.device import get_env_device
  +import gymnasium as gym
  +from gymnasium import spaces
  +import time
  +from collections import OrderedDict
  +from state import initialize_game
  +from actions import decode_action_ids
  +from observation import (
  +    set_obs_device,
  +    encode_state_compact_for_player_fast as _encode_state_compact_for_player_fast,
  +    RANK_OF_ID,
  +    OBS_INCLUDE_INFERRED,
+++    OBS_INCLUDE_INFERRED_L2,
+++    OBS_INCLUDE_INFERRED_L3,
  +    OBS_INCLUDE_RANK_PROBS,
  +    OBS_INCLUDE_SCOPA_PROBS,
  +    OBS_INCLUDE_DEALER,
+++    get_compact_obs_dim,
  +)
  +from utils.compile import maybe_compile_function
  +import torch.nn.functional as F
  +
  +# Per l'ambiente usiamo la CPU per evitare micro-kernel su GPU.
  +# Può essere forzato impostando ENV_DEVICE, ma di default resta CPU.
  +device = get_env_device()
  +_SKIP_STEP_VALIDATION = (os.environ.get('SCOPONE_SKIP_STEP_VALIDATION', '1') == '1')
  +
  +def _suit_to_int(suit):
  +    if suit == 'denari':
  +        return 0
  +    if suit == 'coppe':
  +        return 1
  +    if suit == 'spade':
  +        return 2
  +    return 3  # 'bastoni'
  +
  +def _card_to_id(card):
  +    if isinstance(card, int):
  +        return int(card)
  +    rank, suit = card
  +    return (rank - 1) * 4 + _suit_to_int(suit)
  +
  +def _ids_to_bitset(ids):
  +    bits = 0
  +    for cid in ids:
  +        bits |= (1 << cid)
  +    return bits
  +
  +class ScoponeEnvMA(gym.Env):
  +    def __init__(self, rules=None, k_history: int = 39):
  +        super().__init__()
  +        self.k_history = int(k_history)
  +        # Dimensione compatta dinamica in base ai flag OBS_INCLUDE_* (single source from observation.py)
  +        # Base fisse: 43 (mani) + 40 (tavolo) + 82 (catture) + 61*k (history) + 40 (missing)
  +        #            + 120 (inferred) + 8 (primiera) + 2 (denari) + 1 (settebello) + 2 (score)
  +        #            + 1 (table sum) + 10 (table possible sums) + 2 (scopa counts)
  +        #            + 30 (rank_presence_from_inferred) + 1 (progress) + 2 (last capturing team)
--         include_rank = bool(OBS_INCLUDE_RANK_PROBS)
--         include_scopa = bool(OBS_INCLUDE_SCOPA_PROBS)
--         include_inferred = bool(OBS_INCLUDE_INFERRED)
--         include_dealer = bool(OBS_INCLUDE_DEALER)
--         # Parti fisse comuni (senza inferred): 43+40+82 + 61*k + 40 + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
--         fixed = 43 + 40 + 82 + 61 * self.k_history + 40 + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
--         base = fixed + (120 if include_inferred else 0)
--         obs_dim = base + (10 if include_scopa else 0) + (150 if include_rank else 0) + (4 if include_dealer else 0)
+++        obs_dim = get_compact_obs_dim(self.k_history)
  +        # Observation space con la rappresentazione selezionata
  +        self.observation_space = spaces.Box(low=0, high=1, shape=(obs_dim,))
  +        
  +        # Action space
  +        self.action_space = spaces.MultiBinary(80)
  +        
  +        # OTTIMIZZAZIONE: Migliore struttura dati per la cache delle azioni valide (LRU)
  +        self._valid_actions_cache = OrderedDict()
  +        self._cache_hits = 0
  +        self._cache_misses = 0
  +        
  +        # OTTIMIZZAZIONE: Cache per encode_state_for_player (LRU)
  +        self._observation_cache = OrderedDict()
  +        self._cache_capacity = 2048
  +        # Cache DP per subset-sum sul tavolo (chiave: (table_bits, rank))
  +        self._subset_sum_cache = {}
  +        # Cache maschere per numero di carte sul tavolo: n -> (masks[1..(1<<n)-1], pos[0..n-1])
  +        self._subset_masks_cache = {}
  +        # Cache invarianti per stato del tavolo: table_bits -> (table_ids_t, table_ranks, masks, pos)
  +        self._table_invariants_cache = {}
  +        
  +        # Stato del gioco
  +        self.game_state = None  # usa rappresentazione a tuple (rank,suit) per compat, ma prevedi futura migrazione a ID/bitset
  +        self.done = False
  +        self.current_player = 0
  +        self.rewards = [0,0]
  +        # Cache ID/bitset (parziale)
  +        self._use_id_cache = True
  +        self._use_bitset = True
  +        self._hands_ids = {p: [] for p in range(4)}
  +        self._table_ids = []
  +        self._hands_bits = {p: 0 for p in range(4)}
  +        self._table_bits = 0
  +        # Mirrors (on env device; default CPU). We keep API compatible.
  +        self._hands_bits_t = torch.zeros(4, dtype=torch.int64, device=device)
  +        self._table_bits_t = torch.zeros((), dtype=torch.int64, device=device)
  +        self._captured_bits_t = torch.zeros(2, dtype=torch.int64, device=device)
  +        self._one_i64 = torch.tensor(1, dtype=torch.int64, device=device)
  +        # Precompute id range to avoid per-call allocations
  +        self._id_range = torch.arange(40, dtype=torch.int64, device=device)
  +        # Precompute one-hot matrix for card IDs (40×40) to spare scatter/equality kernels per step
  +        self._card_eye = torch.eye(40, dtype=torch.float32, device=device)
  +        # Profiling flags
  +        self._profiling = (os.environ.get('SCOPONE_PROFILE', '0') != '0')
  +        self._prof_step_total = 0.0
  +        self._prof_step_decode = 0.0
  +        self._prof_step_validate = 0.0
  +        self._prof_step_apply = 0.0
  +        self._prof_step_count = 0
  +        self._prof_legals_total = 0.0
  +        self._prof_legals_calls = 0
  +
  +        # Derived features mirrors (kept as tensors for compile-friendly observation):
  +        self._progress_t = torch.zeros(1, dtype=torch.float32, device=device)
  +        self._last_capturing_team_t = torch.zeros(2, dtype=torch.float32, device=device)
  +        # History mirrors (tensor ring buffer of moves encoded 61-dim)
  +        self._hist_buf_t = torch.zeros(self.k_history, 61, dtype=torch.float32, device=device)
  +        self._history_len_t = torch.zeros((), dtype=torch.long, device=device)
  +        # Ring buffer head index (next write position)
  +        self._hist_head_t = torch.zeros((), dtype=torch.long, device=device)
  +        # Aggregates mirrors
  +        self._played_bits_by_player_t = torch.zeros(4, dtype=torch.int64, device=device)
  +        self._scopa_counts_t = torch.zeros(2, dtype=torch.float32, device=device)
  +        # Track consecutive scope per team to avoid scanning history each step
  +        self._consec_scopa_team_t = torch.zeros(2, dtype=torch.int32, device=device)
  +        # Optional mirrors for probabilistic features (populated eagerly when enabled)
  +        self._inferred_probs_t = torch.zeros(120, dtype=torch.float32, device=device)
+++        self._inferred_probs_l2_t = torch.zeros(120, dtype=torch.float32, device=device)
+++        self._inferred_probs_l3_t = torch.zeros(120, dtype=torch.float32, device=device)
  +        self._rank_probs_by_player_t = torch.zeros(150, dtype=torch.float32, device=device)
  +        self._rank_presence_from_inferred_t = torch.zeros(30, dtype=torch.float32, device=device)
  +        self._scopa_probs_t = torch.zeros(10, dtype=torch.float32, device=device)
  +        self._history_len = 0
  +
  +        # Regole/varianti opzionali della partita
  +        # Nota: negli script con modalità di default, "asso_piglia_tutto" è disattivato
  +        # e quindi ignorato (cioè non viene applicata la variante AP).
  +        # Esempi di chiavi supportate:
  +        #  - asso_piglia_tutto: bool
  +        #  - scopa_on_asso_piglia_tutto: bool
  +        #  - scopa_on_last_capture: bool
  +        #  - re_bello: bool
  +        #  - napola: bool
  +        #  - napola_scoring: "fixed3" | "length"
  +        #  - max_consecutive_scope: int | None (limite per team)
  +        self.rules = rules or {}
  +        
  +        # Contatori per diagnostica prestazioni
  +        self._get_obs_time = 0
  +        self._get_valid_actions_time = 0
  +        self._step_time = 0
  +        self._step_count = 0
  +        
  +        # Ensure observation constants are on the same device as env internals
  +        set_obs_device(device)
  +        # Compile and bind the observation encoder once
  +        self._encode_obs = maybe_compile_function(_encode_state_compact_for_player_fast, name='observation.encode_state_compact_for_player_fast')
  +        # Preallocate observation output buffer to avoid per-call allocations
  +        self._obs_out_buf = None
  +        self.reset()
  +    def _rebuild_id_caches(self):
  +        if not self._use_id_cache:
  +            return
  +        for p in range(4):
  +            ids = [_card_to_id(c) for c in self.game_state['hands'][p]]
  +            self._hands_ids[p] = ids
  +            self._hands_bits[p] = _ids_to_bitset(ids) if self._use_bitset else 0
  +            # sync CUDA mirror strictly
  +            self._hands_bits_t[p] = torch.as_tensor(self._hands_bits[p], dtype=torch.int64, device=device)
  +        tids = [_card_to_id(c) for c in self.game_state['table']]
  +        self._table_ids = tids
  +        self._table_bits = _ids_to_bitset(tids) if self._use_bitset else 0
  +        self._table_bits_t = torch.as_tensor(self._table_bits, dtype=torch.int64, device=device)
  +        # captured squads mirror
  +        bits0 = _ids_to_bitset([_card_to_id(c) for c in self.game_state['captured_squads'][0]])
  +        bits1 = _ids_to_bitset([_card_to_id(c) for c in self.game_state['captured_squads'][1]])
  +        self._captured_bits_t[0] = torch.as_tensor(bits0, dtype=torch.int64, device=device)
  +        self._captured_bits_t[1] = torch.as_tensor(bits1, dtype=torch.int64, device=device)
  +
  +    def _attach_state_views(self):
  +        """Expose tensor mirrors inside game_state once to avoid per-call dict churn."""
  +        if self.game_state is None:
  +            return
  +        self.game_state['_hands_bits_t'] = self._hands_bits_t
  +        self.game_state['_table_bits_t'] = self._table_bits_t
  +        self.game_state['_captured_bits_t'] = self._captured_bits_t
  +        self.game_state['_progress_t'] = self._progress_t
  +        self.game_state['_last_capturing_team_t'] = self._last_capturing_team_t
  +        self.game_state['_hist_buf_t'] = self._hist_buf_t
  +        self.game_state['_history_len_t'] = self._history_len_t
  +        self.game_state['_played_bits_by_player_t'] = self._played_bits_by_player_t
  +        self.game_state['_scopa_counts_t'] = self._scopa_counts_t
  +        self.game_state['_consec_scopa_team_t'] = self._consec_scopa_team_t
  +        self.game_state['_inferred_probs_t'] = self._inferred_probs_t
+++        self.game_state['_inferred_probs_l2_t'] = self._inferred_probs_l2_t
+++        self.game_state['_inferred_probs_l3_t'] = self._inferred_probs_l3_t
  +        self.game_state['_rank_probs_by_player_t'] = self._rank_probs_by_player_t
  +        self.game_state['_rank_presence_from_inferred_t'] = self._rank_presence_from_inferred_t
  +        self.game_state['_scopa_probs_t'] = self._scopa_probs_t
  +
  +    
  +    def clone(self):
  +        """Crea una copia profonda dell'ambiente per simulazione/ricerca."""
  +        import copy
  +        cloned = ScoponeEnvMA(rules=copy.deepcopy(self.rules), k_history=self.k_history)
  +        cloned.game_state = copy.deepcopy(self.game_state)
  +        cloned.done = self.done
  +        cloned.current_player = self.current_player
  +        cloned.rewards = list(self.rewards)
  +        # reset cache per sicurezza
  +        cloned._valid_actions_cache = OrderedDict()
  +        cloned._observation_cache = OrderedDict()
  +        # Ricostruisci le cache ID/bitset per allinearle allo stato copiato
  +        try:
  +            cloned._rebuild_id_caches()
  +        except Exception:
  +            raise RuntimeError('Failed to rebuild id caches')
  +
  +        # Replica gli specchi tensoriali e i contatori runtime per mantenere coerenza nello stato.
  +        mirror_attrs = (
  +            '_progress_t',
  +            '_last_capturing_team_t',
  +            '_hist_buf_t',
  +            '_hist_head_t',
  +            '_history_len_t',
  +            '_played_bits_by_player_t',
  +            '_scopa_counts_t',
  +            '_consec_scopa_team_t',
  +            '_inferred_probs_t',
+++            '_inferred_probs_l2_t',
+++            '_inferred_probs_l3_t',
  +            '_rank_probs_by_player_t',
  +            '_rank_presence_from_inferred_t',
  +            '_scopa_probs_t',
  +        )
  +        for attr in mirror_attrs:
  +            val = getattr(self, attr, None)
  +            if torch.is_tensor(val):
  +                setattr(cloned, attr, val.clone())
  +
  +        # Copia anche le metriche di performance rilevanti per analisi successive.
  +        cloned._step_count = self._step_count
  +        cloned._get_obs_time = self._get_obs_time
  +        cloned._get_valid_actions_time = self._get_valid_actions_time
  +        cloned._step_time = self._step_time
  +        cloned._cache_hits = self._cache_hits
  +        cloned._cache_misses = self._cache_misses
  +        cloned._history_len = self._history_len
  +
  +        # Ogni clone deve allocare il proprio buffer di osservazione quando necessario.
  +        cloned._obs_out_buf = None
  +        cloned._attach_state_views()
  +        return cloned
  +
  +    def get_valid_actions(self):
  +        """Calcola azioni valide e restituisce un tensore (A,80) su device (GPU quando disponibile).
  +        Implementazione vettorizzata: evita loop Python per carta/azione.
  +        Regole replicate: presa diretta (pari-rank) prioritaria su somme; in assenza, somme; altrimenti scarto.
  +        Varianti AP (asso_piglia_tutto / posabile) rispettate.
  +        """
  +        start_time = time.time()
  +        # RANK_OF_ID è definito per CUDA; per CPU calcoliamo al volo
  +        # Usa la variante tensor-native (device-agnostica) per codificare le azioni
  +        from actions import encode_action_from_ids_tensor as encode_action_from_ids
  +
  +        _t_legals_start = time.time() if self._profiling else 0.0
  +
  +        # Chiave di cache LRU basata su (giocatore, mano, tavolo, regole AP)
  +        ap_enabled = bool(self.rules.get("asso_piglia_tutto", False))
  +        ap_posabile = bool(self.rules.get("asso_piglia_tutto_posabile", False))
  +        ap_only_empty = bool(self.rules.get("asso_piglia_tutto_posabile_only_empty", False))
  +        state_key = (
  +            int(self.current_player),
  +            int(self._hands_bits_t[self.current_player].item()),
  +            int(self._table_bits_t.item()),
  +            ap_enabled, ap_posabile, ap_only_empty,
  +        )
  +        cached = self._valid_actions_cache.get(state_key)
  +        if cached is not None:
  +            self._cache_hits += 1
  +            self._get_valid_actions_time += time.time() - start_time
  +            if self._profiling:
  +                self._prof_legals_total += time.time() - _t_legals_start
  +                self._prof_legals_calls += 1
  +            return cached
  +
  +        # Estrai ID mano e tavolo da bitset mirror
  +        ids = self._id_range
  +        hb = self._hands_bits_t[self.current_player]
  +        tb = self._table_bits_t
  +        hand_mask = ((hb >> ids) & 1).to(torch.bool)
  +        table_mask = ((tb >> ids) & 1).to(torch.bool)
  +        hand_ids_t = ids.masked_select(hand_mask)  # (H)
  +        table_ids_t = ids.masked_select(table_mask)  # (n)
  +
  +        if hand_ids_t.numel() == 0:
  +            self._get_valid_actions_time += time.time() - start_time
  +            if self._profiling:
  +                self._prof_legals_total += time.time() - _t_legals_start
  +                self._prof_legals_calls += 1
  +            return torch.zeros((0, 80), dtype=torch.float32, device=device)
  +
  +        # Precompute invariants for this table state
  +        n = int(table_ids_t.numel())
  +        table_ranks = (RANK_OF_ID.to(device=table_ids_t.device, dtype=torch.int16)[table_ids_t].to(torch.int64) if n > 0 else torch.empty(0, dtype=torch.int64, device=device))
  +        # Masks and per-table invariants cached by table bitset
  +        if n > 0:
  +            mp = self._subset_masks_cache.get(n)
  +            if mp is None:
  +                pos = torch.arange(n, dtype=torch.long, device=device)
  +                masks_all = torch.arange(1, 1 << n, dtype=torch.long, device=device)
  +                self._subset_masks_cache[n] = (masks_all, pos)
  +            else:
  +                masks_all, pos = mp
  +            sel = None
  +            sums = None
  +            table_one_hot = None
  +            capture_hot_by_mask = None
  +            tbl_bits_key = int(self._table_bits_t.item())
  +            inv = self._table_invariants_cache.get(tbl_bits_key)
  +            if inv is not None and inv.get('n', 0) == n:
  +                sel = inv.get('sel', None)
  +                sums = inv.get('sums', None)
  +                table_one_hot = inv.get('table_one_hot', None)
  +                capture_hot_by_mask = inv.get('capture_hot_by_mask', None)
  +            if sel is None:
  +                sel = ((masks_all.unsqueeze(1) >> pos) & 1).to(torch.float32)  # (M,n)
  +            if sums is None:
  +                sums = torch.matmul(sel, table_ranks.unsqueeze(1).to(torch.float32)).squeeze(1).to(torch.int64)
  +            if table_one_hot is None:
  +                # Build (n,40) one-hot via precomputed identity matrix to avoid per-call scatter
  +                table_one_hot = self._card_eye.index_select(0, table_ids_t)
  +            # Nota: capture_hot_by_mask calcolato lazy solo se necessario (vedi sotto)
  +        else:
  +            masks_all = torch.empty(0, dtype=torch.long, device=device)
  +            sel = torch.empty(0, 0, dtype=torch.float32, device=device)
  +            sums = torch.empty(0, dtype=torch.int64, device=device)
  +            capture_hot_by_mask = torch.empty(0, 40, dtype=torch.float32, device=device)
  +
  +        hand_ranks = RANK_OF_ID.to(device=hand_ids_t.device, dtype=torch.int16)[hand_ids_t].to(torch.int64)  # (H)
  +
  +        # Direct captures matrix: (H,n) True when table rank == hand rank
  +        direct_mat = (hand_ranks.unsqueeze(1) == (table_ranks.unsqueeze(0) if n>0 else torch.empty((1,0), dtype=torch.int64, device=device)))
  +        has_direct = direct_mat.any(dim=1) if n>0 else torch.zeros_like(hand_ranks, dtype=torch.bool)
  +        # Pairs for direct singles
  +        if n > 0 and bool(direct_mat.any()):
  +            h_idx_d, tpos_idx = direct_mat.nonzero(as_tuple=True)
  +            pid_rows_direct = hand_ids_t[h_idx_d]
  +            cap_ids_direct = table_ids_t[tpos_idx]
  +            captured_hot_direct = self._card_eye.index_select(0, cap_ids_direct)
  +        else:
  +            pid_rows_direct = torch.empty((0,), dtype=torch.long, device=device)
  +            captured_hot_direct = torch.empty((0,40), dtype=torch.float32, device=device)
  +
  +        # Subset-sum captures where no direct exists
  +        if n > 0 and masks_all.numel() > 0:
  +            good_by_p = (sums.unsqueeze(0) == hand_ranks.unsqueeze(1))  # (H,M)
  +            use_mask = (~has_direct).unsqueeze(1) & good_by_p
  +            if bool(use_mask.any()):
  +                h_idx_s, m_idx = use_mask.nonzero(as_tuple=True)
  +                pid_rows_sum = hand_ids_t[h_idx_s]
  +                # Calcola capture_hot_by_mask solo una volta e riutilizzalo
  +                if capture_hot_by_mask is None:
  +                    capture_hot_by_mask = sel.to(torch.float32) @ table_one_hot
  +                captured_hot_sum = capture_hot_by_mask[m_idx]
  +                # Aggiorna cache invarianti per questo tavolo
  +                self._table_invariants_cache[tbl_bits_key] = {
  +                    'n': n,
  +                    'sel': sel,
  +                    'sums': sums,
  +                    'table_one_hot': table_one_hot,
  +                    'capture_hot_by_mask': capture_hot_by_mask
  +                }
  +            else:
  +                pid_rows_sum = torch.empty((0,), dtype=torch.long, device=device)
  +                captured_hot_sum = torch.empty((0,40), dtype=torch.float32, device=device)
  +        else:
  +            pid_rows_sum = torch.empty((0,), dtype=torch.long, device=device)
  +            captured_hot_sum = torch.empty((0,40), dtype=torch.float32, device=device)
  +
  +        # Empty discard when neither direct nor subset exists
  +        need_empty = (~has_direct)
  +        if n > 0 and masks_all.numel() > 0:
  +            any_subset = (sums.unsqueeze(0) == hand_ranks.unsqueeze(1)).any(dim=1)
  +            need_empty = need_empty & (~any_subset)
  +        empty_idx = need_empty.nonzero(as_tuple=True)[0]
  +        pid_rows_empty = hand_ids_t[empty_idx]
  +        captured_hot_empty = torch.zeros((pid_rows_empty.numel(), 40), dtype=torch.float32, device=device)
  +
  +        # AP: asso piglia tutto (aggiungi cattura completa per tutte le A in mano)
  +        is_ace = (hand_ranks == 1)
  +        pid_rows_ap = torch.empty((0,), dtype=torch.long, device=device)
  +        captured_hot_ap = torch.empty((0,40), dtype=torch.float32, device=device)
  +        if ap_enabled and n > 0 and bool(is_ace.any()):
  +            num_ace = int(is_ace.sum().item())
  +            if num_ace > 0:
  +                pid_rows_ap = hand_ids_t[is_ace]
  +                table_capture_row = (table_one_hot.sum(dim=0, keepdim=True) > 0).to(torch.float32)
  +                captured_hot_ap = table_capture_row.expand(num_ace, -1).clone()
  +
  +        # Assemble all actions (played one-hot + captured multi-hot)
  +        pid_all = torch.cat([pid_rows_direct, pid_rows_sum, pid_rows_empty, pid_rows_ap], dim=0)
  +        cap_hot_all = torch.cat([captured_hot_direct, captured_hot_sum, captured_hot_empty, captured_hot_ap], dim=0)
  +        A = int(pid_all.numel())
  +        if A == 0:
  +            actions = torch.zeros((0, 80), dtype=torch.float32, device=device)
  +        else:
  +            # Replace per-call equality with gather from precomputed identity matrix
  +            played_oh = self._card_eye.index_select(0, pid_all)
  +            actions = torch.cat([played_oh, (cap_hot_all > 0).to(torch.float32)], dim=1)
  +
  +        # AP filter: se AP attivo ma posa asso non è consentita (e tavolo non vuoto), rimuovi azioni "asso + no capture"
  +        if ap_enabled and n > 0 and not (ap_posabile and (not ap_only_empty or n == 0)) and A > 0:
  +            played_ids = torch.argmax(actions[:, :40], dim=1)
  +            played_is_ace = ((played_ids // 4 + 1) == 1)
  +            empty_capture = (actions[:, 40:].sum(dim=1) == 0)
  +            keep = ~(played_is_ace & empty_capture)
  +            actions = actions[keep]
  +
  +        # In condizioni corrette, se il giocatore ha carte in mano, deve esistere almeno un'azione valida
  +        if hand_ids_t.numel() > 0 and actions.size(0) == 0:
  +            raise RuntimeError(f"No valid actions for player {self.current_player} (hand_ids={hand_ids_t.tolist()}, table_ids={table_ids_t.tolist()}, rules={self.rules})")
  +
  +        # Aggiorna cache LRU e contatori
  +        # limita la dimensione della cache per evitare crescita non controllata
  +        self._valid_actions_cache[state_key] = actions
  +        self._valid_actions_cache.move_to_end(state_key)
  +        # cap manuale: riusa _cache_capacity (stessa capacità delle osservazioni)
  +        while len(self._valid_actions_cache) > self._cache_capacity:
  +            self._valid_actions_cache.popitem(last=False)
  +        self._cache_misses += 1
  +        self._get_valid_actions_time += time.time() - start_time
  +        if self._profiling:
  +            self._prof_legals_total += time.time() - _t_legals_start
  +            self._prof_legals_calls += 1
  +        return actions
  +    
  +    def step(self, action_vec):
  +        """
  +        Versione ottimizzata di step per ridurre i trasferimenti CPU-GPU
  +        e migliorare le prestazioni complessive.
  +        """
  +        step_start_time = time.time()
  +        self._step_count += 1
  +        
  +        if self.done:
  +            raise ValueError("Partita già finita: non puoi fare altri step.")
  +
  +        _t_step_start = time.time() if self._profiling else 0.0
  +        _t_prev = _t_step_start
  +
  +        # Validazione forma vettore azione (deve essere 80)
  +        import torch as _torch
  +        if hasattr(action_vec, 'shape') and getattr(action_vec, 'ndim', 1) >= 1:
  +            act_len = int(action_vec.numel() if _torch.is_tensor(action_vec) else _torch.as_tensor(action_vec).numel())
  +        else:
  +            act_len = len(action_vec)
  +        if act_len != 80:
  +            raise ValueError(f"Formato vettore azione non valido: atteso 80, ricevuto {act_len}")
  +
  +        # Decodifica l'azione in ID (sempre su CPU per evitare micro-copie GPU)
  +        pid, cap_ids = decode_action_ids(action_vec)
  +        if self._profiling:
  +            _now = time.time()
  +            self._prof_step_decode += (_now - _t_prev)
  +            _t_prev = _now
  +
  +        
  +        # Verifica validità (come prima)
  +        current_player = self.current_player
  +        hand = self.game_state["hands"][current_player]
  +        table = self.game_state["table"]
  +        pre_table_len = len(table)
  +        if self._use_bitset:
  +            if ((self._hands_bits[current_player] >> int(pid)) & 1) == 0:
  +                raise ValueError(f"La carta {pid} non è nella mano del giocatore {current_player}.")
  +        elif pid not in hand:
  +            raise ValueError(f"La carta {pid} non è nella mano del giocatore {current_player}.")
  +
  +        table_ids_list = self._table_ids if self._use_id_cache else [_card_to_id(c) for c in table]
  +
  +        if self._profiling:
  +            _t_validate_start = time.time()
  +        if not _SKIP_STEP_VALIDATION:
  +            # Verifica carte da catturare (ID) via bitset CPU (evita sync GPU)
  +            for cid in cap_ids:
  +                present = (self._table_bits >> int(cid)) & 1
  +                if present == 0:
  +                    raise ValueError(f"La carta {cid} non si trova sul tavolo; cattura non valida.")
  +
  +        # Applicazione nuova regola AP posabilità: forza presa totale se non è consentito posare
  +        rank = pid // 4 + 1
  +        ap_enabled = bool(self.rules.get("asso_piglia_tutto", False))
  +        ap_posabile = bool(self.rules.get("asso_piglia_tutto_posabile", False))
  +        ap_only_empty = bool(self.rules.get("asso_piglia_tutto_posabile_only_empty", False))
  +        forced_ace_capture_on_empty = False
  +        # Override one-shot: la UI può impostare questo flag per auto-presa su tavolo vuoto anche se posabile è ON
  +        force_self_capture_once = bool(self.rules.get("force_ace_self_capture_on_empty_once", False))
  +        if ap_enabled and rank == 1:
  +            can_place_now = ap_posabile and (not ap_only_empty or (ap_only_empty and len(table) == 0))
  +            if (not can_place_now and len(cap_ids) == 0) or (force_self_capture_once and len(table) == 0 and len(cap_ids) == 0):
  +                if len(table) > 0:
  +                    # Forza presa di tutto il tavolo
  +                    cap_ids = table_ids_list.copy()
  +                else:
  +                    # Tavolo vuoto: la posa è vietata, tratta come cattura forzata per scopa
  +                    forced_ace_capture_on_empty = True
  +                # Consuma l'override one-shot, se presente
  +                if force_self_capture_once:
  +                    self.rules["force_ace_self_capture_on_empty_once"] = False
  +
  +        if not _SKIP_STEP_VALIDATION:
  +            # Verifica regole di cattura via bitset su CPU
  +            # Usa solo strutture CPU per evitare kernel micro e sync
  +            same_rank_ids = [i for i in table_ids_list if (i // 4 + 1) == int(rank)]
  +            if same_rank_ids:
  +                # Eccezione: Asso piglia tutto permette di ignorare la regola della presa diretta
  +                ace_take_all = (rank == 1 and self.rules.get("asso_piglia_tutto", False)
  +                                and set(cap_ids) == set(table_ids_list))
  +                if not ace_take_all:
  +                    # Devi catturare UNA carta di pari rank
  +                    if not (len(cap_ids) == 1 and (cap_ids[0] in same_rank_ids)):
  +                        raise ValueError("Quando esistono carte di rank uguale, devi catturarne una (non una combinazione).")
  +            elif cap_ids:
  +                # Verifica somma
  +                # Eccezione: Asso piglia tutto
  +                if not (rank == 1 and self.rules.get("asso_piglia_tutto", False) and set(cap_ids) == set(table_ids_list)):
  +                    sum_chosen = sum(((cid // 4) + 1) for cid in cap_ids)
  +                    if sum_chosen != rank:
  +                        raise ValueError(f"La somma delle carte catturate ({sum_chosen}) deve essere uguale al rank ({rank}).")
  +        
  +        if self._profiling:
  +            _now = time.time()
  +            self._prof_step_validate += (_now - _t_validate_start)
  +            _t_prev = _now
  +
  +        # OTTIMIZZAZIONE: Esegui l'azione in modo più efficiente
  +        capture_type = "no_capture"
  +        
  +        # Rimuovi la carta giocata dalla mano (ID)
  +        hand.remove(pid)
  +        # Aggiorna cache ID/bitset
  +        if self._use_id_cache:
  +            self._hands_ids[current_player].remove(pid)
  +            if self._use_bitset:
  +                self._hands_bits[current_player] &= ~(1 << pid)
  +                # tensor mirror
  +                mask = (self._one_i64 << int(pid))
  +                self._hands_bits_t[current_player] = self._hands_bits_t[current_player] & (~mask)
  +        
  +        if forced_ace_capture_on_empty:
  +            # Cattura forzata su tavolo vuoto con AP
  +            squad_id = 0 if current_player in [0, 2] else 1
  +            self.game_state["captured_squads"][squad_id].append(pid)
  +            self._captured_bits_t[squad_id] = self._captured_bits_t[squad_id] | (self._one_i64 << int(pid))
  +            # Regole scopa su AP a tavolo vuoto
  +            ap_scopa_on = bool(self.rules.get("scopa_on_asso_piglia_tutto", False))
  +            cards_left = sum(len(self.game_state["hands"][p]) for p in range(4))
  +            if ap_scopa_on:
  +                # Se AP ha scopa attiva, conta sempre scopa (anche all'ultima presa)
  +                capture_type = "scopa"
  +            else:
  +                # AP con scopa disattivata: mai scopa a tavolo vuoto
  +                capture_type = "capture"
  +        elif cap_ids:
  +            # Cattura carte
  +            for cid in cap_ids:
  +                table.remove(cid)
  +                if self._use_id_cache:
  +                    self._table_ids.remove(cid)
  +                    if self._use_bitset:
  +                        self._table_bits &= ~(1 << cid)
  +                        self._table_bits_t = self._table_bits_t & (~(self._one_i64 << int(cid)))
  +            
  +            # Aggiungi le carte catturate e la carta giocata alla squadra
  +            squad_id = 0 if current_player in [0, 2] else 1
  +            self.game_state["captured_squads"][squad_id].extend(cap_ids)
  +            self.game_state["captured_squads"][squad_id].append(pid)
  +            take_mask = (self._one_i64 << int(pid))
  +            for cid in cap_ids:
  +                take_mask = take_mask | (self._one_i64 << int(cid))
  +            self._captured_bits_t[squad_id] = self._captured_bits_t[squad_id] | take_mask
  +            
  +            # Verifica scopa
  +            if len(table) == 0:
  +                # Verifica che non sia l'ultima giocata
  +                cards_left = sum(len(self.game_state["hands"][p]) for p in range(4))
  +                if cards_left > 0:
  +                    capture_type = "scopa"
  +                else:
  +                    # Scopa all'ultima presa: opzionale
  +                    capture_type = "scopa" if self.rules.get("scopa_on_last_capture", False) else "capture"
  +            else:
  +                capture_type = "capture"
  +        else:
  +            # Nessuna cattura: la carta va sul tavolo
  +            table.append(pid)
  +            if self._use_id_cache:
  +                self._table_ids.append(pid)
  +                if self._use_bitset:
  +                    self._table_bits |= (1 << pid)
  +                    self._table_bits_t = self._table_bits_t | (self._one_i64 << int(pid))
  +        
  +        # Calcola reward shaping opzionale
  +        shaped_reward = 0.0
  +        if capture_type == "scopa" and bool(self.rules.get("shape_scopa", False)):
  +            shaped_reward = float(self.rules.get("scopa_reward", 0.1))
  +
  +        # Aggiorna history
  +        move_info = {
  +            "player": current_player,
  +            "played_card": pid,
  +            "capture_type": capture_type,
  +            "captured_cards": list(cap_ids)
  +        }
  +        # Eccezione: Asso piglia tutto non conta scopa (a meno di opzione esplicita)
  +        # Tuttavia, se sul tavolo c'era solo un asso ed è stato preso (anche usando AP),
  +        # deve comunque contare come scopa come nella presa diretta normale.
  +        if (move_info["capture_type"] == "scopa" and rank == 1 and 
  +            self.rules.get("asso_piglia_tutto", False) and not self.rules.get("scopa_on_asso_piglia_tutto", False)):
  +            # Demote salvo eccezione: presa DIRETTA di un asso su asso (unica carta prima della mossa)
  +            single_ace_sweep = (pre_table_len == 1 and len(cap_ids) == 1 and ((cap_ids[0] // 4) + 1) == 1)
  +            if not single_ace_sweep:
  +                move_info["capture_type"] = "capture"
  +
  +        # Limite scope consecutive per team
  +        if move_info["capture_type"] == "scopa":
  +            limit = self.rules.get("max_consecutive_scope")
  +            if isinstance(limit, int) and limit > 0:
  +                team_id = 0 if current_player in [0, 2] else 1
  +                consecutive = 0
  +                # Conta solo le scope consecutive nelle giocate della STESSA squadra
  +                for m in reversed(self.game_state["history"]):
  +                    prev_team = 0 if m.get("player") in [0, 2] else 1
  +                    if prev_team != team_id:
  +                        # mosse dell'altra squadra non interrompono né incrementano la serie
  +                        continue
  +                    if m.get("capture_type") == "scopa":
  +                        consecutive += 1
  +                    else:
  +                        # una giocata senza scopa della stessa squadra interrompe la serie
  +                        break
  +                if consecutive >= limit:
  +                    move_info["capture_type"] = "capture"
  +
  +        self.game_state["history"].append(move_info)
  +        # Aggiorna derived mirrors: progress len(history)/40 e last capturing team
  +        hlen = len(self.game_state["history"])
  +        pr = min(1.0, max(0.0, float(hlen) / 40.0))
  +        self._progress_t.fill_(pr)
  +        # last capturing team: cerca ultima presa/scopa
  +        l0 = l1 = 0.0
  +        for m in reversed(self.game_state["history"]):
  +            if m.get("capture_type") in ("capture", "scopa"):
  +                l0 = 1.0 if (m.get("player") in [0, 2]) else 0.0
  +                l1 = 1.0 - l0
  +                break
  +        self._last_capturing_team_t[0] = float(l0)
  +        self._last_capturing_team_t[1] = float(l1)
  +        # update history mirrors (ring buffer): encode move to 61-d without torch.roll
  +        from observation import encode_move
  +        enc = encode_move(move_info).to(device=device, dtype=torch.float32)
  +        H = int(self._hist_buf_t.size(0))
  +        head = int(self._hist_head_t.item())
  +        self._hist_buf_t[head] = enc
  +        # advance head and bump length up to capacity
  +        self._hist_head_t = (self._hist_head_t + 1) % H
  +        self._history_len_t = torch.clamp(self._history_len_t + 1, max=H)
  +        self._history_len = min(H, hlen)
  +        # update aggregates mirrors: played bits and scopa counts
  +        pc = move_info.get("played_card")
  +        if isinstance(pc, int):
  +            cid = int(pc)
  +        else:
  +            r, s = pc
  +            cid = int((int(r) - 1) * 4 + _suit_to_int(s))
  +        if 0 <= cid < 40:
  +            pidx = int(move_info.get("player", -1))
  +            if 0 <= pidx <= 3:
  +                self._played_bits_by_player_t[pidx] = self._played_bits_by_player_t[pidx] | (self._one_i64 << cid)
  +        if move_info.get("capture_type") == "scopa":
  +            team_id = 0 if current_player in [0, 2] else 1
  +            self._scopa_counts_t[team_id] = self._scopa_counts_t[team_id] + 1.0
  +            # update consecutive scopa streak for the acting team
  +            self._consec_scopa_team_t[team_id] = self._consec_scopa_team_t[team_id] + 1
  +        else:
  +            # reset acting team streak on non-scopa move
  +            team_id = 0 if current_player in [0, 2] else 1
  +            self._consec_scopa_team_t[team_id] = torch.zeros((), dtype=torch.int32, device=device)
  +        
  +        # OTTIMIZZAZIONE: Invalida la cache delle osservazioni (LRU)
  +        self._observation_cache = OrderedDict()
  +        
  +        # Verifica se la partita è finita
  +        done = all(len(self.game_state["hands"][p]) == 0 for p in range(4))
  +        self.done = done
  +        
  +        if done:
  +            # Non è consentito terminare una mano senza alcuna presa effettuata
  +            if not any(m.get("capture_type") in ("capture", "scopa") for m in self.game_state["history"]):
  +                raise ValueError("La mano non può terminare senza alcuna presa.")
  +            # Assegna le carte rimaste sul tavolo (opzionale)
  +            if self.game_state["table"]:
  +                if self.rules.get("last_cards_to_dealer", True):
  +                    last_capturing_team = None
  +                    for m in reversed(self.game_state["history"]):
  +                        if m["capture_type"] in ["capture", "scopa"]:
  +                            last_capturing_team = 0 if m["player"] in [0, 2] else 1
  +                            break
  +                    
  +                    if last_capturing_team is not None:
  +                        self.game_state["captured_squads"][last_capturing_team].extend(self.game_state["table"])
  +                        add_mask = torch.zeros((), dtype=torch.int64, device=device)
  +                        for cid in self.game_state["table"]:
  +                            add_mask = add_mask | (self._one_i64 << int(_card_to_id(cid) if not isinstance(cid, int) else int(cid)))
  +                        self._captured_bits_t[last_capturing_team] = self._captured_bits_t[last_capturing_team] | add_mask
  +                # In ogni caso svuota il tavolo a fine mano
  +                self.game_state["table"].clear()
  +                # reset cache table
  +                if self._use_id_cache:
  +                    self._table_ids.clear()
  +                    self._table_bits = 0
  +                    self._table_bits_t = torch.zeros((), dtype=torch.int64, device=device)
  +            
  +            # Calcolo punteggio finale
  +            from rewards import compute_final_score_breakdown, compute_final_reward_from_breakdown
  +            final_breakdown = compute_final_score_breakdown(self.game_state, rules=self.rules)
  +            final_reward = compute_final_reward_from_breakdown(final_breakdown)
  +            
  +            info = {
  +                "score_breakdown": final_breakdown,
  +                "team_rewards": [final_reward[0], final_reward[1]]
  +            }
  +            
  +            # OTTIMIZZAZIONE: Crea uno stato finale di zeri senza ricalcolo (rimane tensor su CUDA)
  +            obs_final = torch.zeros(self.observation_space.shape, dtype=torch.float32, device=device)
  +            
  +            # Aggiorna il tempo di esecuzione
  +            _end = time.time()
  +            if self._profiling:
  +                self._prof_step_apply += (_end - _t_prev)
  +                self._prof_step_total += (_end - _t_step_start)
  +                self._prof_step_count += 1
  +            self._step_time += _end - step_start_time
  +            
  +            # Restituisci la ricompensa finale per il team del giocatore corrente
  +            current_team = 0 if current_player in [0, 2] else 1
  +            return obs_final, float(final_reward[current_team]), True, info
  +        else:
  +            # Passa al prossimo giocatore
  +            self.current_player = (self.current_player + 1) % 4
  +            
  +            # OTTIMIZZAZIONE: Invalida la cache delle azioni valide
  +            
  +            # Ottieni la prossima osservazione
  +            next_obs = self._get_observation(self.current_player)
  +            
  +            # Aggiorna il tempo di esecuzione
  +            _end = time.time()
  +            if self._profiling:
  +                self._prof_step_apply += (_end - _t_prev)
  +                self._prof_step_total += (_end - _t_step_start)
  +                self._prof_step_count += 1
  +            self._step_time += _end - step_start_time
  +            
  +            return next_obs, shaped_reward, False, {"last_move": move_info}
  +    
  +    #@profile
  +    def _get_observation(self, player_id):
  +        """
  +        Versione ottimizzata per performance che utilizza caching aggressivo
  +        """
  +        start_time = time.time()
  +        
  +        # OTTIMIZZAZIONE: Chiave cache basata su bitset mirror (O(1)) + history_len
  +        hb = int(self._hands_bits_t[player_id].item())
  +        tb = int(self._table_bits_t.item())
  +        cb0 = int(self._captured_bits_t[0].item())
  +        cb1 = int(self._captured_bits_t[1].item())
  +        hlen = len(self.game_state.get("history", []))
  +        cache_key = (player_id, int(self.current_player), hb, tb, cb0, cb1, int(self.k_history), int(hlen))
  +        
  +        # Verifica la cache
  +        if cache_key in self._observation_cache:
  +            result = self._observation_cache[cache_key]
  +            # LRU refresh
  +            self._observation_cache.move_to_end(cache_key)
  +        else:
  +            # Calcola l'osservazione (solo compatta)
  +            from observation import encode_state_compact_for_player_fast
  +            # esponi mirrors per fast-path GPU
  +            self.game_state['_hands_bits_t'] = self._hands_bits_t
  +            self.game_state['_table_bits_t'] = self._table_bits_t
  +            self.game_state['_captured_bits_t'] = self._captured_bits_t
  +            # esponi derived mirrors (progress, last_capturing_team)
  +            self.game_state['_progress_t'] = self._progress_t
  +            self.game_state['_last_capturing_team_t'] = self._last_capturing_team_t
  +            # history mirrors for compile-friendly encoding
  +            self.game_state['_hist_buf_t'] = self._hist_buf_t
  +            self.game_state['_history_len_t'] = self._history_len_t
  +            # expose aggregates mirrors
  +            self.game_state['_played_bits_by_player_t'] = self._played_bits_by_player_t
  +            self.game_state['_scopa_counts_t'] = self._scopa_counts_t
  +            # probabilistic feature mirrors (if OBS_* enabled, compute eagerly once here)
--             if bool(OBS_INCLUDE_INFERRED):
--                 from observation import compute_inferred_probabilities
--                 ip = compute_inferred_probabilities(self.game_state, player_id)
--                 ip_t = ip if torch.is_tensor(ip) else torch.as_tensor(ip, dtype=torch.float32, device=device)
--                 self._inferred_probs_t.copy_(ip_t.to(dtype=torch.float32, device=device))
+++            if bool(OBS_INCLUDE_INFERRED) or bool(OBS_INCLUDE_INFERRED_L2) or bool(OBS_INCLUDE_INFERRED_L3):
+++                from observation import (
+++                    compute_inferred_probabilities,
+++                    compute_inferred_probabilities_level2,
+++                    compute_inferred_probabilities_level3,
+++                )
+++                if bool(OBS_INCLUDE_INFERRED):
+++                    ip = compute_inferred_probabilities(self.game_state, player_id)
+++                    ip_t = ip if torch.is_tensor(ip) else torch.as_tensor(ip, dtype=torch.float32, device=device)
+++                    self._inferred_probs_t.copy_(ip_t.to(dtype=torch.float32, device=device))
+++                if bool(OBS_INCLUDE_INFERRED_L2):
+++                    ip2 = compute_inferred_probabilities_level2(self.game_state, player_id)
+++                    ip2_t = ip2 if torch.is_tensor(ip2) else torch.as_tensor(ip2, dtype=torch.float32, device=device)
+++                    self._inferred_probs_l2_t.copy_(ip2_t.to(dtype=torch.float32, device=device))
+++                if bool(OBS_INCLUDE_INFERRED_L3):
+++                    ip3 = compute_inferred_probabilities_level3(self.game_state, player_id)
+++                    ip3_t = ip3 if torch.is_tensor(ip3) else torch.as_tensor(ip3, dtype=torch.float32, device=device)
+++                    self._inferred_probs_l3_t.copy_(ip3_t.to(dtype=torch.float32, device=device))
  +            if bool(OBS_INCLUDE_RANK_PROBS):
  +                from observation import compute_rank_probabilities_by_player
  +                rpb = compute_rank_probabilities_by_player(self.game_state, player_id).flatten()
  +                self._rank_probs_by_player_t.copy_(rpb.to(dtype=torch.float32, device=device))
  +                # derive rank_presence_from_inferred (30) from inferred or rank_probs if needed
  +                if bool(OBS_INCLUDE_INFERRED):
  +                    # reshape 3x40 -> sum over suits to 3x10
  +                    ip3x40 = self._inferred_probs_t.view(3, 40)
  +                    rp = ip3x40.view(3, 10, 4).sum(dim=2).reshape(-1)
  +                    self._rank_presence_from_inferred_t.copy_(rp.to(dtype=torch.float32, device=device))
  +            if bool(OBS_INCLUDE_SCOPA_PROBS):
  +                from observation import compute_next_player_scopa_probabilities
  +                sp = compute_next_player_scopa_probabilities(self.game_state, player_id)
  +                sp_t = sp if torch.is_tensor(sp) else torch.as_tensor(sp, dtype=torch.float32, device=device)
  +                self._scopa_probs_t.copy_(sp_t.to(dtype=torch.float32, device=device))
  +            # esponi anche il current_player per feature derivate (es. dealer)
  +            self.game_state['current_player'] = int(self.current_player)
  +            # Reuse preallocated output buffer when shape matches
  +            if self._obs_out_buf is None:
  +                # compute expected size based on flags
  +                include_rank = bool(OBS_INCLUDE_RANK_PROBS)
  +                include_scopa = bool(OBS_INCLUDE_SCOPA_PROBS)
  +                include_inferred = bool(OBS_INCLUDE_INFERRED)
  +                include_dealer = bool(OBS_INCLUDE_DEALER)
  +                fixed = 43 + 40 + 82 + 61 * self.k_history + 40 + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
  +                exp = fixed + (120 if include_inferred else 0) + (10 if include_scopa else 0) + (150 if include_rank else 0) + (4 if include_dealer else 0)
  +                self._obs_out_buf = torch.empty((exp,), dtype=torch.float32, device=device)
  +            encoded = self._encode_obs(self.game_state, player_id, k_history=self.k_history, out=self._obs_out_buf)
  +            # Decouple cached tensors from the shared staging buffer to avoid aliasing corruption.
  +            result = encoded.clone()
  +
  +            # Salva in cache (LRU) senza forzare CPU: lascia il tensore sul suo device
  +            self._observation_cache[cache_key] = result
  +            self._observation_cache.move_to_end(cache_key)
  +            while len(self._observation_cache) > self._cache_capacity:
  +                self._observation_cache.popitem(last=False)
  +        
  +        # Aggiorna il tempo di esecuzione
  +        self._get_obs_time += time.time() - start_time
  +        
  +        # Ritorna direttamente il tensore sul suo device
  +        return result
  +    
  +    def reset(self, starting_player=None):
  +        """Versione ottimizzata di reset"""
  +        # Reimposta lo stato del gioco rispettando le regole/varianti
  +        self.game_state = initialize_game(rules=self.rules)
  +        # Mantieni lo stato in ID senza riconversioni
  +        self.done = False
  +        self.current_player = starting_player if starting_player is not None else 0
  +        self.rewards = [0, 0]
  +        
  +        # Reset delle cache (LRU)
  +        self._valid_actions_cache = OrderedDict()
  +        self._observation_cache = OrderedDict()
  +        # Ricostruisci cache ID/bitset
  +        # se in stato ID
  +        if isinstance(self.game_state['hands'][0][0], int):
  +            # popola cache con ID direttamente
  +            for p in range(4):
  +                self._hands_ids[p] = list(self.game_state['hands'][p])
  +                self._hands_bits[p] = _ids_to_bitset(self._hands_ids[p]) if self._use_bitset else 0
  +                self._hands_bits_t[p] = torch.as_tensor(self._hands_bits[p], dtype=torch.int64, device=device)
  +            self._table_ids = list(self.game_state['table'])
  +            self._table_bits = _ids_to_bitset(self._table_ids) if self._use_bitset else 0
  +            self._table_bits_t = torch.as_tensor(self._table_bits, dtype=torch.int64, device=device)
  +        else:
  +            self._rebuild_id_caches()
  +        # reset captured bits tensors
  +        bits0 = _ids_to_bitset([_card_to_id(c) for c in self.game_state['captured_squads'][0]])
  +        bits1 = _ids_to_bitset([_card_to_id(c) for c in self.game_state['captured_squads'][1]])
  +        self._captured_bits_t[0] = torch.as_tensor(bits0, dtype=torch.int64, device=device)
  +        self._captured_bits_t[1] = torch.as_tensor(bits1, dtype=torch.int64, device=device)
  +        # Reset derived mirrors
  +        hlen = len(self.game_state.get("history", []))
  +        pr = min(1.0, max(0.0, float(hlen) / 40.0))
  +        self._progress_t.fill_(pr)
  +        self._last_capturing_team_t.zero_()
  +        # Reset history mirrors
  +        self._hist_buf_t.zero_()
  +        self._history_len_t.zero_()
  +        self._history_len = hlen
  +        # Reset aggregates mirrors
  +        self._played_bits_by_player_t.zero_()
  +        self._scopa_counts_t.zero_()
  +        self._attach_state_views()
  +
  +        # Ottieni l'osservazione iniziale
  +        return self._get_observation(self.current_player)
  +    
  +    def consume_profile_stats(self):
  +        if not self._profiling:
  +            return {}
  +        out = {}
  +        if self._prof_step_count > 0:
  +            count = self._prof_step_count
  +            out.update({
  +                'step/count': count,
  +                'step/total': self._prof_step_total,
  +                'step/decode_total': self._prof_step_decode,
  +                'step/validate_total': self._prof_step_validate,
  +                'step/apply_total': self._prof_step_apply,
  +                'step/decode_avg': self._prof_step_decode / count,
  +                'step/validate_avg': self._prof_step_validate / count,
  +                'step/apply_avg': self._prof_step_apply / count,
  +                'step/total_avg': self._prof_step_total / count,
  +            })
  +        if self._prof_legals_calls > 0:
  +            calls = self._prof_legals_calls
  +            out.update({
  +                'legals/calls': calls,
  +                'legals/total': self._prof_legals_total,
  +                'legals/avg': self._prof_legals_total / calls,
  +            })
  +        # Reset accumulators
  +        self._prof_step_total = 0.0
  +        self._prof_step_decode = 0.0
  +        self._prof_step_validate = 0.0
  +        self._prof_step_apply = 0.0
  +        self._prof_step_count = 0
  +        self._prof_legals_total = 0.0
  +        self._prof_legals_calls = 0
  +        return out
  +
  +    def print_stats(self):
  +        """Stampa statistiche di performance"""
  +        if self._step_count > 0:
  +            print(f"Environment stats after {self._step_count} steps:")
  +            print(f"  Avg. _get_observation time: {self._get_obs_time / self._step_count * 1000:.2f} ms")
  +            print(f"  Avg. get_valid_actions time: {self._get_valid_actions_time / self._step_count * 1000:.2f} ms")
  +            print(f"  Avg. step time: {self._step_time / self._step_count * 1000:.2f} ms")
  +            
  +            total_cache = self._cache_hits + self._cache_misses
  +            if total_cache > 0:
  +                hit_rate = self._cache_hits / total_cache * 100
  +                print(f"  Action cache hit rate: {hit_rate:.1f}% ({self._cache_hits}/{total_cache})")
diff --cc evaluation/eval.py
index 00865e9,00865e9,0000000..21f2a4c
mode 100644,100644,000000..100644
--- a/evaluation/eval.py
+++ b/evaluation/eval.py
@@@@ -1,957 -1,957 -1,0 +1,709 @@@@
  +import os
  +# Default compile-friendly settings (work on CPU and CUDA)
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE', '0')
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE_MODE', 'max-autotune')
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE_BACKEND', 'inductor')
  +os.environ.setdefault('SCOPONE_INDUCTOR_AUTOTUNE', '1')
  +import torch
  +import random
  +import time
  +import platform
  +import multiprocessing as mp
  +from tqdm import tqdm
  +import shutil
  +from typing import Tuple
  +from environment import ScoponeEnvMA
  +from heuristics.baseline import pick_action_heuristic
  +from selfplay.league import League
  +try:
  +    from torch.utils.tensorboard import SummaryWriter as _SummaryWriter
  +    TB_AVAILABLE = True
  +except Exception:
  +    TB_AVAILABLE = False
+++from belief import sample_determinization
  +from models.action_conditioned import ActionConditionedActor
  +from utils.compile import maybe_compile_module
  +from algorithms.is_mcts import run_is_mcts
  +# BeliefState legacy opzionale (non usato nello scenario corrente)
  +
  +# Force evaluation on CPU across the project for consistency and to avoid GPU H2D overhead in unbatched eval
  +device = torch.device('cpu')
  +
  +# Debug helper for parallel eval
  +_DEF_FALSE = ['0','false','no','off']
  +_DEF_TRUE = ['1','true','yes','on']
  +
  +def _eval_debug_enabled() -> bool:
  +    try:
  +        return str(os.environ.get('SCOPONE_EVAL_DEBUG', '0')).strip().lower() in _DEF_TRUE
  +    except Exception:
  +        return False
  +
  +def _dbg(msg: str):
  +    if _eval_debug_enabled():
  +        try:
  +            tqdm.write(f"[eval-debug] {msg}")
  +        except Exception:
  +            print(f"[eval-debug] {msg}", flush=True)
  +
  +def _get_mp_ctx():
  +    # Decide safe start method for parallel eval
  +    meth = (os.environ.get('SCOPONE_EVAL_MP_START')
  +            or os.environ.get('SCOPONE_MP_START')
  +            or '').strip().lower()
  +    if meth in ('spawn', 'fork', 'forkserver'):
  +        _dbg(f"using mp start method from env: {meth}")
  +        return mp.get_context(meth)
  +    # Heuristics: prefer spawn on WSL2 or when CUDA is available; else forkserver
  +    try:
  +        is_wsl = ('WSL_INTEROP' in os.environ) or ('microsoft' in platform.release().lower())
  +    except Exception:
  +        is_wsl = False
  +    try:
  +        has_cuda = torch.cuda.is_available() or str(os.environ.get('SCOPONE_TRAIN_DEVICE','cpu')).startswith('cuda')
  +    except Exception:
  +        has_cuda = False
  +    if is_wsl or has_cuda:
  +        _dbg("using mp start method: spawn (WSL/CUDA heuristic)")
  +        return mp.get_context('spawn')
  +    _dbg("using mp start method: forkserver (default)")
  +    return mp.get_context('forkserver')
  +
  +def _get_eval_pool_timeout_seconds():
  +    """
  +    Parse SCOPONE_EVAL_POOL_TIMEOUT_S from environment.
  +    Returns:
  +        float timeout in seconds if > 0; None to indicate 'wait indefinitely'.
  +    Accepted 'off' values: '0', 'none', 'inf', 'infinity', negatives.
  +    """
  +    try:
  +        raw = str(os.environ.get('SCOPONE_EVAL_POOL_TIMEOUT_S', '600')).strip().lower()
  +        if raw in ('none', 'inf', 'infinity'):
  +            return None
  +        val = float(raw)
  +        if val <= 0.0:
  +            return None
  +        return float(val)
  +    except Exception:
  +        return 600.0
  +
  +def _true_env(val: str) -> bool:
  +    return str(val).strip().lower() in _DEF_TRUE
  +
  +def _eval_progress_alpha(env, start: float, full: float) -> float:
  +    try:
  +        hist = env.game_state.get('history', []) if hasattr(env, 'game_state') and isinstance(env.game_state, dict) else []
  +        progress = float(min(1.0, max(0.0, (len(hist) if isinstance(hist, list) else 0) / 40.0)))
  +    except Exception:
  +        progress = 0.0
  +    denom = max(1e-6, float(full) - float(start))
  +    alpha = min(1.0, max(0.0, (progress - float(start)) / denom))
  +    return float(alpha)
  +
  +def _eval_resolve_mcts_params(env, mcts_cfg: dict | None):
  +    """Return (sims_scaled, root_temp_dyn) for eval MCTS, mirroring training scaling.
  +    Uses env overrides when available:
  +      - SCOPONE_EVAL_MCTS_SCALING (default '1')
  +      - SCOPONE_EVAL_MCTS_PROGRESS_START (default '0.25')
  +      - SCOPONE_EVAL_MCTS_PROGRESS_FULL (default '0.75')
  +      - SCOPONE_EVAL_MCTS_MIN_SIMS (default '0')
  +      - SCOPONE_EVAL_MCTS_TRAIN_FACTOR (default '1.0')
  +    """
  +    if mcts_cfg is None:
  +        return 0, 0.0
  +    try:
  +        sims_def = int(mcts_cfg.get('sims', 128))
  +    except Exception:
  +        sims_def = 128
  +    scaling_on = _true_env(os.environ.get('SCOPONE_EVAL_MCTS_SCALING', '1'))
  +    try:
  +        ps = float(mcts_cfg.get('progress_start', os.environ.get('SCOPONE_EVAL_MCTS_PROGRESS_START', '0.25')))
  +        pf = float(mcts_cfg.get('progress_full', os.environ.get('SCOPONE_EVAL_MCTS_PROGRESS_FULL', '0.75')))
  +        min_sims = int(mcts_cfg.get('min_sims', os.environ.get('SCOPONE_EVAL_MCTS_MIN_SIMS', '0')))
  +        train_factor = float(mcts_cfg.get('train_factor', os.environ.get('SCOPONE_EVAL_MCTS_TRAIN_FACTOR', '1.0')))
  +    except Exception:
  +        ps, pf, min_sims, train_factor = 0.25, 0.75, 0, 1.0
  +    alpha = _eval_progress_alpha(env, ps, pf)
  +    if not scaling_on:
  +        sims_scaled = int(max(0, sims_def))
  +    else:
  +        import math as _math
  +        sims_base = int(_math.ceil(float(sims_def) * (0.25 + 0.75 * alpha)))
  +        sims_base = int(_math.ceil(float(sims_base) * float(train_factor)))
  +        sims_scaled = int(max(int(min_sims), sims_base))
  +        if int(sims_def) > 0 and sims_scaled <= 0:
  +            sims_scaled = 1
  +    # Dynamic root temperature like training: if root_temp<=0, use (1 - alpha)
  +    try:
  +        rt = float(mcts_cfg.get('root_temp', 0.0))
  +    except Exception:
  +        rt = 0.0
  +    root_temp_dyn = float(rt) if float(rt) > 0.0 else float(max(0.0, 1.0 - alpha))
  +    return int(sims_scaled), float(root_temp_dyn)
  +
  +def _terminal_ncols() -> int:
  +    try:
  +        return max(60, int(shutil.get_terminal_size(fallback=(120, 20)).columns))
  +    except Exception:
  +        return 120
  +
  +def _fit_desc(desc: str, reserve: int = 48) -> str:
  +    cols = _terminal_ncols()
  +    max_len = max(8, cols - int(reserve))
  +    if len(desc) <= max_len:
  +        return desc
  +    return desc[:max_len - 1] + '…'
  +
  +
  +def play_match(agent_fn_team0, agent_fn_team1, games: int = 50, k_history: int = 12,
  +               tqdm_desc: str = None, tqdm_position: int = 0, tqdm_disable: bool = False) -> Tuple[float, dict]:
  +    """
  +    Gioca N partite e ritorna diff media (punti actor0 − punti actor1) e breakdown medio dei punteggi.
  +    Il team che inizia è sorteggiato alla prima partita e poi alternato; inoltre, le posizioni
  +    dei team ai posti (0/2 vs 1/3) vengono invertite a ogni partita per equità.
  +    agent_fn_*: callable(env) -> action (usa env.get_valid_actions())
  +    """
  +    # Evaluation runs strictly on CPU (independent from training device)
  +    wins = 0
  +    actor_tot_sum = {0: 0.0, 1: 0.0}
  +    breakdown_sum = {0: {'carte': 0.0, 'denari': 0.0, 'settebello': 0.0, 'primiera': 0.0, 'scope': 0.0, 'total': 0.0},
  +                     1: {'carte': 0.0, 'denari': 0.0, 'settebello': 0.0, 'primiera': 0.0, 'scope': 0.0, 'total': 0.0}}
  +    def _to_float_scalar(x):
  +            return float(x)
  +
  +    bar_desc = _fit_desc(tqdm_desc or 'Eval matches')
  +    _ncols = _terminal_ncols()
  +    _disable = bool(os.environ.get('TQDM_DISABLE','0') in ['1','true','yes','on']) or bool(tqdm_disable)
  +    # Estrai team iniziale che parte (0 o 1) e alterna ad ogni partita
  +    start_team = int(random.random() < 0.5)
  +    for gi in tqdm(range(games), desc=bar_desc, position=tqdm_position, dynamic_ncols=False, ncols=_ncols,
  +                  leave=True, disable=_disable, miniters=1, mininterval=0.05, smoothing=0):
  +        env = ScoponeEnvMA(k_history=k_history)
  +        invert_seats = (gi % 2) == 1  # inverti mappatura posti: (0,2)<->(1,3)
  +        team_start = start_team ^ (gi & 1)
  +        # Calcola starting_player coerente con inversione posti
  +        if not invert_seats:
  +            starting_player = (0 if team_start == 0 else 1)
  +        else:
  +            starting_player = (1 if team_start == 0 else 0)
  +        env.reset(starting_player=starting_player)
  +        done = False
  +        info = {}
  +        while not done:
  +            legals = env.get_valid_actions()
  +            # Robust emptiness check: supports list/tuple or tensor returns
  +            _empty = False
  +            if legals is None:
  +                _empty = True
  +            elif isinstance(legals, (list, tuple)):
  +                _empty = (len(legals) == 0)
  +            elif torch.is_tensor(legals):
  +                _empty = (legals.numel() == 0) or (legals.shape[0] == 0)
  +            else:
  +                _empty = (len(legals) == 0)
  +            if _empty:
  +                break
  +            # Se invert_seats, il team0 actor gioca sui posti 1/3 (parità invertita)
  +            if not invert_seats:
  +                if env.current_player in [0, 2]:
  +                    action = agent_fn_team0(env)
  +                else:
  +                    action = agent_fn_team1(env)
  +            else:
  +                if env.current_player in [0, 2]:
  +                    action = agent_fn_team1(env)
  +                else:
  +                    action = agent_fn_team0(env)
  +            _, _, done, info = env.step(action)
  +        if 'score_breakdown' in info:
  +            bd = info['score_breakdown']
  +            for t in [0, 1]:
  +                for k in breakdown_sum[t].keys():
  +                    breakdown_sum[t][k] += _to_float_scalar(bd[t].get(k, 0))
  +            _t0 = _to_float_scalar(bd[0].get('total', 0))
  +            _t1 = _to_float_scalar(bd[1].get('total', 0))
  +            if not invert_seats:
  +                actor_tot_sum[0] += _t0
  +                actor_tot_sum[1] += _t1
  +                if _t0 > _t1:
  +                    wins += 1
  +            else:
  +                actor_tot_sum[0] += _t1
  +                actor_tot_sum[1] += _t0
  +                if _t1 > _t0:
  +                    wins += 1
  +        elif 'team_rewards' in info:
  +            tr = info['team_rewards']
  +            _r0 = _to_float_scalar(tr[0])
  +            _r1 = _to_float_scalar(tr[1])
  +            if not invert_seats:
  +                actor_tot_sum[0] += _r0
  +                actor_tot_sum[1] += _r1
  +                if _r0 > _r1:
  +                    wins += 1
  +            else:
  +                actor_tot_sum[0] += _r1
  +                actor_tot_sum[1] += _r0
  +                if _r1 > _r0:
  +                    wins += 1
  +    # medie
  +    for t in [0, 1]:
  +        for k in breakdown_sum[t].keys():
  +            breakdown_sum[t][k] /= games
  +    wr = wins / games if games > 0 else 0.0
  +    breakdown_sum['meta'] = {'win_rate': float(wr)}
  +    diff_avg = (actor_tot_sum[0] - actor_tot_sum[1]) / float(games) if games > 0 else 0.0
  +    return float(diff_avg), breakdown_sum
  +
  +
  +def series_to_points(win_func, target_points=11):
  +    """Gioca una serie a target_points (es. 11) e ritorna vincitore (0/1)."""
  +    s0 = 0
  +    s1 = 0
  +    while s0 < target_points and s1 < target_points:
  +        w = win_func()
  +        if w:
  +            s0 += 1
  +        else:
  +            s1 += 1
  +    return 0 if s0 >= target_points else 1
  +
  +
  +def eval_vs_baseline(games=50, k_history=12, log_tb=False):
  +    writer = None
  +    if log_tb and TB_AVAILABLE and os.environ.get('SCOPONE_DISABLE_TB', '0') != '1':
  +        writer = _SummaryWriter(log_dir='runs/eval')
  +    def agent_fn_team0(env):
  +        # actor placeholder: usa euristica come baseline anche per team0 se serve
  +        return pick_action_heuristic(env.get_valid_actions())
  +    def agent_fn_team1(env):
  +        return pick_action_heuristic(env.get_valid_actions())
  +    wr, bd = play_match(agent_fn_team0, agent_fn_team1, games, k_history)
  +    if writer is not None:
  +        writer.add_scalar('eval/win_rate_team0', wr, 0)
  +        writer.close()
  +    return wr, bd
  +
  +
  +def evaluate_pair_actors(ckpt_a: str, ckpt_b: str, games: int = 10,
  +                         k_history: int = 12,
  +                         mcts: dict = None,
  +                         belief_particles: int = 0, belief_ess_frac: float = 0.5,
  +                         tqdm_desc: str = None, tqdm_position: int = 0, tqdm_disable: bool = False):
  +    """
  +    Valuta due checkpoint (A vs B) giocando N partite. Ritorna diff media (punti A − punti B) e breakdown medio.
  +    - Se mcts è fornito, usa IS-MCTS con i parametri dati per la selezione.
  +    - belief_particles>0 abilita belief a particelle per prior MCTS.
  +    """
  +    # Primo env per determinare obs_dim
  +    env0 = ScoponeEnvMA(k_history=k_history)
  +    obs_dim = env0.observation_space.shape[0]
  +    del env0
  +    # Carica attori
  +    actor_a = maybe_compile_module(ActionConditionedActor(obs_dim=obs_dim), name='ActionConditionedActor[eval_A]').to(device)
  +    actor_b = maybe_compile_module(ActionConditionedActor(obs_dim=obs_dim), name='ActionConditionedActor[eval_B]').to(device)
  +    if ckpt_a and os.path.isfile(ckpt_a):
  +        st_a = torch.load(ckpt_a, map_location=device)
  +        if isinstance(st_a, dict) and 'actor' in st_a:
  +            actor_a.load_state_dict(st_a['actor'])
  +    if ckpt_b and os.path.isfile(ckpt_b):
  +        st_b = torch.load(ckpt_b, map_location=device)
  +        if isinstance(st_b, dict) and 'actor' in st_b:
  +            actor_b.load_state_dict(st_b['actor'])
  +    actor_a.eval(); actor_b.eval()
  +
  +    def make_agent_fn(actor_model):
  +        def _select(env: ScoponeEnvMA):
  +            legals = env.get_valid_actions()
  +            cp = env.current_player
  +            # seat/team vec
  +            seat_vec = torch.zeros(6, dtype=torch.float32, device='cpu')
  +            seat_vec[cp] = 1.0
  +            seat_vec[4] = 1.0 if cp in [0, 2] else 0.0
  +            seat_vec[5] = 1.0 if cp in [1, 3] else 0.0
  +            # belief
  +            bsum = None
  +            # MCTS path
  +            if mcts is not None and len(legals) > 1:
  +                belief_obj = None
  +                def policy_fn(obs, legal_list):
  +                    # compute scores with actor on GPU
  +                    o_cpu = obs if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32)
  +                    if torch.is_tensor(o_cpu):
  +                        o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
  +                    leg_cpu = torch.stack([
  +                        (x.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32))
  +                    for x in legal_list], dim=0)
  +                    s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
  +                    if device.type == 'cuda':
  +                        o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
  +                        leg_t = leg_cpu.pin_memory().to(device=device, non_blocking=True)
  +                        s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
  +                    else:
  +                        o_t = o_cpu.unsqueeze(0).to(device=device)
  +                        leg_t = leg_cpu.to(device=device)
  +                        s_t = s_cpu.unsqueeze(0).to(device=device)
  +                    with torch.no_grad():
  +                        logits = actor_model(o_t, leg_t, s_t)
  +                        probs = torch.softmax(logits, dim=0).detach().cpu().numpy()
  +                    return probs
  +                # belief sampler neurale
  +                def belief_sampler_neural(_env):
--                     obs_cur = _env._get_observation(_env.current_player)
--                     o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
--                     if torch.is_tensor(o_cpu):
--                         o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
--                     s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
--                     o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                     s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                     with torch.no_grad():
--                         state_feat = actor_model.state_enc(o_t, s_t)
--                         logits = actor_model.belief_net(state_feat)
--                         hand_table = o_t[:, :83]
--                         hand_mask = hand_table[:, :40] > 0.5
--                         table_mask = hand_table[:, 43:83] > 0.5
--                         captured = o_t[:, 83:165]
--                         cap0_mask = captured[:, :40] > 0.5
--                         cap1_mask = captured[:, 40:80] > 0.5
--                         visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
--                         probs_flat = actor_model.belief_net.probs(logits, visible_mask)
--                     probs = probs_flat.view(3, 40).detach().cpu().numpy()
--                     det = {}
--                     others = [(_env.current_player + 1) % 4, (_env.current_player + 2) % 4, (_env.current_player + 3) % 4]
--                     for i, pid in enumerate(others):
--                         det[pid] = []
--                     vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
--                     unknown_ids = [cid for cid in range(40) if not vis[cid]]
--                     counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
--                     caps = [int(counts.get(pid, 0)) for pid in others]
--                     n = len(unknown_ids)
--                     if sum(caps) != n:
--                         caps[2] = max(0, n - caps[0] - caps[1])
--                     # semplice greedy per eval (si può allineare alla DP del trainer se serve)
--                     for cid in unknown_ids:
--                         pc = probs[:, cid]
--                         ps = pc / max(1e-9, pc.sum())
--                         j = int(torch.argmax(torch.tensor(ps)).item())
--                         if caps[j] > 0:
--                             det[others[j]].append(cid)
--                             caps[j] -= 1
--                     return det
--                 sims_scaled, root_temp_dyn = _eval_resolve_mcts_params(env, dict(mcts))
--                 mc = dict(mcts)
--                 mc['sims'] = int(sims_scaled)
--                 mc['root_temp'] = float(root_temp_dyn)
--                 action = run_is_mcts(
--                     env,
--                     policy_fn=policy_fn,
--                     value_fn=lambda _o, _e: 0.0,  # solo policy-guided in eval rapida
--                     num_simulations=int(mc.get('sims', 128)),
--                     c_puct=float(mc.get('c_puct', 1.0)),
--                     belief=None,
--                     num_determinization=int(mc.get('dets', 1)),
--                     root_temperature=float(mc.get('root_temp', 0.0)),
--                     prior_smooth_eps=float(mc.get('prior_smooth_eps', 0.0)),
--                     robust_child=True,
--                     root_dirichlet_alpha=float(mc.get('root_dirichlet_alpha', 0.25)),
--                     root_dirichlet_eps=float(mc.get('root_dirichlet_eps', 0.25)),
--                     belief_sampler=belief_sampler_neural
--                 )
--                 return action
--             # Greedy actor selection con belief neurale
--             obs = env._get_observation(cp)
--             o_cpu = obs if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32)
--             if torch.is_tensor(o_cpu):
--                 o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
--             leg_cpu = torch.stack([
--                 (x.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32))
--             for x in legals], dim=0)
--             s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
--             if device.type == 'cuda':
--                 o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                 leg_t = leg_cpu.pin_memory().to(device=device, non_blocking=True)
--                 s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--             else:
--                 o_t = o_cpu.unsqueeze(0).to(device=device)
--                 leg_t = leg_cpu.to(device=device)
--                 s_t = s_cpu.unsqueeze(0).to(device=device)
--             with torch.no_grad():
--                 logits = actor_model(o_t, leg_t, s_t)
--                 idx = torch.argmax(logits).to('cpu')
--             return leg_cpu[idx]
--         # If a global line-profiler decorator is provided, wrap the closure so time
--         # is attributed inside the function rather than to its call sites.
--         _LP = globals().get('LINE_PROFILE_DECORATOR', None)
--         if _LP is not None:
--             _select = _LP(_select)
--         return _select
-- 
--     t_eval_start = time.time()
--     agent_fn_team0 = make_agent_fn(actor_a)
--     agent_fn_team1 = make_agent_fn(actor_b)
--     # Default description uses basename of checkpoints
--     if tqdm_desc is None:
--         a_name = os.path.basename(ckpt_a) if ckpt_a else 'A'
--         b_name = os.path.basename(ckpt_b) if ckpt_b else 'B'
--         tqdm_desc = f"Eval {a_name} vs {b_name}"
--     # Valuta workers paralleli da env: SCOPONE_EVAL_WORKERS>1 abilita versione parallela
--     num_workers_env = int(os.environ.get('SCOPONE_EVAL_WORKERS', '1'))
--     if num_workers_env > 1:
--         _dbg(f"parallel eval requested: workers={num_workers_env} games={int(games)} dets={(mcts or {}).get('dets', 1)} desc={tqdm_desc}")
--         # Distribuisci le determinizzazioni tra i worker: dets_per_worker >= 1
--         dets_total = int((mcts or {}).get('dets', 1))
--         if dets_total > 1:
--             dets_per_worker = max(1, dets_total // num_workers_env)
--             dets_rem = dets_total - dets_per_worker * num_workers_env
--             # Costruisci per-worker mcts con dets distribuiti quasi uniformemente
--             mcts_base = dict(mcts or {})
--             mcts_list = []
--             for i in range(num_workers_env):
--                 d = dets_per_worker + (1 if i < dets_rem else 0)
--                 mc = dict(mcts_base)
--                 mc['dets'] = int(max(1, d))
--                 mcts_list.append(mc)
--             _dbg(f"distributing dets across workers: total={dets_total} list={[mc['dets'] for mc in mcts_list]}")
--             diff, bd = evaluate_pair_actors_parallel_dist(ckpt_a, ckpt_b, games=games, k_history=k_history,
--                                                           mcts_list=mcts_list, belief_particles=belief_particles, belief_ess_frac=belief_ess_frac,
--                                                           num_workers=num_workers_env,
--                                                           tqdm_desc=tqdm_desc, tqdm_position=int(tqdm_position or 0), tqdm_disable=bool(tqdm_disable))
--             elapsed = time.time() - t_eval_start
--             try:
--                 tqdm.write(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers={num_workers_env}, dist-dets)")
--             except Exception:
--                 print(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers={num_workers_env}, dist-dets)", flush=True)
--             return diff, bd
--         # Nessuna multi-dets: usa parallelo standard
--         _dbg("starting parallel eval without multi-dets distribution")
--         diff, bd = evaluate_pair_actors_parallel(ckpt_a, ckpt_b, games=games, k_history=k_history,
--                                              mcts=mcts, belief_particles=belief_particles, belief_ess_frac=belief_ess_frac,
--                                              num_workers=num_workers_env,
--                                              tqdm_desc=tqdm_desc, tqdm_position=int(tqdm_position or 0), tqdm_disable=bool(tqdm_disable))
--         elapsed = time.time() - t_eval_start
--         try:
--             tqdm.write(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers={num_workers_env})")
--         except Exception:
--             print(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers={num_workers_env})", flush=True)
--         return diff, bd
--     else:
--         diff, bd = play_match(agent_fn_team0, agent_fn_team1, games=games, k_history=k_history,
--                             tqdm_desc=tqdm_desc, tqdm_position=int(tqdm_position or 0), tqdm_disable=bool(tqdm_disable))
--         elapsed = time.time() - t_eval_start
--         try:
--             tqdm.write(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers=1)")
--         except Exception:
--             print(f"[eval] {tqdm_desc}: {int(games)} games in {elapsed:.2f}s (workers=1)", flush=True)
--         return diff, bd
-- 
-- 
-- def _eval_pair_chunk_worker(args):
--     """Worker: esegue un sottoinsieme di partite e ritorna (wins_int, breakdown_sum_dict)."""
--     # Args can be (wid, ckpt_a, ckpt_b, games, ...) or without wid
--     if len(args) == 8:
--         wid, ckpt_a, ckpt_b, games, k_history, mcts, belief_particles, belief_ess_frac = args
--     else:
--         wid = -1
--         (ckpt_a, ckpt_b, games, k_history, mcts, belief_particles, belief_ess_frac) = args
--     _dbg(f"worker[{wid}] start: games={int(games)} k_history={int(k_history)} dets={(mcts or {}).get('dets',1)}")
--     # Limit CPU threads per worker
--     try:
--         wt = int(os.environ.get('SCOPONE_WORKER_THREADS', '1'))
--         os.environ['OMP_NUM_THREADS'] = str(wt)
--         os.environ['MKL_NUM_THREADS'] = str(wt)
--         torch.set_num_threads(wt)
--         torch.set_num_interop_threads(1)
--     except Exception:
--         pass
--     # Ricrea attori in ciascun processo
--     from models.action_conditioned import ActionConditionedActor
--     from utils.compile import maybe_compile_module
--     from environment import ScoponeEnvMA
--     import os as _os
--     _os.environ['TQDM_DISABLE'] = '1'
--     env0 = ScoponeEnvMA(k_history=k_history)
--     obs_dim = env0.observation_space.shape[0]
--     del env0
--     _dbg(f"worker[{wid}] loading actors …")
--     actor_a = maybe_compile_module(ActionConditionedActor(obs_dim=obs_dim), name='ActionConditionedActor[eval_A]').to(device)
--     actor_b = maybe_compile_module(ActionConditionedActor(obs_dim=obs_dim), name='ActionConditionedActor[eval_B]').to(device)
--     if ckpt_a and os.path.isfile(ckpt_a):
--         st_a = torch.load(ckpt_a, map_location=device)
--         if isinstance(st_a, dict) and 'actor' in st_a:
--             actor_a.load_state_dict(st_a['actor'])
--     if ckpt_b and os.path.isfile(ckpt_b):
--         st_b = torch.load(ckpt_b, map_location=device)
--         if isinstance(st_b, dict) and 'actor' in st_b:
--             actor_b.load_state_dict(st_b['actor'])
--     actor_a.eval(); actor_b.eval()
--     _dbg(f"worker[{wid}] actors ready; starting matches …")
-- 
--     def make_agent_fn(actor_model):
--         def _select(env: ScoponeEnvMA):
--             legals = env.get_valid_actions()
--             cp = env.current_player
--             seat_vec = torch.zeros(6, dtype=torch.float32, device='cpu')
--             seat_vec[cp] = 1.0
--             seat_vec[4] = 1.0 if cp in [0, 2] else 0.0
--             seat_vec[5] = 1.0 if cp in [1, 3] else 0.0
--             if mcts is not None and len(legals) > 1:
--                 belief_obj = None
--                 def policy_fn(obs, legal_list):
--                     o_cpu = obs if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32)
--                     if torch.is_tensor(o_cpu):
--                         o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
--                     leg_cpu = torch.stack([
--                         (x.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32))
--                     for x in legal_list], dim=0)
--                     s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
--                     o_t = o_cpu.unsqueeze(0).to(device=device)
--                     leg_t = leg_cpu.to(device=device)
--                     s_t = s_cpu.unsqueeze(0).to(device=device)
--                     with torch.no_grad():
--                         logits = actor_model(o_t, leg_t, s_t)
--                         probs = torch.softmax(logits, dim=0).detach().cpu().numpy()
--                     return probs
--                 def belief_sampler_neural(_env):
--                     obs_cur = _env._get_observation(_env.current_player)
--                     o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
--                     if torch.is_tensor(o_cpu):
--                         o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
--                     s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
--                     o_t = o_cpu.unsqueeze(0).to(device=device)
--                     s_t = s_cpu.unsqueeze(0).to(device=device)
--                     with torch.no_grad():
--                         state_feat = actor_model.state_enc(o_t, s_t)
--                         logits = actor_model.belief_net(state_feat)
--                         hand_table = o_t[:, :83]
--                         hand_mask = hand_table[:, :40] > 0.5
--                         table_mask = hand_table[:, 43:83] > 0.5
--                         captured = o_t[:, 83:165]
--                         cap0_mask = captured[:, :40] > 0.5
--                         cap1_mask = captured[:, 40:80] > 0.5
--                         visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
--                         probs_flat = actor_model.belief_net.probs(logits, visible_mask)
--                     probs = probs_flat.view(3, 40).detach().cpu().numpy()
--                     det = {}
--                     others = [(_env.current_player + 1) % 4, (_env.current_player + 2) % 4, (_env.current_player + 3) % 4]
--                     for i, pid in enumerate(others):
--                         det[pid] = []
--                     vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
--                     unknown_ids = [cid for cid in range(40) if not vis[cid]]
--                     counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
--                     caps = [int(counts.get(pid, 0)) for pid in others]
--                     n = len(unknown_ids)
--                     if sum(caps) != n:
--                         caps[2] = max(0, n - caps[0] - caps[1])
--                     for cid in unknown_ids:
--                         pc = probs[:, cid]
--                         ps = pc / max(1e-9, pc.sum())
--                         j = int(torch.argmax(torch.tensor(ps)).item())
--                         if caps[j] > 0:
--                             det[others[j]].append(cid)
--                             caps[j] -= 1
--                     return det
+++                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
+++                    noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
+++                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
  +                from algorithms.is_mcts import run_is_mcts
  +                sims_scaled, root_temp_dyn = _eval_resolve_mcts_params(env, dict(mcts))
  +                mc = dict(mcts)
  +                mc['sims'] = int(sims_scaled)
  +                mc['root_temp'] = float(root_temp_dyn)
  +                action = run_is_mcts(
  +                    env,
  +                    policy_fn=policy_fn,
  +                    value_fn=lambda _o, _e: 0.0,
  +                    num_simulations=int(mc.get('sims', 128)),
  +                    c_puct=float(mc.get('c_puct', 1.0)),
  +                    belief=None,
  +                    num_determinization=int(mc.get('dets', 1)),
  +                    root_temperature=float(mc.get('root_temp', 0.0)),
  +                    prior_smooth_eps=float(mc.get('prior_smooth_eps', 0.0)),
  +                    robust_child=True,
  +                    root_dirichlet_alpha=float(mc.get('root_dirichlet_alpha', 0.25)),
  +                    root_dirichlet_eps=float(mc.get('root_dirichlet_eps', 0.25)),
  +                    belief_sampler=belief_sampler_neural
  +                )
  +                return action
  +            obs = env._get_observation(cp)
  +            o_cpu = obs if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32)
  +            if torch.is_tensor(o_cpu):
  +                o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
  +            leg_cpu = torch.stack([
  +                (x.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32))
  +            for x in legals], dim=0)
  +            s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
  +            o_t = o_cpu.unsqueeze(0).to(device=device)
  +            leg_t = leg_cpu.to(device=device)
  +            s_t = s_cpu.unsqueeze(0).to(device=device)
  +            with torch.no_grad():
  +                logits = actor_model(o_t, leg_t, s_t)
  +                idx = torch.argmax(logits).to('cpu')
  +            return leg_cpu[idx]
  +        return _select
  +
  +    agent_fn_team0 = make_agent_fn(actor_a)
  +    agent_fn_team1 = make_agent_fn(actor_b)
  +    diff, bd = play_match(agent_fn_team0, agent_fn_team1, games=games, k_history=k_history,
  +                        tqdm_desc=None, tqdm_position=0, tqdm_disable=True)
  +    # Converti breakdown medio in somma per aggregazione
  +    bd_sum = {0: {}, 1: {}}
  +    for t in [0, 1]:
  +        for k in bd[t].keys():
  +            bd_sum[t][k] = float(bd[t][k]) * float(games)
  +    wr = float((bd.get('meta') or {}).get('win_rate', 0.0))
  +    wins_int = int(round(wr * games))
  +    diff_sum = float(diff) * float(games)
  +    _dbg(f"worker[{wid}] done: wins={wins_int}/{int(games)} diff_sum={diff_sum:.2f}")
  +    return diff_sum, bd_sum, wins_int, int(games)
  +
  +
  +def evaluate_pair_actors_parallel(ckpt_a: str, ckpt_b: str, games: int = 10,
  +                                  k_history: int = 12,
  +                                  mcts: dict = None,
  +                                  belief_particles: int = 0, belief_ess_frac: float = 0.5,
  +                                  num_workers: int = 2,
  +                                  tqdm_desc: str = None, tqdm_position: int = 0, tqdm_disable: bool = True):
  +    """Esegue l'eval in parallelo su più processi e aggrega i risultati con progress condiviso."""
  +    num_workers = max(1, int(num_workers))
  +    if num_workers == 1 or games <= 1:
  +        return evaluate_pair_actors(ckpt_a, ckpt_b, games, k_history, mcts, belief_particles, belief_ess_frac, tqdm_desc, tqdm_position, tqdm_disable)
  +    # Suddividi i giochi in chunk
  +    base = games // num_workers
  +    rem = games % num_workers
  +    chunks = [base + (1 if i < rem else 0) for i in range(num_workers)]
  +    chunks = [c for c in chunks if c > 0]
  +    args_list = [(i, ckpt_a, ckpt_b, chunks[i], k_history, mcts, belief_particles, belief_ess_frac) for i in range(len(chunks))]
  +    _dbg(f"parallel setup: workers={num_workers} chunks={chunks} args={len(args_list)}")
  +    # Progress bar aggregata
  +    total_games = sum(chunks)
  +    pbar = None
  +    if not tqdm_disable:
  +        pbar = tqdm(total=total_games, desc=(tqdm_desc or 'Eval'), position=int(tqdm_position or 0), dynamic_ncols=True, leave=True)
  +    # Esegui i worker in modo che possiamo aggiornare il pbar al completamento
  +    ctx = _get_mp_ctx()
  +    results = []
  +    _dbg("creating pool and dispatching tasks …")
  +    with ctx.Pool(processes=len(args_list)) as pool:
  +        try:
  +            async_res = pool.imap_unordered(_eval_pair_chunk_worker, args_list)
  +            timeout_s = _get_eval_pool_timeout_seconds()
  +            for idx in range(len(args_list)):
  +                _dbg(f"waiting result {idx+1}/{len(args_list)} …")
  +                while True:
  +                    try:
  +                        item = async_res.next() if timeout_s is None else async_res.next(timeout=timeout_s)
  +                    except mp.TimeoutError as te:
  +                        # If timeout is disabled, keep waiting indefinitely
  +                        if timeout_s is None:
  +                            _dbg("no result yet; keep waiting …")
  +                            continue
  +                        raise
  +                    try:
  +                        diff_sum_i, bd_sum_i, wins_i, games_i = item
  +                    except Exception as e:
  +                        _dbg(f"received malformed result {item!r}: {e}")
  +                        break
  +                    if pbar is not None:
  +                        pbar.update(int(games_i))
  +                    results.append((diff_sum_i, bd_sum_i, wins_i, games_i))
  +                    break
  +        except mp.TimeoutError as te:
  +            _dbg("pool timeout while waiting for results; terminating pool …")
  +            pool.terminate()
  +            raise RuntimeError(f"Evaluation timeout: no result within SCOPONE_EVAL_POOL_TIMEOUT_S for {tqdm_desc or 'Eval'}") from te
  +        except KeyboardInterrupt:
  +            _dbg("KeyboardInterrupt: terminating pool …")
  +            pool.terminate()
  +            raise
  +        except Exception as e:
  +            _dbg(f"pool.imap_unordered raised: {type(e).__name__}: {e}")
  +            pool.terminate()
  +            raise
  +        else:
  +            _dbg("all chunks finished, closing pool …")
  +    if pbar is not None:
  +        pbar.close()
  +    _dbg(f"aggregating {len(results)} results")
  +    total_games = 0
  +    total_wins = 0
  +    total_diff_sum = 0.0
  +    agg = {0: {}, 1: {}}
  +    for diff_sum_i, bd_sum_i, wins_i, games_i in results:
  +        total_games += int(games_i)
  +        total_wins += int(wins_i)
  +        total_diff_sum += float(diff_sum_i)
  +        for t in [0, 1]:
  +            for k, v in bd_sum_i[t].items():
  +                agg[t][k] = agg[t].get(k, 0.0) + float(v)
  +    # Media finale breakdown
  +    if total_games <= 0:
  +        return 0.0, {0: {}, 1: {}}
  +    bd_avg = {0: {}, 1: {}}
  +    for t in [0, 1]:
  +        for k, v in agg[t].items():
  +            bd_avg[t][k] = (float(v) / float(total_games))
  +    wr = float(total_wins) / float(total_games)
  +    bd_avg['meta'] = {'win_rate': float(wr)}
  +    diff_avg = float(total_diff_sum) / float(total_games)
  +    return float(diff_avg), bd_avg
  +
  +
  +def _eval_pair_chunk_worker_dist(args):
  +    """Worker: come _eval_pair_chunk_worker ma con mcts per-worker (es. dets distribuiti)."""
  +    (ckpt_a, ckpt_b, games, k_history, mcts_per_worker, belief_particles, belief_ess_frac) = args
  +    from models.action_conditioned import ActionConditionedActor
  +    from utils.compile import maybe_compile_module
  +    from environment import ScoponeEnvMA
  +    import os as _os
  +    _os.environ['TQDM_DISABLE'] = '1'
  +    env0 = ScoponeEnvMA(k_history=k_history)
  +    obs_dim = env0.observation_space.shape[0]
  +    del env0
  +    actor_a = maybe_compile_module(ActionConditionedActor(obs_dim=obs_dim), name='ActionConditionedActor[eval_A]').to(device)
  +    actor_b = maybe_compile_module(ActionConditionedActor(obs_dim=obs_dim), name='ActionConditionedActor[eval_B]').to(device)
  +    if ckpt_a and os.path.isfile(ckpt_a):
  +        st_a = torch.load(ckpt_a, map_location=device)
  +        if isinstance(st_a, dict) and 'actor' in st_a:
  +            actor_a.load_state_dict(st_a['actor'])
  +    if ckpt_b and os.path.isfile(ckpt_b):
  +        st_b = torch.load(ckpt_b, map_location=device)
  +        if isinstance(st_b, dict) and 'actor' in st_b:
  +            actor_b.load_state_dict(st_b['actor'])
  +    actor_a.eval(); actor_b.eval()
  +
  +    def make_agent_fn(actor_model, mcts_local):
  +        def _select(env: ScoponeEnvMA):
  +            legals = env.get_valid_actions()
  +            cp = env.current_player
  +            seat_vec = torch.zeros(6, dtype=torch.float32, device='cpu')
  +            seat_vec[cp] = 1.0
  +            seat_vec[4] = 1.0 if cp in [0, 2] else 0.0
  +            seat_vec[5] = 1.0 if cp in [1, 3] else 0.0
  +            if mcts_local is not None and len(legals) > 1:
  +                def policy_fn(obs, legal_list):
  +                    o_cpu = obs if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32)
  +                    if torch.is_tensor(o_cpu):
  +                        o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
  +                    leg_cpu = torch.stack([
  +                        (x.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32))
  +                    for x in legal_list], dim=0)
  +                    s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
  +                    o_t = o_cpu.unsqueeze(0).to(device=device)
  +                    leg_t = leg_cpu.to(device=device)
  +                    s_t = s_cpu.unsqueeze(0).to(device=device)
  +                    with torch.no_grad():
  +                        logits = actor_model(o_t, leg_t, s_t)
  +                        probs = torch.softmax(logits, dim=0).detach().cpu().numpy()
  +                    return probs
  +                from algorithms.is_mcts import run_is_mcts
  +                sims_scaled, root_temp_dyn = _eval_resolve_mcts_params(env, dict(mcts_local))
  +                mc = dict(mcts_local)
  +                mc['sims'] = int(sims_scaled)
  +                mc['root_temp'] = float(root_temp_dyn)
  +                action = run_is_mcts(
  +                    env,
  +                    policy_fn=policy_fn,
  +                    value_fn=lambda _o, _e: 0.0,
  +                    num_simulations=int(mc.get('sims', 128)),
  +                    c_puct=float(mc.get('c_puct', 1.0)),
  +                    belief=None,
  +                    num_determinization=int(mc.get('dets', 1)),
  +                    root_temperature=float(mc.get('root_temp', 0.0)),
  +                    prior_smooth_eps=float(mc.get('prior_smooth_eps', 0.0)),
  +                    robust_child=True,
  +                    root_dirichlet_alpha=float(mc.get('root_dirichlet_alpha', 0.25)),
  +                    root_dirichlet_eps=float(mc.get('root_dirichlet_eps', 0.25)),
  +                )
  +                return action
  +            obs = env._get_observation(cp)
  +            o_cpu = obs if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32)
  +            if torch.is_tensor(o_cpu):
  +                o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
  +            leg_cpu = torch.stack([
  +                (x.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32))
  +            for x in legals], dim=0)
  +            s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
  +            o_t = o_cpu.unsqueeze(0).to(device=device)
  +            leg_t = leg_cpu.to(device=device)
  +            s_t = s_cpu.unsqueeze(0).to(device=device)
  +            with torch.no_grad():
  +                logits = actor_model(o_t, leg_t, s_t)
  +                idx = torch.argmax(logits).to('cpu')
  +            return leg_cpu[idx]
  +        return _select
  +
  +    agent_fn_team0 = make_agent_fn(actor_a, mcts_per_worker)
  +    agent_fn_team1 = make_agent_fn(actor_b, mcts_per_worker)
  +    diff, bd = play_match(agent_fn_team0, agent_fn_team1, games=games, k_history=k_history,
  +                        tqdm_desc=None, tqdm_position=0, tqdm_disable=True)
  +    bd_sum = {0: {}, 1: {}}
  +    for t in [0, 1]:
  +        for k in bd[t].keys():
  +            bd_sum[t][k] = float(bd[t][k]) * float(games)
  +    wr = float((bd.get('meta') or {}).get('win_rate', 0.0))
  +    wins_int = int(round(wr * games))
  +    diff_sum = float(diff) * float(games)
  +    return diff_sum, bd_sum, wins_int, int(games)
  +
  +
  +def evaluate_pair_actors_parallel_dist(ckpt_a: str, ckpt_b: str, games: int = 10,
  +                                       k_history: int = 12,
  +                                       mcts_list: list = None,
  +                                       belief_particles: int = 0, belief_ess_frac: float = 0.5,
  +                                       num_workers: int = 2,
  +                                       tqdm_desc: str = None, tqdm_position: int = 0, tqdm_disable: bool = True):
  +    """Parallelo con MCTS per-worker (es. dets distribuiti) con progress condiviso."""
  +    num_workers = max(1, int(num_workers))
  +    if not mcts_list or len(mcts_list) == 0:
  +        return evaluate_pair_actors_parallel(ckpt_a, ckpt_b, games, k_history, None, belief_particles, belief_ess_frac, num_workers, tqdm_desc, tqdm_position, tqdm_disable)
  +    # Chunk giochi per worker
  +    base = games // num_workers
  +    rem = games % num_workers
  +    chunks = [base + (1 if i < rem else 0) for i in range(num_workers)]
  +    chunks = [c for c in chunks if c > 0]
  +    # Allinea mcts_list ai worker (se meno, ricicla gli ultimi; se più, tronca)
  +    mcts_eff = (mcts_list + mcts_list[-1:]*num_workers)[:num_workers]
  +    max_chunk = int(os.environ.get('SCOPONE_EVAL_MAX_GAMES_PER_CHUNK', '32'))
  +    if max_chunk <= 0:
  +        max_chunk = None
  +    args_list: list = []
  +    for i, games_i in enumerate(chunks):
  +        if games_i <= 0:
  +            continue
  +        cfg = mcts_eff[min(i, len(mcts_eff) - 1)] if len(mcts_eff) > 0 else None
  +        remaining = games_i
  +        while remaining > 0:
  +            take = remaining if max_chunk is None else min(remaining, max_chunk)
  +            args_list.append((ckpt_a, ckpt_b, int(take), k_history, cfg, belief_particles, belief_ess_frac))
  +            remaining -= take
  +    if len(args_list) == 0:
  +        return 0.0, {0: {}, 1: {}}
  +    _dbg(f"parallel-dist setup: workers={num_workers} chunks={chunks} tasks={len(args_list)} dets={[m.get('dets',1) for m in mcts_eff]} max_chunk={max_chunk if max_chunk is not None else 'inf'}")
  +    # Progress bar aggregata
  +    total_games = sum(chunks)
  +    pbar = None
  +    if not tqdm_disable:
  +        pbar = tqdm(total=total_games, desc=(tqdm_desc or 'Eval'), position=int(tqdm_position or 0), dynamic_ncols=True, leave=True)
  +    ctx = _get_mp_ctx()
  +    results = []
  +    pool_size = max(1, min(num_workers, len(args_list)))
  +    with ctx.Pool(processes=pool_size) as pool:
  +        try:
  +            async_res = pool.imap_unordered(_eval_pair_chunk_worker_dist, args_list)
  +            timeout_s = _get_eval_pool_timeout_seconds()
  +            for idx in range(len(args_list)):
  +                _dbg(f"waiting dist result {idx+1}/{len(args_list)} …")
  +                while True:
  +                    try:
  +                        item = async_res.next() if timeout_s is None else async_res.next(timeout=timeout_s)
  +                    except mp.TimeoutError as te:
  +                        if timeout_s is None:
  +                            _dbg("no dist result yet; keep waiting …")
  +                            continue
  +                        raise
  +                    try:
  +                        diff_sum_i, bd_sum_i, wins_i, games_i = item
  +                    except Exception as e:
  +                        _dbg(f"received malformed result {item!r}: {e}")
  +                        break
  +                    if pbar is not None:
  +                        pbar.update(int(games_i))
  +                    results.append((diff_sum_i, bd_sum_i, wins_i, games_i))
  +                    break
  +        except mp.TimeoutError as te:
  +            _dbg("pool timeout (dist) while waiting for results; terminating pool …")
  +            pool.terminate()
  +            raise RuntimeError(f"Evaluation timeout (dist): no result within SCOPONE_EVAL_POOL_TIMEOUT_S for {tqdm_desc or 'Eval'}") from te
  +        except KeyboardInterrupt:
  +            _dbg("KeyboardInterrupt: terminating pool … (dist)")
  +            pool.terminate()
  +            raise
  +        except Exception as e:
  +            _dbg(f"pool.imap_unordered(dist) raised: {type(e).__name__}: {e}")
  +            pool.terminate()
  +            raise
  +    if pbar is not None:
  +        pbar.close()
  +    _dbg(f"aggregating {len(results)} dist results")
  +    total_games = 0
  +    total_wins = 0
  +    total_diff_sum = 0.0
  +    agg = {0: {}, 1: {}}
  +    for diff_sum_i, bd_sum_i, wins_i, games_i in results:
  +        total_games += int(games_i)
  +        total_wins += int(wins_i)
  +        total_diff_sum += float(diff_sum_i)
  +        for t in [0, 1]:
  +            for k, v in bd_sum_i[t].items():
  +                agg[t][k] = agg[t].get(k, 0.0) + float(v)
  +    if total_games <= 0:
  +        return 0.0, {0: {}, 1: {}}
  +    bd_avg = {0: {}, 1: {}}
  +    for t in [0, 1]:
  +        for k, v in agg[t].items():
  +            bd_avg[t][k] = (float(v) / float(total_games))
  +    wr = float(total_wins) / float(total_games)
  +    bd_avg['meta'] = {'win_rate': float(wr)}
  +    diff_avg = float(total_diff_sum) / float(total_games)
  +    return float(diff_avg), bd_avg
  +
  +def league_eval_and_update(league_dir='checkpoints/league', games=20, target_points=11):
  +    """Esegue sfide tra ultimi due checkpoint registrati e aggiorna Elo nel league in base alla differenza media di punti (reward)."""
  +    league = League(base_dir=league_dir)
  +    if len(league.history) < 2:
  +        return
  +    a, b = league.history[-2], league.history[-1]
  +    # Usa serie di partite per stimare la differenza media di punti di A contro B
  +    diff, bd = evaluate_pair_actors(a, b, games=games, k_history=12, mcts=None)
  +    # Aggiorna Elo usando la differenza media di punti -> mapping lineare a score
  +    league.update_elo_from_diff(a, b, diff)
  +    return league.elo
-- 
diff --cc main.py
index 4e4779e,4e4779e,0000000..15fa6ba
mode 100644,100644,000000..100644
--- a/main.py
+++ b/main.py
@@@@ -1,294 -1,294 -1,0 +1,297 @@@@
  +import os
  +import threading
  +import torch as _torch
  +_train_dev = os.environ.get('SCOPONE_TRAIN_DEVICE', 'cpu')
  +if (_train_dev.startswith('cuda') and _torch.cuda.is_available()):
  +    _n_threads = int(os.environ.get('SCOPONE_TRAIN_THREADS', '2'))
  +    _n_interop = int(os.environ.get('SCOPONE_TRAIN_INTEROP_THREADS', '1'))
  +else:
  +    _cores = int(max(1, (os.cpu_count() or 1)))
  +    _target = max(1, int(_cores * 0.60))
  +    _n_threads = int(os.environ.get('SCOPONE_TRAIN_THREADS', str(_target)))
  +    _n_interop_default = max(1, _n_threads // 8)
  +    _n_interop = int(os.environ.get('SCOPONE_TRAIN_INTEROP_THREADS', str(_n_interop_default)))
  +
  +# Silence TensorFlow/absl noise before any heavy imports
  +os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '3')  # hide INFO/WARNING/ERROR from TF C++ logs
  +os.environ.setdefault('ABSL_LOGGING_MIN_LOG_LEVEL', '3')  # absl logs: only FATAL
  +os.environ.setdefault('TF_ENABLE_ONEDNN_OPTS', '0')  # disable oneDNN custom ops info spam
  +
  +# Abilita TensorBoard di default (override con SCOPONE_DISABLE_TB=1 per disattivarlo)
  +os.environ.setdefault('SCOPONE_DISABLE_TB', '0')
  +
  +## Abilita torch.compile di default per l'intero progetto (override via env)
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE', '0')
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE_MODE', 'reduce-overhead')
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE_BACKEND', 'inductor')
  +os.environ.setdefault('SCOPONE_COMPILE_VERBOSE', '1')
  +
  +
  +## Autotune controllabile: di default ON su CPU beneficia di fusioni; può essere disattivato via env
  +os.environ.setdefault('SCOPONE_INDUCTOR_AUTOTUNE', '1')
  +os.environ.setdefault('TORCHINDUCTOR_MAX_AUTOTUNE_GEMM', '0')
  +
  +## Evita graph break su .item() catturando scalari nei grafi
  +os.environ.setdefault('TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS', '1')
  +
  +## Abilita dynamic shapes per ridurre errori di symbolic shapes FX
  +os.environ.setdefault('TORCHDYNAMO_DYNAMIC_SHAPES', '0')
  +
  +## Alza il limite del cache di Dynamo per ridurre recompilazioni
  +os.environ.setdefault('TORCHDYNAMO_CACHE_SIZE_LIMIT', '32')
  +
  +## Non impostare TORCH_LOGS ad un valore invalido; lascia al default o definisci mapping esplicito se necessario
  +# Abilita e blocca i flag dell'osservazione all'avvio (usati da observation/environment al load)
  +# Se l'utente li ha già impostati nel proprio run, li rispettiamo (setdefault)
  +os.environ.setdefault('OBS_INCLUDE_DEALER', '1')
  +os.environ.setdefault('OBS_INCLUDE_INFERRED', '0')
+++os.environ.setdefault('OBS_INCLUDE_INFERRED_L2', '0')
+++os.environ.setdefault('OBS_INCLUDE_INFERRED_L3', '0')
  +os.environ.setdefault('OBS_INCLUDE_RANK_PROBS', '0')
  +os.environ.setdefault('OBS_INCLUDE_SCOPA_PROBS', '0')
  +
  +# Imposta ENV_DEVICE una sola volta coerente con SCOPONE_DEVICE o disponibilità CUDA
  +os.environ.setdefault('SCOPONE_DEVICE', 'cpu')
  +os.environ.setdefault('ENV_DEVICE', 'cpu')
  +
  +# Training compute device (models stay on CPU during env collection; moved only inside update)
  +os.environ.setdefault('SCOPONE_TRAIN_DEVICE', 'cpu')
  +
  +# Enable approximate GELU and gate all runtime checks via a single flag
  +os.environ.setdefault('SCOPONE_APPROX_GELU', '1')
  +os.environ.setdefault('SCOPONE_STRICT_CHECKS', '0')
  +os.environ.setdefault('SCOPONE_PROFILE', '0')
  +
  +# Additional trainer/eval tunables exposed via environment (defaults; override as needed)
  +os.environ.setdefault('SCOPONE_PAR_DEBUG', '0')  # abilita log di debug per raccolta parallela/eval
  +os.environ.setdefault('SCOPONE_PPO_DEBUG', '0')  # abilita log di debug per PPO
  +os.environ.setdefault('SCOPONE_WORKER_THREADS', '1')  # thread CPU per processo worker (eval)
  +os.environ.setdefault('SCOPONE_TORCH_PROF', '0')  # abilita PyTorch profiler (main/workers)
  +os.environ.setdefault('SCOPONE_TORCH_TB_DIR', '')  # directory TensorBoard per tracce profiler
  +os.environ.setdefault('SCOPONE_RPC_TIMEOUT_S', '30')  # timeout RPC dei collector paralleli (s)
  +os.environ.setdefault('SCOPONE_RAISE_ON_INVALID_SIMS', '0')  # solleva eccezione se sims MCTS scalate sono invalide
  +os.environ.setdefault('SCOPONE_EP_PUT_TIMEOUT_S', '15')  # timeout inserimento episodio nelle code (s)
  +os.environ.setdefault('SCOPONE_TORCH_PROF_DIR', 'profiles')  # cartella output per tracce profiler JSON
  +os.environ.setdefault('SCOPONE_RAISE_ON_CKPT_FAIL', '0')  # solleva se fallisce il load del checkpoint
  +os.environ.setdefault('ENABLE_BELIEF_SUMMARY', '0')  # stampa riassunti belief (diagnostica)
+++os.environ.setdefault('SCOPONE_BELIEF_BLEND_ALPHA', '0.65')
  +os.environ.setdefault('DET_NOISE', '0.0')  # rumore per determinizzazioni MCTS (IS-MCTS)
  +os.environ.setdefault('SCOPONE_COLLECT_MIN_BATCH', '0')  # minima dimensione batch prima di flush del collector
  +os.environ.setdefault('SCOPONE_COLLECT_MAX_LATENCY_MS', '3.0')  # latenza massima (ms) prima del flush del collector
  +os.environ.setdefault('SCOPONE_COLLECTOR_STALL_S', '30')  # watchdog: tempo di stallo consentito (s)
  +
  +# Gameplay/training topology flags
  +os.environ.setdefault('SCOPONE_START_OPP', os.environ.get('SCOPONE_START_OPP', 'top1'))
  +
  +# Evaluation process knobs
  +os.environ.setdefault('SCOPONE_EVAL_DEBUG', '0')  # abilita log di debug in valutazione
  +os.environ.setdefault('SCOPONE_EVAL_MP_START', os.environ.get('SCOPONE_MP_START', 'forkserver'))  # metodo start multiprocessing per eval
  +os.environ.setdefault('SCOPONE_EVAL_POOL_TIMEOUT_S', '0')  # timeout attesa risultati pool eval (0=illimitato)
  +os.environ.setdefault('SCOPONE_ELO_DIFF_SCALE', '6.0')  # fattore per mappare diff punti -> Elo
  +
  +# TQDM_DISABLE: 1=disattiva tutte le barre/logging di tqdm; 0=abilitato
  +os.environ.setdefault('TQDM_DISABLE', '0')  # disabilita barre di progresso TQDM globali
  +## SCOPONE_PER_ENV_TQDM: 1=mostra barre per-env; 0=nascondi barre per-env (lascia barra globale)
  +os.environ.setdefault('SCOPONE_PER_ENV_TQDM', os.environ.get('SCOPONE_PER_ENV_TQDM', '0'))  # barre per-env (se 1)
  +
  +# SELFPLAY: 1=single net (self-play), 0=dual nets (Team A/B)
  +_selfplay_env = str(os.environ.get('SCOPONE_SELFPLAY', '1')).strip().lower()
  +_selfplay = (_selfplay_env in ['1', 'true', 'yes', 'on'])
  +
  +# SCOPONE_OPP_FROZEN: 1=freeze the opponent, 0=co-train with the opponent
  +_opp_frozen = os.environ.get('SCOPONE_OPP_FROZEN','0') in ['1','true','yes','on']
  +
  +# SCOPONE_TRAIN_FROM_BOTH_TEAMS: effective ONLY when SELFPLAY=1 and OPP_FROZEN=0.
  +# Uses transitions from both teams for the single net; otherwise ignored (on-policy).
  +_tfb = os.environ.get('SCOPONE_TRAIN_FROM_BOTH_TEAMS','0') in ['1','true','yes','on']
  +
  +# Warm-start policy controlled by SCOPONE_WARM_START: '0' start-from-scratch, '1' force top1 clone, '2' use top2 if available
  +os.environ.setdefault('SCOPONE_WARM_START', '2')
  +
  +# SCOPONE_ALTERNATE_ITERS: in dual-nets+frozen, train A for N iters then swap to B (and vice versa)
  +os.environ.setdefault('SCOPONE_ALTERNATE_ITERS', '1')
  +
  +# SCOPONE_FROZEN_UPDATE_EVERY: in selfplay+frozen, refresh the shadow (frozen) opponent every N iters
  +os.environ.setdefault('SCOPONE_FROZEN_UPDATE_EVERY', '1')
  +
  +# Refresh League from disk at startup (scan checkpoints/). 1=ON, 0=OFF
  +os.environ.setdefault('SCOPONE_LEAGUE_REFRESH', '0')
  +
  +# Parallel eval workers: 1=serial, >1 parallel via multiprocessing
  +os.environ.setdefault('SCOPONE_EVAL_WORKERS', str(max(1, (os.cpu_count() or 1))))  # numero processi worker per eval
  +
  +# Training flags (manual overrides available via env)
  +_save_every = int(os.environ.get('SCOPONE_SAVE_EVERY','10'))  # salva checkpoint ogni N iterazioni
  +os.environ.setdefault('SCOPONE_MINIBATCH', '4096')  # dimensione minibatch PPO per update
  +
  +# Checkpoint path control
  +os.environ.setdefault('SCOPONE_CKPT', 'checkpoints/ppo_ac.pth')  # percorso checkpoint predefinito
  +
  +# Default to random seed for training runs (set -1); stable only if user sets it
  +seed_env = int(os.environ.get('SCOPONE_SEED', '-1'))  # seed globale (-1=random)
  +
  +# Allow configuring iterations/horizon/num_envs via env; sensible defaults
  +iters = int(os.environ.get('SCOPONE_ITERS', '3'))  # numero iterazioni di training
  +horizon = int(os.environ.get('SCOPONE_HORIZON', '16384'))  # horizon di raccolta per iterazione
  +num_envs = int(os.environ.get('SCOPONE_NUM_ENVS', '32'))  # numero di environment paralleli
  +
  +# Read checkpoint path from env for training
  +ckpt_path_env = os.environ.get('SCOPONE_CKPT', 'checkpoints/ppo_ac.pth')
  +
  +_mcts_warmup_iters = int(os.environ.get('SCOPONE_MCTS_WARMUP_ITERS', '0'))  # iterazioni con MCTS disattivato in train
  +
  +# MCTS eval flags
  +_eval_every = int(os.environ.get('SCOPONE_EVAL_EVERY', '10'))  # esegui eval ogni N iterazioni
  +_eval_c_puct = float(os.environ.get('SCOPONE_EVAL_MCTS_C_PUCT','1.0'))  # c_puct per MCTS in eval
  +_eval_root_temp = float(os.environ.get('SCOPONE_EVAL_MCTS_ROOT_TEMP','0.0'))  # temperatura root per MCTS in eval
  +_eval_prior_eps = float(os.environ.get('SCOPONE_EVAL_MCTS_PRIOR_SMOOTH_EPS','0.0'))  # smoothing epsilon dei prior
  +_eval_dir_alpha = float(os.environ.get('SCOPONE_EVAL_MCTS_DIRICHLET_ALPHA','0.25'))  # alpha Dirichlet al root
  +_eval_dir_eps = float(os.environ.get('SCOPONE_EVAL_MCTS_DIRICHLET_EPS','0.25'))  # mixing epsilon del rumore Dirichlet
  +_eval_belief_particles = int(os.environ.get('SCOPONE_EVAL_BELIEF_PARTICLES','0'))  # particelle belief per prior MCTS (eval)
  +_eval_belief_ess = float(os.environ.get('SCOPONE_EVAL_BELIEF_ESS_FRAC','0.5'))  # soglia ESS per resampling belief (eval)
  +_eval_use_mcts = os.environ.get('SCOPONE_EVAL_USE_MCTS','1').lower() in ['1','true','yes','on']  # abilita MCTS in eval
  +_eval_mcts_sims = int(os.environ.get('SCOPONE_EVAL_MCTS_SIMS','4'))  # simulazioni base per mossa (pre-scaling)
  +_eval_mcts_dets = int(os.environ.get('SCOPONE_EVAL_MCTS_DETS','2'))  # determinizzazioni per mossa (IS-MCTS)
  +_eval_kh = int(os.environ.get('SCOPONE_EVAL_K_HISTORY','39'))  # ampiezza cronologia osservazioni (k_history)
  +_eval_games = int(os.environ.get('SCOPONE_EVAL_GAMES','1000'))  # numero partite per valutazione
  +os.environ.setdefault('SCOPONE_EVAL_MAX_GAMES_PER_CHUNK', '4')  # partite per task/worker (granularità progress)
  +_eval_max_games_per_chunk = int(os.environ.get('SCOPONE_EVAL_MAX_GAMES_PER_CHUNK', '4'))  # letta per logging
  +## Eval MCTS scaling controls
  +os.environ.setdefault('SCOPONE_EVAL_MCTS_SCALING', '1')  # abilita scaling per-mano di sims/root_temp in eval
  +os.environ.setdefault('SCOPONE_EVAL_MCTS_PROGRESS_START', '0.25')  # inizio finestra di progress (alpha=0)
  +os.environ.setdefault('SCOPONE_EVAL_MCTS_PROGRESS_FULL', '0.75')  # fine finestra di progress (alpha=1)
  +os.environ.setdefault('SCOPONE_EVAL_MCTS_MIN_SIMS', '0')  # soglia minima simulazioni in eval
  +os.environ.setdefault('SCOPONE_EVAL_MCTS_TRAIN_FACTOR', '1.0')  # moltiplicatore globale simulazioni in eval
  +
  +# Training config flags (from env)
  +_entropy_sched = os.environ.get('SCOPONE_ENTROPY_SCHED','linear')  # schedulazione entropia (es. linear, cos)
  +_belief_particles = int(os.environ.get('SCOPONE_BELIEF_PARTICLES','512'))  # particelle belief per training
  +_belief_ess = float(os.environ.get('SCOPONE_BELIEF_ESS_FRAC','0.5'))  # soglia ESS per resampling belief (train)
  +_mcts_c_puct = float(os.environ.get('SCOPONE_MCTS_C_PUCT','1.0'))  # c_puct per MCTS in training
  +_mcts_root_temp = float(os.environ.get('SCOPONE_MCTS_ROOT_TEMP','0.0'))  # temperatura root per MCTS in training
  +_mcts_prior_eps = float(os.environ.get('SCOPONE_MCTS_PRIOR_SMOOTH_EPS','0.0'))  # smoothing epsilon dei prior (train)
  +_mcts_dir_alpha = float(os.environ.get('SCOPONE_MCTS_DIRICHLET_ALPHA','0.25'))  # alpha Dirichlet root (train)
  +_mcts_dir_eps = float(os.environ.get('SCOPONE_MCTS_DIRICHLET_EPS','0.25'))  # mixing epsilon rumore Dirichlet (train)
  +_mcts_train = os.environ.get('SCOPONE_MCTS_TRAIN','1') in ['1','true','yes','on']  # abilita MCTS nella raccolta
  +_mcts_sims = int(os.environ.get('SCOPONE_MCTS_SIMS','4'))  # simulazioni base per mossa in training
  +_mcts_dets = int(os.environ.get('SCOPONE_MCTS_DETS','2'))  # determinizzazioni per mossa (train)
  +## Training MCTS scaling controls (used by trainer if supported)
  +os.environ.setdefault('SCOPONE_MCTS_PROGRESS_START', '0.25')  # inizio finestra progress per scaling (train)
  +os.environ.setdefault('SCOPONE_MCTS_PROGRESS_FULL', '0.75')  # fine finestra progress per scaling (train)
  +os.environ.setdefault('SCOPONE_MCTS_MIN_SIMS', '0')  # soglia minima simulazioni in training
  +os.environ.setdefault('SCOPONE_MCTS_TRAIN_FACTOR', '1.0')  # moltiplicatore globale simulazioni (train)
  +os.environ.setdefault('SCOPONE_MCTS_SCALING', '1')  # abilita scaling per-mano di sims/root_temp in training
  +os.environ.setdefault('SCOPONE_MCTS_BOTH_SIDES', '1')  # applica MCTS su entrambi i lati (non solo main) durante training
  +
  +
  +# Targeted FD-level stderr filter to drop absl/TF CUDA registration warnings from C++
  +_SILENCE_ABSL = os.environ.get('SCOPONE_SILENCE_ABSL', '1') == '1'
  +if _SILENCE_ABSL:
  +    _SUPPRESS_SUBSTRINGS = (
  +        "All log messages before absl::InitializeLog() is called are written to STDERR",
  +        "Unable to register cuDNN factory",
  +        "Unable to register cuBLAS factory",
  +        "cuda_dnn.cc",
  +        "cuda_blas.cc",
  +    )
  +
  +    _orig_fd2 = os.dup(2)
  +    _r_fd, _w_fd = os.pipe()
  +    os.dup2(_w_fd, 2)
  +
  +    def _stderr_reader(r_fd, orig_fd, suppressed):
  +        with os.fdopen(r_fd, 'rb', buffering=0) as r:
  +            buffer = b""
  +            while True:
  +                chunk = r.read(1024)
  +                if not chunk:
  +                    break
  +                buffer += chunk
  +                while b"\n" in buffer:
  +                    line, buffer = buffer.split(b"\n", 1)
  +                    txt = line.decode('utf-8', errors='ignore')
  +                    if not any(s in txt for s in suppressed):
  +                        os.write(orig_fd, line + b"\n")
  +            if buffer:
  +                txt = buffer.decode('utf-8', errors='ignore')
  +                if not any(s in txt for s in suppressed):
  +                    os.write(orig_fd, buffer)
  +
  +    _t = threading.Thread(target=_stderr_reader, args=(_r_fd, _orig_fd2, _SUPPRESS_SUBSTRINGS), daemon=True)
  +    _t.start()
  +
  +os.environ.setdefault('ENV_DEVICE', os.environ.get('SCOPONE_DEVICE', 'cpu'))
  +## Imposta metodo mp sicuro per CUDA: forkserver (override con SCOPONE_MP_START)
  +os.environ.setdefault('SCOPONE_MP_START', 'forkserver')
  +
  +import torch
  +from utils.device import get_compute_device
  +from tqdm import tqdm
  +from trainers.train_ppo import train_ppo
  +from utils.seed import resolve_seed
  +
  +def _maybe_launch_tensorboard():
  +    """Launch TensorBoard in background if enabled.
  +    Controlled by env:
  +      - SCOPONE_AUTO_TB (default '1'): enable/disable auto launch
  +      - TB_LOGDIR (default 'runs'): log directory
  +      - TB_HOST (default '0.0.0.0'): bind host
  +      - TB_PORT (default '6006'): port
  +    """
  +    if os.environ.get('SCOPONE_DISABLE_TB', '0') == '1':
  +        return
  +    if os.environ.get('SCOPONE_AUTO_TB', '1') != '1':
  +        return
  +    logdir = os.environ.get('TB_LOGDIR', os.path.abspath('runs'))
  +    host = os.environ.get('TB_HOST', '0.0.0.0')
  +    port = os.environ.get('TB_PORT', '6006')
  +    import subprocess
  +    # Start TensorBoard as a detached background process
  +    cmd = ['tensorboard', '--logdir', logdir, '--host', host, '--port', str(port)]
  +    subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
  +    tqdm.write(f"TensorBoard auto-started at http://localhost:{port}/ (logdir={logdir})")
  +
  +# Minimal entrypoint: launch PPO training only
  +if __name__ == "__main__":
  +    device = get_compute_device()
  +    tqdm.write(f"Using device: {device}")
  +    tqdm.write(f"Training compute device: {os.environ.get('SCOPONE_TRAIN_DEVICE', 'cpu')}")
  +    tqdm.write(f"Self-play: {'ON' if _selfplay else 'OFF (League)'}")
  +    # Configure CPU threads for training in the main process only (workers handled in trainers/train_ppo.py)
  +    # On GPU training, keep minimal CPU threads to reduce host contention
  +    torch.set_num_threads(_n_threads)
  +    torch.set_num_interop_threads(_n_interop)
  +    tqdm.write(f"Training threads: num_threads={_n_threads} interop={_n_interop}")
  +    _maybe_launch_tensorboard()
  +
  +    tqdm.write(f"Parallel envs: {num_envs}  (SCOPONE_PROFILE={os.environ.get('SCOPONE_PROFILE','0')})")
  +    tqdm.write(f"Train from both team transitions: {'ON' if _tfb else 'OFF'}")
  +    tqdm.write(f"Opponent frozen: {'ON' if _opp_frozen else 'OFF'}")
  +    tqdm.write(f"Warm start mode: {os.environ.get('SCOPONE_WARM_START','2')}")
  +    tqdm.write(f"League startup refresh: {'ON' if os.environ.get('SCOPONE_LEAGUE_REFRESH','1') in ['1','true','yes','on'] else 'OFF'}")
  +    tqdm.write(f"Eval cfg: games={_eval_games} mcts={'ON' if _eval_use_mcts else 'OFF'} sims={_eval_mcts_sims} dets={_eval_mcts_dets} kh={_eval_kh}")
  +
  +
  +    train_ppo(num_iterations=iters, horizon=horizon, k_history=_eval_kh,
  +              num_envs=num_envs,
  +              mcts_train=_mcts_train,
  +              mcts_sims=_mcts_sims,
  +              mcts_sims_eval=_eval_mcts_sims,
  +              save_every=_save_every,
  +              ckpt_path=ckpt_path_env,
  +              entropy_schedule_type=_entropy_sched,
  +              eval_every=_eval_every,
  +              mcts_in_eval=_eval_use_mcts,
  +              mcts_dets=_mcts_dets,
  +              mcts_c_puct=_mcts_c_puct,
  +              mcts_root_temp=_mcts_root_temp,
  +              mcts_prior_smooth_eps=_mcts_prior_eps,
  +              mcts_dirichlet_alpha=_mcts_dir_alpha,
  +              mcts_dirichlet_eps=_mcts_dir_eps,
  +              belief_particles=_belief_particles,
  +              belief_ess_frac=_belief_ess,
  +              eval_games=_eval_games,
  +              seed=seed_env,
  +              use_selfplay=_selfplay,
  +              train_both_teams=_tfb,
  +              mcts_warmup_iters=_mcts_warmup_iters)
diff --cc models/action_conditioned.py
index 473dd73,473dd73,0000000..db32273
mode 100644,100644,000000..100644
--- a/models/action_conditioned.py
+++ b/models/action_conditioned.py
@@@@ -1,1107 -1,1107 -1,0 +1,1120 @@@@
  +import torch
  +import torch.nn as nn
  +import math
  +import time
  +import os
  +from contextlib import nullcontext
  +from typing import Dict, Tuple, Optional
  +from utils.device import get_compute_device, get_amp_dtype
  +from utils.fallback import notify_fallback
  +try:
  +    # Prefer new SDPA backend selector if available (PyTorch >= 2.3)
  +    from torch.nn.attention import sdpa_kernel as _sdpa_kernel_ctx, SDPBackend as _SDPBackend  # type: ignore
  +except Exception:
  +    _sdpa_kernel_ctx = None  # type: ignore
  +    _SDPBackend = None  # type: ignore
  +
  +device = get_compute_device()
  +autocast_device = device.type
  +autocast_dtype = get_amp_dtype()
  +STRICT = (os.environ.get('SCOPONE_STRICT_CHECKS', '0') == '1')
  +
  +# Observation flags are now imported from observation.py to ensure consistency.
  +from observation import (
  +    OBS_INCLUDE_INFERRED as _OBS_INCLUDE_INFERRED,
+++    OBS_INCLUDE_INFERRED_L2 as _OBS_INCLUDE_INFERRED_L2,
+++    OBS_INCLUDE_INFERRED_L3 as _OBS_INCLUDE_INFERRED_L3,
  +    OBS_INCLUDE_RANK_PROBS as _OBS_INCLUDE_RANK_PROBS,
  +    OBS_INCLUDE_SCOPA_PROBS as _OBS_INCLUDE_SCOPA_PROBS,
  +    OBS_INCLUDE_DEALER as _OBS_INCLUDE_DEALER,
+++    get_compact_obs_dim as _get_compact_obs_dim,
  +)
  +
  +import torch._dynamo as _dynamo  # type: ignore
  +_dynamo_disable = _dynamo.disable  # type: ignore[attr-defined]
  +
  +
  +
+++def _default_k_history() -> int:
+++    return int(os.environ.get('SCOPONE_K_HISTORY', os.environ.get('SCOPONE_EVAL_K_HISTORY', '39')))
+++
+++
+++
  +class StateEncoderCompact(nn.Module):
  +    """
  +    Encoder per osservazione compatta con storia-k (blocchi da 61) e stats a
  +    dimensione variabile (in base ai flag di osservazione). Le sezioni sono:
  +      - hand_table: 83
  +      - captured: 82
  +      - history: 61*k (k variabile)
  +      - stats: variabile (resto delle feature)
  +      - seat/team: 6 (passato separatamente)
  +    """
  +    def __init__(self, k_history: Optional[int] = None):
  +        super().__init__()
  +        self.k_history_hint: Optional[int] = k_history
  +        # Card embedding for permutation-invariant set encoding (40 card IDs)
  +        # Initialize deterministically and ensure finite values
  +        torch.manual_seed(0)
  +        ce = torch.randn(40, 32, device=device, dtype=torch.float32) * 0.02
  +        ce = torch.nan_to_num(ce, nan=0.0, posinf=0.0, neginf=0.0)
  +        self.card_emb = nn.Parameter(ce)
  +        # Small processors for counts and concatenations
  +        self.counts_head_hand = nn.Sequential(nn.Linear(3, 16), nn.ReLU())    # other hands sizes
  +        self.counts_head_cap = nn.Sequential(nn.Linear(2, 16), nn.ReLU())     # captured counts
  +        # Cross-attention mano↔tavolo per relazioni pari-rank e subset-sum
  +        self.cross_attn_h2t = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)
  +        self.cross_attn_t2h = nn.MultiheadAttention(embed_dim=32, num_heads=4, batch_first=True)
  +        # set_merge: [hand(32) | table(32) | cap0(32) | cap1(32) | hand_attn(32) | table_attn(32) | counts(16+16)]
  +        self.set_merge_head = nn.Sequential(nn.Linear(32 * 6 + 16 + 16, 64), nn.ReLU())
  +
  +        # History Transformer (sequence encoder)
  +        self.hist_proj = nn.Linear(61, 64)
  +        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=128, batch_first=True)
  +        self.hist_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)
  +        self.hist_pos_emb = nn.Embedding(40, 64)  # up to 40 recent moves
  +
  +        # Stats and seat/team: fix input dim deterministically from flags to avoid LazyLinear pitfalls
  +        self.stats_in_dim = 99 \
  +            + (120 if _OBS_INCLUDE_INFERRED else 0) \
+++            + (120 if _OBS_INCLUDE_INFERRED_L2 else 0) \
+++            + (120 if _OBS_INCLUDE_INFERRED_L3 else 0) \
  +            + (10 if _OBS_INCLUDE_SCOPA_PROBS else 0) \
  +            + (150 if _OBS_INCLUDE_RANK_PROBS else 0) \
  +            + (4 if _OBS_INCLUDE_DEALER else 0)
  +        self.stats_processor = nn.Sequential(nn.Linear(self.stats_in_dim, 64), nn.ReLU())
  +        self.seat_head = nn.Sequential(nn.Linear(6, 32), nn.ReLU())
  +
  +        # Combiner to 256-d state context
  +        # Inputs: set_merge(64) + hist(64) + stats(64) + seat(32) = 224 → 256
  +        # Se OBS_INCLUDE_DEALER=1 aggiungiamo +4 nelle stats. Usiamo Linear(224,256) e
  +        # deleghiamo a self.stats_processor (LazyLinear) l'adattamento alla nuova dimensione.
  +        self.combiner = nn.Sequential(nn.Linear(224, 256), nn.ReLU())
  +        self.to(device)
  +        # Final guard after move to device
  +        with torch.no_grad():
  +            if (os.environ.get('SCOPONE_STRICT_CHECKS', '0') == '1') and (not torch.isfinite(self.card_emb).all()):
  +                self.card_emb.copy_(torch.zeros_like(self.card_emb))
  +        # Pre-create positional ids for up to 40 steps to avoid per-forward arange
  +        self.register_buffer('_hist_pos_ids', torch.arange(40, dtype=torch.long, device=device))
  +
  +    def _attn_ctx(self):
  +        if device.type == 'cuda':
  +            if _sdpa_kernel_ctx is not None and _SDPBackend is not None:
  +                return _sdpa_kernel_ctx([_SDPBackend.FLASH_ATTENTION, _SDPBackend.EFFICIENT_ATTENTION])
  +            return torch.backends.cuda.sdp_kernel(enable_flash=True, enable_mem_efficient=True, enable_math=False)
  +        return nullcontext()
  +
  +    def _safe_mha(self, mha: nn.Module, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, kpm: torch.Tensor) -> torch.Tensor:
  +        """Run MHA with key padding mask safely; returns zeros where all tokens are masked.
  +        Expects batch-first (B, T, E)."""
  +        out = torch.zeros_like(q)
  +        if kpm.dim() == 2:
  +            with self._attn_ctx():
  +                o, _ = mha(query=q, key=k, value=v, key_padding_mask=kpm, need_weights=False)
  +            if o.dtype != out.dtype:
  +                o = o.to(dtype=out.dtype)
  +            all_masked = kpm.all(dim=1)
  +            if all_masked.dtype != torch.bool:
  +                all_masked = all_masked.to(torch.bool)
  +            # Zero-out rows where all keys are masked (out-of-place to avoid autograd versioning issues)
  +            o = torch.where(all_masked.view(-1, 1, 1), torch.zeros_like(o), o)
  +            out = o
  +        return out
  +
  +    def _mha_masked_mean(self, mha: nn.Module, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor,
  +                          q_present: torch.Tensor, k_present: torch.Tensor) -> torch.Tensor:
  +        """Compute MHA and return masked mean over query tokens using a single key_padding_mask path.
  +        Returns shape (B, E)."""
  +        B, _, E = q.shape
  +        kpm = (~k_present)
  +        # Compute per-row validity and, if possible, avoid MHA on fully-masked rows
  +        q_any = q_present.any(dim=1)
  +        k_any = k_present.any(dim=1)
  +        valid_rows = (q_any & k_any)
  +        out = torch.zeros((B, q.size(1), E), dtype=q.dtype, device=q.device)
  +        if bool(valid_rows.any()):
  +            idx = valid_rows.nonzero(as_tuple=True)[0]
  +            q_sel = q.index_select(0, idx)
  +            k_sel = k.index_select(0, idx)
  +            v_sel = v.index_select(0, idx)
  +            kpm_sel = kpm.index_select(0, idx)
  +            with self._attn_ctx():
  +                o_sel, _ = mha(query=q_sel, key=k_sel, value=v_sel, key_padding_mask=kpm_sel, need_weights=False)
  +            if o_sel.dtype != out.dtype:
  +                o_sel = o_sel.to(dtype=out.dtype)
  +            out.index_copy_(0, idx, o_sel)
  +        # Mask invalid rows to zeros (already zero) and compute masked mean across query tokens
  +        m = q_present.unsqueeze(-1).to(out.dtype)
  +        summed = (out * m).sum(dim=1)
  +        denom = m.sum(dim=1).clamp_min(1.0)
  +        return summed / denom
  +
  +    def forward(self, obs: torch.Tensor, seat_team_vec: torch.Tensor = None) -> torch.Tensor:
  +        import torch.nn.functional as F
  +        # Use the module's parameter device as the target to avoid CPU/GPU mismatches after .to(...)
  +        target_device = self.card_emb.device
  +        if not torch.is_tensor(obs):
  +            obs = torch.as_tensor(obs, dtype=torch.float32, device=target_device)
  +        if obs.dim() == 1:
  +            obs = obs.unsqueeze(0)
  +        if (obs.device != target_device) or (obs.dtype != torch.float32):
  +            obs = obs.to(device=target_device, dtype=torch.float32)
  +
  +        B = obs.size(0)
  +        D = obs.size(1)
  +        # Deterministic k inference from flags and total observation size.
  +        # D = 165 + 61*k + stats_len, stats_len = 99 + [inferred(120)] + [scopa(10)] + [rank(150)] + [dealer(4)]
  +        base_prefix = 165
  +        base_stats = 99
  +        stats_len = base_stats \
  +            + (120 if _OBS_INCLUDE_INFERRED else 0) \
  +            + (10 if _OBS_INCLUDE_SCOPA_PROBS else 0) \
  +            + (150 if _OBS_INCLUDE_RANK_PROBS else 0) \
  +            + (4 if _OBS_INCLUDE_DEALER else 0)
  +        k = 0
  +        if self.k_history_hint is not None:
  +            k = int(self.k_history_hint)
  +        else:
  +            rem = int(D) - base_prefix - stats_len
  +            if rem >= 0 and (rem % 61) == 0:
  +                k = rem // 61
  +            else:
  +                # Fallback to heuristic search (legacy), but signal mismatch
  +                notify_fallback('models.state_encoder_compact.heuristic_k')
  +                found = False
  +                for kk in range(40, -1, -1):
  +                    rem2 = D - base_prefix - 61 * kk
  +                    if rem2 < base_stats:
  +                        continue
  +                    delta = rem2 - base_stats
  +                    # Option dims: inferred(120), scopa(10), rank(150), dealer(4)
  +                    option_dims = [120, 10, 150, 4]
  +                    ok = False
  +                    for a in (0, 1):
  +                        for b in (0, 1):
  +                            for c in (0, 1):
  +                                for d in (0, 1):
  +                                    if (a * option_dims[0] + b * option_dims[1] + c * option_dims[2] + d * option_dims[3]) == delta:
  +                                        ok = True
  +                                        break
  +                                if ok:
  +                                    break
  +                            if ok:
  +                                break
  +                        if ok:
  +                            break
  +                    if ok:
  +                        k = kk
  +                        found = True
  +                        break
  +
  +        # Autocast per tutto il compute del forward compatto
  +        cm = torch.autocast(device_type=target_device.type, dtype=autocast_dtype) if target_device.type == 'cuda' else nullcontext()
  +        with cm:
  +            # Sezioni
  +            hand_table = obs[:, :83]
  +            captured = obs[:, 83:165]
  +            hist_start = 165
  +            hist_end = 165 + 61 * k
  +            history = obs[:, hist_start:hist_end]
  +            stats = obs[:, hist_end:]
  +            # Sanitize stats to avoid dynamic asserts/guards under torch.compile
  +            stats = torch.nan_to_num(stats, nan=0.0, posinf=0.0, neginf=0.0).clamp(-1e6, 1e6)
  +            # Early structure guard: stats segment must be non-empty
  +            if STRICT:
  +                if stats.size(1) <= 0:
  +                    raise RuntimeError(
  +                        f"StateEncoderCompact: empty stats segment (D={int(D)}, hist_end={int(hist_end)}, k={int(k)}, expected_stats_len≈{int(stats_len)})"
  +                    )
  +            # Strict shape check for stats segment to match configured flags
  +            if STRICT:
  +                if int(stats.size(1)) != int(stats_len):
  +                    raise RuntimeError(
  +                        f"StateEncoderCompact: stats segment dimension mismatch: got {int(stats.size(1))}, expected {int(stats_len)}; "
  +                        f"D={int(D)} hist_end={int(hist_end)} k={int(k)} flags(inferred={int(_OBS_INCLUDE_INFERRED)}, scopa={int(_OBS_INCLUDE_SCOPA_PROBS)}, rank={int(_OBS_INCLUDE_RANK_PROBS)}, dealer={int(_OBS_INCLUDE_DEALER)})"
  +                    )
  +            # Validate stats input before feeding to LazyLinear
  +            if STRICT:
  +                torch._assert(torch.isfinite(stats).all(), "StateEncoderCompact: stats input contains non-finite values")
  +                if stats.numel() > 0:
  +                    torch._assert((stats.abs().amax() <= 1e6), "StateEncoderCompact: stats input magnitude too large (>1e6)")
  +            # If LazyLinear already materialized, ensure weights are finite
  +            lin: nn.Linear = self.stats_processor[0]  # type: ignore
  +            w = getattr(lin, 'weight', None)
  +            b = getattr(lin, 'bias', None)
  +            if STRICT:
  +                if (w is not None) and (not isinstance(w, torch.nn.parameter.UninitializedParameter)) and w.numel() > 0:
  +                    torch._assert(torch.isfinite(w).all(), "StateEncoderCompact: stats_processor.weight non-finite")
  +                    if w.numel() > 0:
  +                        torch._assert((w.abs().amax() <= 1e6), "StateEncoderCompact: stats_processor.weight magnitude too large (>1e6)")
  +                if (b is not None) and (not isinstance(b, torch.nn.parameter.UninitializedParameter)) and b.numel() > 0:
  +                    torch._assert(torch.isfinite(b).all(), "StateEncoderCompact: stats_processor.bias non-finite")
  +                    if b.numel() > 0:
  +                        torch._assert((b.abs().amax() <= 1e6), "StateEncoderCompact: stats_processor.bias magnitude too large (>1e6)")
  +
  +            # ----- Set encoders -----
  +            hand_mask = hand_table[:, :40]
  +            other_counts = hand_table[:, 40:43]
  +            table_mask = hand_table[:, 43:83]
  +            # Validate inputs are finite and in [0,1]
  +            if STRICT:
  +                for name, t in (('hand_mask', hand_mask), ('other_counts', other_counts), ('table_mask', table_mask)):
  +                    torch._assert(torch.isfinite(t).all(), f"StateEncoderCompact: {name} contains non-finite values")
  +                    torch._assert((t.min().ge(0) & t.max().le(1)), f"StateEncoderCompact: {name} out of [0,1] range")
  +            card_emb = self.card_emb
  +            # Parameter sanity before use
  +            if STRICT:
  +                torch._assert(torch.isfinite(card_emb).all(), "StateEncoderCompact: card_emb contains non-finite values")
  +            w_cnt: nn.Linear = self.counts_head_hand[0]  # type: ignore
  +            if STRICT:
  +                if (not torch.isfinite(w_cnt.weight).all()) or (not torch.isfinite(w_cnt.bias).all()):
  +                    raise RuntimeError("StateEncoderCompact: counts_head_hand weights contain non-finite values")
  +            hand_feat = torch.matmul(hand_mask, card_emb)           # (B,32)
  +            table_feat = torch.matmul(table_mask, card_emb)         # (B,32)
  +            other_cnt_feat = self.counts_head_hand(other_counts)    # (B,16)
  +            if STRICT:
  +                torch._assert((torch.isfinite(hand_feat).all() & torch.isfinite(table_feat).all() & torch.isfinite(other_cnt_feat).all()), "StateEncoderCompact: non-finite set base features (hand/table/counts)")
  +            # Cross-attention mano↔tavolo
  +            hand_present = (hand_mask > 0.5)
  +            table_present = (table_mask > 0.5)
  +            hand_seq = hand_mask.unsqueeze(-1) * card_emb           # (B,40,32)
  +            table_seq = table_mask.unsqueeze(-1) * card_emb         # (B,40,32)
  +            hand_kpm = (~hand_present)
  +            table_kpm = (~table_present)
  +            # Safe MHA wrapper batched + SDPA
  +            hand_attn_feat = self._mha_masked_mean(self.cross_attn_h2t, hand_seq, table_seq, table_seq,
  +                                                   hand_present, table_present)  # (B,32)
  +            table_attn_feat = self._mha_masked_mean(self.cross_attn_t2h, table_seq, hand_seq, hand_seq,
  +                                                    table_present, hand_present)  # (B,32)
  +            if STRICT:
  +                torch._assert((torch.isfinite(hand_attn_feat).all() & torch.isfinite(table_attn_feat).all()), "StateEncoderCompact: non-finite attention features")
  +
  +            # captured
  +            cap0_mask = captured[:, :40]
  +            cap1_mask = captured[:, 40:80]
  +            cap_counts = captured[:, 80:82]
  +            cap0_feat = torch.matmul(cap0_mask, card_emb)           # (B,32)
  +            cap1_feat = torch.matmul(cap1_mask, card_emb)           # (B,32)
  +            cap_cnt_feat = self.counts_head_cap(cap_counts)         # (B,16)
  +
  +            # Reduce cat overhead by preallocating and slicing
  +            set_merged = torch.empty((B, 32*6 + 16 + 16), dtype=hand_feat.dtype, device=hand_feat.device)
  +            pos = 0
  +            set_merged[:, pos:pos+32] = hand_feat; pos += 32
  +            set_merged[:, pos:pos+32] = table_feat; pos += 32
  +            set_merged[:, pos:pos+32] = cap0_feat; pos += 32
  +            set_merged[:, pos:pos+32] = cap1_feat; pos += 32
  +            set_merged[:, pos:pos+32] = hand_attn_feat; pos += 32
  +            set_merged[:, pos:pos+32] = table_attn_feat; pos += 32
  +            set_merged[:, pos:pos+16] = other_cnt_feat; pos += 16
  +            set_merged[:, pos:pos+16] = cap_cnt_feat; pos += 16
  +            set_feat = self.set_merge_head(set_merged)               # (B,64)
  +            if STRICT:
  +                torch._assert(torch.isfinite(set_feat).all(), "StateEncoderCompact: non-finite set_feat")
  +                if set_feat.numel() > 0:
  +                    torch._assert((set_feat.abs().amax() <= 1e6), "StateEncoderCompact: set_feat magnitude too large (>1e6)")
  +
  +            # ----- History Transformer -----
  +            if k > 0:
  +                hist_reshaped = history.view(B, k, 61)               # (B,k,61)
  +                hproj = self.hist_proj(hist_reshaped)                # (B,k,64)
  +                # Use precomputed position ids buffer (max 40)
  +                pos_idx = self._hist_pos_ids[:k].unsqueeze(0).expand(B, k)
  +                hpos = self.hist_pos_emb(pos_idx)
  +                hseq = hproj + hpos
  +                # Enable efficient SDPA kernels during history attention when available
  +                with self._attn_ctx():
  +                    henc = self.hist_encoder(hseq)                   # (B,k,64)
  +                hist_feat = henc.mean(dim=1)                         # (B,64)
  +            else:
  +                hist_feat = torch.zeros((B, 64), dtype=obs.dtype, device=obs.device)
  +            if STRICT:
  +                torch._assert(torch.isfinite(hist_feat).all(), "StateEncoderCompact: non-finite hist_feat")
  +                if hist_feat.numel() > 0:
  +                    torch._assert((hist_feat.abs().amax() <= 1e6), "StateEncoderCompact: hist_feat magnitude too large (>1e6)")
  +
  +            # Stats e seat/team
  +            stats_feat = self.stats_processor(stats)
  +            if STRICT:
  +                if not torch.isfinite(stats_feat).all():
  +                    # Detailed diagnostics on failure
  +                    lin: nn.Linear = self.stats_processor[0]  # type: ignore
  +                    w = getattr(lin, 'weight', None)
  +                    b = getattr(lin, 'bias', None)
  +                    def _st(t: torch.Tensor):
  +                        return {
  +                            'min': float(t.min().item()) if t.numel() > 0 else None,
  +                            'max': float(t.max().item()) if t.numel() > 0 else None,
  +                            'mean': float(t.mean().item()) if t.numel() > 0 else None,
  +                            'numel': int(t.numel())
  +                        }
  +                    w_stats = _st(w)
  +                    b_stats = _st(b)
  +                    s_stats = _st(stats)
  +                    raise RuntimeError(f"StateEncoderCompact: non-finite stats_feat; weight_stats={w_stats}, bias_stats={b_stats}, stats_input_stats={s_stats}")
  +                if stats_feat.numel() > 0:
  +                    torch._assert((stats_feat.abs().amax() <= 1e6), "StateEncoderCompact: stats_feat magnitude too large (>1e6)")
  +            if seat_team_vec is None:
  +                seat_team_vec = torch.zeros((B, 6), dtype=torch.float32, device=obs.device)
  +            elif seat_team_vec.dim() == 1:
  +                seat_team_vec = seat_team_vec.unsqueeze(0)
  +            # Validate seat/team vector: one-hot seat and flags in [0,1]
  +            if STRICT:
  +                if seat_team_vec.size(-1) != 6:
  +                    raise ValueError("StateEncoderCompact: seat_team_vec must have shape (B,6)")
  +                if not (seat_team_vec[:, :4].sum(dim=1) == 1).all():
  +                    raise RuntimeError("StateEncoderCompact: seat one-hot invalid (sum != 1)")
  +                if ((seat_team_vec[:, 4:6] < 0) | (seat_team_vec[:, 4:6] > 1)).any():
  +                    raise RuntimeError("StateEncoderCompact: team flags out of [0,1]")
  +            seat_feat = F.relu(self.seat_head[0](seat_team_vec), inplace=True)
  +            if STRICT:
  +                torch._assert(torch.isfinite(seat_feat).all(), "StateEncoderCompact: non-finite seat_feat")
  +                if seat_feat.numel() > 0:
  +                    torch._assert((seat_feat.abs().amax() <= 1e6), "StateEncoderCompact: seat_feat magnitude too large (>1e6)")
  +
  +            combined = torch.empty((B, 64+64+64+32), dtype=set_feat.dtype, device=set_feat.device)
  +            p2 = 0
  +            combined[:, p2:p2+64] = set_feat; p2 += 64
  +            combined[:, p2:p2+64] = hist_feat; p2 += 64
  +            combined[:, p2:p2+64] = stats_feat; p2 += 64
  +            combined[:, p2:p2+32] = seat_feat; p2 += 32
  +            context = F.relu(self.combiner[0](combined), inplace=True)
  +            if STRICT:
  +                torch._assert(torch.isfinite(context).all(), "StateEncoderCompact: non-finite context output")
  +                if context.numel() > 0:
  +                    torch._assert((context.abs().amax() <= 1e6), "StateEncoderCompact: context magnitude too large (>1e6)")
  +        return context
  +
  +
  +class ActionEncoder80(nn.Module):
  +    """Encoda l'azione binaria 80-dim in un embedding 64-dim."""
  +    def __init__(self, action_dim: int = 80):
  +        super().__init__()
  +        self.net = nn.Sequential(
  +            nn.Linear(action_dim, 128), nn.ReLU(),
  +            nn.Linear(128, 64), nn.ReLU(),
  +        )
  +        self.to(device)
  +
  +    def forward(self, actions: torch.Tensor) -> torch.Tensor:
  +        target_device = next(self.parameters()).device
  +        if not torch.is_tensor(actions):
  +            actions = torch.as_tensor(actions, dtype=torch.float32, device=target_device)
  +        if actions.dim() == 1:
  +            actions = actions.unsqueeze(0)
  +        if (actions.device != target_device) or (actions.dtype != torch.float32):
  +            actions = actions.to(device=target_device, dtype=torch.float32)
  +        # Validate last-dimension matches expected input features
  +        try:
  +            in_dim = int(self.net[0].in_features)  # type: ignore[attr-defined]
  +        except Exception:
  +            in_dim = 80
  +        if actions.size(-1) != in_dim:
  +            raise ValueError(f"ActionEncoder80: expected last-dim {in_dim}, got {int(actions.size(-1))}")
  +        out = self.net(actions)
  +        # Keep checks only in STRICT mode
  +        if STRICT:
  +            torch._assert(torch.isfinite(out).all(), "ActionEncoder80.forward: non-finite output embedding")
  +            if out.numel() > 0:
  +                max_abs = out.abs().amax()
  +                torch._assert((max_abs <= 1e6), "ActionEncoder80.forward: output magnitude too large (>1e6)")
  +        return out
  +
  +
  +class BeliefNet(nn.Module):
  +    """
  +    Belief network che predice 3x40 logits (altri giocatori) a partire da state features (256).
  +    - Architettura profonda con residual, LayerNorm e GELU per capacità e stabilità.
  +    - Parametro di temperatura per calibrazione delle probabilità.
  +    """
  +    def __init__(self, in_dim: int = 256, hidden_dim: int = 512):
  +        super().__init__()
  +        self.fc_in = nn.Linear(in_dim, hidden_dim)
  +        self.ln1 = nn.LayerNorm(hidden_dim)
  +        self.fc_mid1 = nn.Linear(hidden_dim, hidden_dim)
  +        self.ln2 = nn.LayerNorm(hidden_dim)
  +        self.fc_mid2 = nn.Linear(hidden_dim, hidden_dim)
  +        self.ln3 = nn.LayerNorm(hidden_dim)
  +        self.fc_out = nn.Linear(hidden_dim, 120)
  +        # temperatura appresa (clampata in (0.25, 4.0))
  +        self._log_temp = nn.Parameter(torch.log(torch.tensor(1.0)))
  +        # CPU-friendly: allow approximate GELU when enabled via env (preserves quality in practice)
  +        approx_gelu = (os.environ.get('SCOPONE_APPROX_GELU', '0') == '1')
  +        self.act = nn.GELU(approximate='tanh' if approx_gelu else 'none')
  +        self.dropout = nn.Dropout(p=0.1)
  +        self.to(device)
  +
  +    def forward(self, state_feat: torch.Tensor) -> torch.Tensor:
  +        x = state_feat
  +        h = self.act(self.ln1(self.fc_in(x)))
  +        h = self.dropout(h)
  +        # residuo 1
  +        r = self.act(self.ln2(self.fc_mid1(h)))
  +        h = self.dropout(h + r)
  +        # residuo 2
  +        r2 = self.act(self.ln3(self.fc_mid2(h)))
  +        h = self.dropout(h + r2)
  +        logits = self.fc_out(h)
  +        return logits  # (B,120) logits per 3x40
  +
  +    def temperature(self) -> torch.Tensor:
  +        log_temp = self._log_temp
  +        if not torch.isfinite(log_temp).all():
  +            raise RuntimeError("BeliefNet.temperature: _log_temp contains non-finite values")
  +        temp = torch.exp(log_temp)
  +        if STRICT:
  +            torch._assert(torch.isfinite(temp).all(), "BeliefNet.temperature: exp(log_temp) produced non-finite values")
  +        return torch.clamp(temp, 0.25, 4.0)
  +
  +    def probs(self, logits: torch.Tensor, visible_mask_40: torch.Tensor = None) -> torch.Tensor:
  +        """
  +        Converte logits 3x40 in probabilità normalizzate per-carta tra i 3 giocatori.
  +        - logits: (B,120)
  +        - visible_mask_40: (B,40) boolean, True = carta visibile → probabilità a 0
  +        Ritorna: (B,120) flatten di (B,3,40) probabilità
  +        """
  +        if logits.dim() == 1:
  +            logits = logits.unsqueeze(0)
  +        B = logits.size(0)
  +        t = self.temperature().to(logits.device, dtype=logits.dtype)
  +        x = logits.view(B, 3, 40)
  +        x = x / t
  +        # softmax per-carta (dim=1 sui 3 giocatori)
  +        probs = torch.softmax(x, dim=1)
  +        if STRICT:
  +            torch._assert(torch.isfinite(probs).all(), "BeliefNet.probs: softmax produced non-finite probabilities")
  +        if visible_mask_40 is not None:
  +            if visible_mask_40.dim() == 1:
  +                visible_mask_40 = visible_mask_40.unsqueeze(0)
  +            # Azzera probabilità per carte visibili e rinormalizza solo dove la somma per carta > 0
  +            m = visible_mask_40.to(probs.dtype).unsqueeze(1)  # (B,1,40)
  +            probs = probs * (1.0 - m)
  +            sums = probs.sum(dim=1, keepdim=True)  # (B,1,40)
  +            nz = (sums > 0)
  +            # Broadcast-safe: divide solo sulle posizioni nz, altrimenti lascia 0
  +            probs = torch.where(nz.expand_as(probs), probs / torch.clamp_min(sums, 1e-12), probs)
  +        return probs.view(B, 120)
  +
  +
  +class ActionConditionedActor(torch.nn.Module):
  +    """
  +    Actor realmente action-conditioned:
  +      - State encoder compatto (usa storia-k con pooling) → 256-d
  +      - Belief head (120 → 64)
  +      - Proiezione stato → 64 e scoring via prodotto scalare con embedding azione (80 → 64)
  +    """
--     def __init__(self, obs_dim=10823, action_dim=80, state_encoder: StateEncoderCompact = None):
+++    def __init__(self, obs_dim: Optional[int] = None, action_dim: int = 80, state_encoder: StateEncoderCompact = None):
  +        super().__init__()
+++        if obs_dim is None:
+++            obs_dim = _get_compact_obs_dim(_default_k_history())
  +        self.obs_dim = obs_dim
  +        self.action_dim = action_dim
  +        # Encoders
  +        self.state_enc = state_encoder if state_encoder is not None else StateEncoderCompact()
  +        self.belief_head = nn.Sequential(nn.Linear(120, 64), nn.ReLU())
  +        # BeliefNet neurale migliorata (state_feat 256 -> logits 120)
  +        self.belief_net = BeliefNet(in_dim=256, hidden_dim=512)
  +        # Partner-aware: embed per-carta (40→32) e gating separato partner/opps
  +        self.belief_card_emb = nn.Parameter(torch.randn(40, 32, device=device, dtype=torch.float32) * 0.02)
  +        self.partner_gate = nn.Sequential(nn.Linear(256, 32), nn.Sigmoid())
  +        self.opp_gate = nn.Sequential(nn.Linear(256, 32), nn.Sigmoid())
  +        # Merge: stato 256 + belief_head 64 + partner 32 + opp 32 = 384
  +        self.merge = nn.Sequential(nn.Linear(384, 256), nn.ReLU())
  +        self.state_to_action = nn.Linear(256, 64)
  +        # Initialize small and stable
  +        nn.init.kaiming_uniform_(self.state_to_action.weight, a=math.sqrt(5))
  +        if self.state_to_action.bias is not None:
  +            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.state_to_action.weight)
  +            bound = 1 / math.sqrt(fan_in)
  +            nn.init.uniform_(self.state_to_action.bias, -bound, bound)
  +        self.action_enc = ActionEncoder80(action_dim)
  +        # Embedding per la selezione carta (40 carte)
  +        self.card_emb_play = nn.Parameter(torch.randn(40, 64, device=device, dtype=torch.float32) * 0.02)
  +        # Cache di tutte le azioni one-hot (80 x 80) per calcolare logits pieni
  +        self.register_buffer('all_actions_eye', torch.eye(action_dim, dtype=torch.float32))
  +        # Cache embedding azioni per device/dtype (solo per inference)
  +        self._cached_action_emb = None  # legacy single-cache for backward compat
  +        self._cached_action_emb_variants: Dict[Tuple[str, torch.dtype], torch.Tensor] = {}
  +        self.to(device)
  +
  +    @staticmethod
  +    def _visible_mask_from_obs(x_obs: torch.Tensor) -> torch.Tensor:
  +        hand_table = x_obs[:, :83]
  +        # Single threshold mask and slicing to avoid repeated > 0.5 ops
  +        mask_all = hand_table > 0.5
  +        hand_mask = mask_all[:, :40]
  +        table_mask = mask_all[:, 43:83]
  +        captured = x_obs[:, 83:165] > 0.5
  +        cap0_mask = captured[:, :40]
  +        cap1_mask = captured[:, 40:80]
  +        return hand_mask | table_mask | cap0_mask | cap1_mask
  +
  +    def compute_state_proj(self, obs: torch.Tensor, seat_team_vec: torch.Tensor) -> torch.Tensor:
  +        target_device = next(self.parameters()).device
  +        _par = (os.environ.get('SCOPONE_PROFILE', '0') != '0')
  +        t_state_enc = 0.0; t_belief_logits = 0.0; t_belief_probs = 0.0
  +        t_partner = 0.0; t_opp = 0.0; t_merge = 0.0; t_proj = 0.0
  +        _t0 = time.time() if _par else 0.0
  +        if torch.is_tensor(obs):
  +            if (obs.device == target_device) and (obs.dtype == torch.float32):
  +                x_obs = obs
  +            elif obs.device == target_device:
  +                x_obs = obs.to(dtype=torch.float32)
  +            else:
  +                x_obs = obs.to(device=target_device, dtype=torch.float32)
  +        else:
  +            x_obs = torch.as_tensor(obs, dtype=torch.float32, device=target_device)
  +        if x_obs.dim() == 1:
  +            x_obs = x_obs.unsqueeze(0)
  +        if seat_team_vec is None:
  +            raise ValueError("seat_team_vec is required (B,6)")
  +        else:
  +            seat_team_vec = (seat_team_vec if torch.is_tensor(seat_team_vec) else torch.as_tensor(seat_team_vec, dtype=torch.float32))
  +            if seat_team_vec.dim() == 1:
  +                seat_team_vec = seat_team_vec.unsqueeze(0)
  +            if (seat_team_vec.device != x_obs.device) or (seat_team_vec.dtype != torch.float32):
  +                seat_team_vec = seat_team_vec.to(x_obs.device, dtype=torch.float32)
  +        t1 = time.time() if _par else 0.0
  +        state_feat = self.state_enc(x_obs, seat_team_vec)  # (B,256)
  +        if _par: t_state_enc += (time.time() - t1)
  +        if STRICT:
  +            torch._assert(torch.isfinite(state_feat).all(), "state_enc produced non-finite features")
  +            if state_feat.numel() > 0:
  +                torch._assert((state_feat.abs().amax() <= 1e6), "compute_state_proj: state_feat magnitude too large (>1e6)")
  +        # Ensure BeliefNet receives its parameter dtype (avoids Half/Float mismatch outside autocast)
  +        bn_dtype = self.belief_net.fc_in.weight.dtype
  +        if state_feat.dtype != bn_dtype:
  +            state_feat = state_feat.to(dtype=bn_dtype)
  +        # belief neurale interno con maschera carte visibili
  +        visible_mask = self._visible_mask_from_obs(x_obs)
  +        t1 = time.time() if _par else 0.0
  +        belief_logits = self.belief_net(state_feat)        # (B,120)
  +        if _par: t_belief_logits += (time.time() - t1)
  +        # Safety clamp before softmax to avoid NaNs from extreme values
  +        belief_logits = torch.nan_to_num(belief_logits, nan=0.0, posinf=1e6, neginf=-1e6).clamp(-30.0, 30.0)
  +        if STRICT:
  +            torch._assert(torch.isfinite(belief_logits).all(), "BeliefNet produced non-finite logits")
  +        t1 = time.time() if _par else 0.0
  +        belief_probs_flat = self.belief_net.probs(belief_logits, visible_mask)
  +        if _par: t_belief_probs += (time.time() - t1)
  +        belief_probs_flat = torch.nan_to_num(belief_probs_flat, nan=0.0, posinf=0.0, neginf=0.0)
  +        if STRICT:
  +            torch._assert(torch.isfinite(belief_probs_flat).all(), "BeliefNet.probs produced non-finite probabilities")
  +        # Probability range guard
  +        if STRICT:
  +            torch._assert(((belief_probs_flat >= 0).all() & (belief_probs_flat <= 1).all()), "compute_state_proj: belief_probs out of [0,1]")
  +        belief_feat = self.belief_head(belief_probs_flat)  # (B,64)
  +        if STRICT:
  +            torch._assert(torch.isfinite(belief_feat).all(), "compute_state_proj: belief_feat non-finite")
  +            if belief_feat.numel() > 0:
  +                torch._assert((belief_feat.abs().amax() <= 1e6), "compute_state_proj: belief_feat magnitude too large (>1e6)")
  +        partner_slice = belief_probs_flat[:, 40:80]
  +        opps_slice = belief_probs_flat[:, 0:40] + belief_probs_flat[:, 80:120]
  +        emb = self.belief_card_emb
  +        t1 = time.time() if _par else 0.0
  +        partner_feat = torch.matmul(partner_slice, emb)     # (B,32)
  +        opp_feat = torch.matmul(opps_slice, emb)            # (B,32)
  +        if _par: t_partner += (time.time() - t1)
  +        t1 = time.time() if _par else 0.0
  +        pg = self.partner_gate(state_feat)
  +        og = self.opp_gate(state_feat)
  +        if _par: t_opp += (time.time() - t1)
  +        # Gating range guards
  +        if STRICT:
  +            torch._assert(((pg >= 0).all() & (pg <= 1).all()), "compute_state_proj: partner_gate out of [0,1]")
  +            torch._assert(((og >= 0).all() & (og <= 1).all()), "compute_state_proj: opp_gate out of [0,1]")
  +        partner_feat = partner_feat * pg
  +        opp_feat = opp_feat * og
  +        if STRICT:
  +            torch._assert(torch.isfinite(partner_feat).all(), "compute_state_proj: partner_feat non-finite before merge")
  +            torch._assert(torch.isfinite(opp_feat).all(), "compute_state_proj: opp_feat non-finite before merge")
  +            if partner_feat.numel() > 0:
  +                torch._assert((partner_feat.abs().amax() <= 1e6), "compute_state_proj: partner_feat magnitude too large (>1e6)")
  +            if opp_feat.numel() > 0:
  +                torch._assert((opp_feat.abs().amax() <= 1e6), "compute_state_proj: opp_feat magnitude too large (>1e6)")
  +        B_ctx = state_feat.size(0)
  +        t1 = time.time() if _par else 0.0
  +        ctx_in = torch.empty((B_ctx, 256 + 64 + 32 + 32), dtype=state_feat.dtype, device=state_feat.device)
  +        p = 0
  +        ctx_in[:, p:p+256] = state_feat; p += 256
  +        ctx_in[:, p:p+64] = belief_feat; p += 64
  +        ctx_in[:, p:p+32] = partner_feat; p += 32
  +        ctx_in[:, p:p+32] = opp_feat; p += 32
  +        # Lightweight compile-friendly guard (tensor assert)
  +        if STRICT:
  +            if ctx_in.numel() > 0:
  +                torch._assert((ctx_in.abs().amax() <= 1e6), "merge input (ctx_in) magnitude too large (>1e6)")
  +        state_ctx = self.merge(ctx_in)  # (B,256)
  +        if _par: t_merge += (time.time() - t1)
  +        if STRICT:
  +            torch._assert(torch.isfinite(state_ctx).all(), "merge produced non-finite state_ctx")
  +            # Guard extremely large activations (tensor assert)
  +            if state_ctx.numel() > 0:
  +                torch._assert((state_ctx.abs().amax() <= 1e6), "merge produced extremely large state_ctx (>1e6)")
  +        t1 = time.time() if _par else 0.0
  +        state_proj = self.state_to_action(state_ctx)  # (B,64)
  +        if _par: t_proj += (time.time() - t1)
  +        if STRICT:
  +            torch._assert(torch.isfinite(state_proj).all(), "state_to_action produced non-finite state_proj")
  +            if state_proj.numel() > 0:
  +                torch._assert((state_proj.abs().amax() <= 1e6), "state_to_action produced extremely large state_proj (>1e6)")
  +        # Parameter guard for card_emb_play
  +        # Evita sync CPU: controlla in modo leggero
  +        if STRICT:
  +            torch._assert((self.card_emb_play.abs().amax() <= 1e3), "card_emb_play parameter magnitude exploded (>1e3)")
  +        # Export sub-timers via a lightweight global (avoid imports to trainer)
  +        if _par:
  +            from utils.prof import accum_actor_stateproj
  +            accum_actor_stateproj(t_state_enc, t_belief_logits, t_belief_probs, t_partner + t_opp, t_merge, t_proj)
  +        return state_proj
  +
  +    def compute_state_features(self, obs: torch.Tensor, seat_team_vec: torch.Tensor) -> torch.Tensor:
  +        """Calcola solo le feature di stato (256) dal pair (obs, seat)."""
  +        target_device = next(self.parameters()).device
  +        if torch.is_tensor(obs):
  +            if (obs.device == target_device) and (obs.dtype == torch.float32):
  +                x_obs = obs
  +            elif obs.device == target_device:
  +                x_obs = obs.to(dtype=torch.float32)
  +            else:
  +                x_obs = obs.to(device=target_device, dtype=torch.float32)
  +        else:
  +            x_obs = torch.as_tensor(obs, dtype=torch.float32, device=target_device)
  +        if x_obs.dim() == 1:
  +            x_obs = x_obs.unsqueeze(0)
  +        if seat_team_vec is None:
  +            raise ValueError("seat_team_vec is required (B,6)")
  +        else:
  +            seat_team_vec = (seat_team_vec if torch.is_tensor(seat_team_vec) else torch.as_tensor(seat_team_vec, dtype=torch.float32))
  +            if seat_team_vec.dim() == 1:
  +                seat_team_vec = seat_team_vec.unsqueeze(0)
  +            if (seat_team_vec.device != x_obs.device) or (seat_team_vec.dtype != torch.float32):
  +                seat_team_vec = seat_team_vec.to(x_obs.device, dtype=torch.float32)
  +        sf = self.state_enc(x_obs, seat_team_vec)  # (B,256)
  +        if STRICT:
  +            if not torch.isfinite(sf).all():
  +                bad = sf[~torch.isfinite(sf)]
  +                raise RuntimeError(f"state_enc produced non-finite features (count={int(bad.numel())})")
  +        return sf
  +
  +    def compute_state_proj_from_state(self, state_feat: torch.Tensor, x_obs: torch.Tensor, visible_mask_40: torch.Tensor = None) -> torch.Tensor:
  +        """Proietta feature di stato (256) in spazio azione (64) usando belief/gating dell'actor.
  +        Richiede l'osservazione per calcolare la maschera carte visibili.
  +        """
  +        target_device = next(self.parameters()).device
  +        if x_obs.dim() == 1:
  +            x_obs = x_obs.unsqueeze(0)
  +        if x_obs.device != target_device:
  +            x_obs = x_obs.to(device=target_device)
  +        if state_feat.device != target_device:
  +            state_feat = state_feat.to(device=target_device)
  +        visible_mask = (visible_mask_40 if visible_mask_40 is not None else self._visible_mask_from_obs(x_obs))
  +        belief_logits = self.belief_net(state_feat)        # (B,120)
  +        belief_logits = torch.nan_to_num(belief_logits, nan=0.0, posinf=1e6, neginf=-1e6).clamp(-30.0, 30.0)
  +        if STRICT:
  +            if not torch.isfinite(belief_logits).all():
  +                bad = belief_logits[~torch.isfinite(belief_logits)]
  +                raise RuntimeError(f"BeliefNet produced non-finite logits (count={int(bad.numel())})")
  +        belief_probs_flat = self.belief_net.probs(belief_logits, visible_mask)
  +        belief_probs_flat = torch.nan_to_num(belief_probs_flat, nan=0.0, posinf=0.0, neginf=0.0)
  +        if STRICT:
  +            if not torch.isfinite(belief_probs_flat).all():
  +                bad = belief_probs_flat[~torch.isfinite(belief_probs_flat)]
  +                raise RuntimeError(f"BeliefNet.probs produced non-finite probabilities (count={int(bad.numel())})")
  +        belief_feat = self.belief_head(belief_probs_flat)  # (B,64)
  +        partner_slice = belief_probs_flat[:, 40:80]
  +        opps_slice = belief_probs_flat[:, 0:40] + belief_probs_flat[:, 80:120]
  +        emb = self.belief_card_emb
  +        partner_feat = torch.matmul(partner_slice, emb)     # (B,32)
  +        opp_feat = torch.matmul(opps_slice, emb)            # (B,32)
  +        pg = self.partner_gate(state_feat)
  +        og = self.opp_gate(state_feat)
  +        partner_feat = partner_feat * pg
  +        opp_feat = opp_feat * og
  +        B_ctx = state_feat.size(0)
  +        ctx_in = torch.empty((B_ctx, 256 + 64 + 32 + 32), dtype=state_feat.dtype, device=state_feat.device)
  +        p = 0
  +        ctx_in[:, p:p+256] = state_feat; p += 256
  +        ctx_in[:, p:p+64] = belief_feat; p += 64
  +        ctx_in[:, p:p+32] = partner_feat; p += 32
  +        ctx_in[:, p:p+32] = opp_feat; p += 32
  +        state_ctx = self.merge(ctx_in)  # (B,256)
  +        if STRICT:
  +            if not torch.isfinite(state_ctx).all():
  +                bad = state_ctx[~torch.isfinite(state_ctx)]
  +                raise RuntimeError(f"merge produced non-finite state_ctx (count={int(bad.numel())})")
  +        if STRICT:
  +            if state_ctx.numel() > 0:
  +                max_abs_ctx = state_ctx.abs().amax()
  +                torch._assert((max_abs_ctx <= 1e6), "merge produced extremely large state_ctx (>1e6)")
  +        sp = self.state_to_action(state_ctx)
  +        if STRICT:
  +            if not torch.isfinite(sp).all():
  +                bad = sp[~torch.isfinite(sp)]
  +                raise RuntimeError(f"state_to_action produced non-finite state_proj (count={int(bad.numel())})")
  +        if STRICT:
  +            if sp.numel() > 0:
  +                max_abs_proj = sp.abs().amax()
  +                torch._assert((max_abs_proj <= 1e6), "state_to_action produced extremely large state_proj (>1e6)")
  +            torch._assert((self.card_emb_play.abs().amax() <= 1e3), "card_emb_play parameter magnitude exploded")
  +        return sp
  +
  +    def invalidate_action_cache(self) -> None:
  +        """Invalida la cache degli embedding delle azioni (usata in inference)."""
  +        self._cached_action_emb = None
  +        self._cached_action_emb_variants.clear()
  +
  +    def get_action_emb_table_cached(self, device: Optional[torch.device] = None, dtype: Optional[torch.dtype] = None) -> torch.Tensor:
  +        """Ritorna la tabella (80,64) degli embedding azione.
  +        Mantiene una cache per (device,dtype) per evitare copie .to ripetute.
  +        Se device/dtype non sono specificati, usa il device del modulo e float32.
  +        """
  +        target_device = device or next(self.action_enc.parameters()).device
  +        target_dtype = dtype or next(self.action_enc.parameters()).dtype
  +        key = (str(target_device), target_dtype)
  +        cached = self._cached_action_emb_variants.get(key, None)
  +        if cached is not None:
  +            return cached
  +
  +        eye = self.all_actions_eye.to(device=target_device, dtype=torch.float32)
  +        tbl = self.action_enc(eye).to(device=target_device, dtype=target_dtype).contiguous()
  +        self._cached_action_emb_variants[key] = tbl
  +        return tbl
  +
  +    def forward(self, obs: torch.Tensor, legals: torch.Tensor = None,
  +                seat_team_vec: torch.Tensor = None) -> torch.Tensor:
  +        # Stato: (B, D)
  +        target_device = next(self.parameters()).device
  +        if torch.is_tensor(obs):
  +            if (obs.device == target_device) and (obs.dtype == torch.float32):
  +                x_obs = obs
  +            elif obs.device == target_device:
  +                x_obs = obs.to(dtype=torch.float32)
  +            else:
  +                x_obs = obs.to(device=target_device, dtype=torch.float32)
  +        else:
  +            x_obs = torch.as_tensor(obs, dtype=torch.float32, device=target_device)
  +        if x_obs.dim() == 1:
  +            x_obs = x_obs.unsqueeze(0)
  +        if seat_team_vec is None:
  +            raise ValueError("seat_team_vec is required (B,6)")
  +        else:
  +            seat_team_vec = (seat_team_vec if torch.is_tensor(seat_team_vec) else torch.as_tensor(seat_team_vec, dtype=torch.float32))
  +            if seat_team_vec.dim() == 1:
  +                seat_team_vec = seat_team_vec.unsqueeze(0)
  +            if (seat_team_vec.device != x_obs.device) or (seat_team_vec.dtype != torch.float32):
  +                seat_team_vec = seat_team_vec.to(x_obs.device, dtype=torch.float32)
  +        # belief neurale interno con maschera carte visibili
  +        state_feat = self.state_enc(x_obs, seat_team_vec)  # (B,256)
  +        bn_dtype = self.belief_net.fc_in.weight.dtype
  +        if state_feat.dtype != bn_dtype:
  +            state_feat = state_feat.to(dtype=bn_dtype)
  +        visible_mask = self._visible_mask_from_obs(x_obs)
  +        belief_logits = self.belief_net(state_feat)
  +        belief_logits = torch.nan_to_num(belief_logits, nan=0.0, posinf=1e6, neginf=-1e6).clamp(-30.0, 30.0)
  +        if not torch.isfinite(belief_logits).all():
  +            bad = belief_logits[~torch.isfinite(belief_logits)]
  +            raise RuntimeError(f"BeliefNet produced non-finite logits (count={int(bad.numel())})")
  +        belief_probs_flat = self.belief_net.probs(belief_logits, visible_mask)
  +        if not torch.isfinite(belief_probs_flat).all():
  +            bad = belief_probs_flat[~torch.isfinite(belief_probs_flat)]
  +            raise RuntimeError(f"BeliefNet.probs produced non-finite probabilities (count={int(bad.numel())})")
  +        belief_feat = self.belief_head(belief_probs_flat)  # (B,64)
  +        # Partner index fisso nel nostro belief: slice centrale [40:80]
  +        partner_slice = belief_probs_flat[:, 40:80]
  +        opps_slice = belief_probs_flat[:, 0:40] + belief_probs_flat[:, 80:120]
  +        emb = self.belief_card_emb
  +        partner_feat = torch.matmul(partner_slice, emb)     # (B,32)
  +        opp_feat = torch.matmul(opps_slice, emb)            # (B,32)
  +        # Gating dipendente dallo stato
  +        pg = self.partner_gate(state_feat)
  +        og = self.opp_gate(state_feat)
  +        partner_feat = partner_feat * pg
  +        opp_feat = opp_feat * og
  +        B_ctx2 = state_feat.size(0)
  +        ctx2_in = torch.empty((B_ctx2, 256 + 64 + 32 + 32), dtype=state_feat.dtype, device=state_feat.device)
  +        p2 = 0
  +        ctx2_in[:, p2:p2+256] = state_feat; p2 += 256
  +        ctx2_in[:, p2:p2+64] = belief_feat; p2 += 64
  +        ctx2_in[:, p2:p2+32] = partner_feat; p2 += 32
  +        ctx2_in[:, p2:p2+32] = opp_feat; p2 += 32
  +        state_ctx = self.merge(ctx2_in)  # (B,256)
  +        if not torch.isfinite(state_ctx).all():
  +            bad = state_ctx[~torch.isfinite(state_ctx)]
  +            raise RuntimeError(f"merge produced non-finite state_ctx (count={int(bad.numel())})")
  +        state_proj = self.state_to_action(state_ctx)  # (B,64)
  +        if not torch.isfinite(state_proj).all():
  +            bad = state_proj[~torch.isfinite(state_proj)]
  +            raise RuntimeError(f"state_to_action produced non-finite state_proj (count={int(bad.numel())})")
  +
  +        if legals is None:
  +            # Calcola logits per tutte le 80 azioni: (B,64) @ (64,80) -> (B,80)
  +            all_actions = self.all_actions_eye
  +            # usa cache embedding azioni se disponibile
  +            action_emb = self.get_action_emb_table_cached(device=state_proj.device, dtype=state_proj.dtype)
  +            logits = torch.matmul(state_proj, action_emb.t())  # (B,80)
  +            return logits if logits.size(0) > 1 else logits.squeeze(0)
  +        # legals: (A,80). Calcola score per azioni legali via prodotto scalare
  +        if not torch.is_tensor(legals):
  +            legals_t = torch.as_tensor(legals, dtype=torch.float32, device=state_proj.device)
  +        else:
  +            legals_t = legals.to(state_proj.device, dtype=torch.float32)
  +        # Validate legals shape and structure
  +        if legals_t.dim() != 2 or legals_t.size(1) != 80:
  +            raise ValueError(f"Actor.forward: legals must be (A,80), got {tuple(legals_t.shape)}")
  +        ones = legals_t[:, :40].sum(dim=1)
  +        if STRICT:
  +            torch._assert(torch.allclose(ones, torch.ones_like(ones)), "Actor.forward: each legal must have exactly one played bit in [:40]")
  +        cap = legals_t[:, 40:]
  +        cap_bad = ((cap > 0.0 + 1e-6) & (cap < 1.0 - 1e-6)) | (cap < -1e-6) | (cap > 1.0 + 1.0e-6)
  +        torch._assert((~cap_bad).all(), "Actor.forward: captured section must be binary (0/1)")
  +        # B atteso = 1 in path di selezione
  +        # In training evita la tabella cache per mantenere gradiente
  +        if self.training:
  +            a_emb = self.action_enc(legals_t)
  +        else:
  +            a_tbl = self.get_action_emb_table_cached(device=legals_t.device, dtype=state_proj.dtype)
  +            a_emb = torch.matmul(legals_t, a_tbl)
  +        if state_proj.size(0) == 1:
  +            scores = torch.matmul(a_emb, state_proj.squeeze(0))  # (A)
  +        else:
  +            # Row-wise dot product: (A,64) ⊙ (A,64) → (A)
  +            scores = (a_emb * state_proj).sum(dim=1)
  +        return scores
  +
  +    def compute_two_stage_logp(self,
  +                                card_logits_all: torch.Tensor,
  +                                legals_mb: torch.Tensor,
  +                                sample_idx_per_legal: torch.Tensor,
  +                                state_proj: torch.Tensor,
  +                                a_emb_mb: torch.Tensor) -> torch.Tensor:
  +        """Compute per-legal total log-prob under the two-stage policy.
  +        Inputs:
  +          - card_logits_all: (B,40)
  +          - legals_mb: (M,80)
  +          - sample_idx_per_legal: (M,) mapping each legal row to its sample index [0..B)
  +          - state_proj: (B,64)
  +          - a_emb_mb: (M,64) action embeddings for each legal row (use action_enc during training)
  +        Returns:
  +          - logp_total_per_legal: (M,)
  +        """
  +        device_local = card_logits_all.device
  +        dtype_local = card_logits_all.dtype
  +        if legals_mb.numel() == 0:
  +            return torch.zeros((0,), dtype=dtype_local, device=device_local)
  +        # Validate shapes
  +        if card_logits_all.dim() != 2 or card_logits_all.size(1) != 40:
  +            raise ValueError(f"compute_two_stage_logp: card_logits_all must be (B,40), got {tuple(card_logits_all.shape)}")
  +        if state_proj.dim() != 2 or state_proj.size(1) != 64:
  +            raise ValueError(f"compute_two_stage_logp: state_proj must be (B,64), got {tuple(state_proj.shape)}")
  +        if legals_mb.dim() != 2 or legals_mb.size(1) != 80:
  +            raise ValueError(f"compute_two_stage_logp: legals_mb must be (M,80), got {tuple(legals_mb.shape)}")
  +        if a_emb_mb.dim() != 2 or a_emb_mb.size(1) != 64:
  +            raise ValueError(f"compute_two_stage_logp: a_emb_mb must be (M,64), got {tuple(a_emb_mb.shape)}")
  +
  +        # Played card ids per legal
  +        ones_per_row = legals_mb[:, :40].sum(dim=1)
  +        if os.environ.get('SCOPONE_STRICT_CHECKS', '0') == '1':
  +            if not torch.allclose(ones_per_row, torch.ones_like(ones_per_row)):
  +                raise RuntimeError("compute_two_stage_logp: legals must have exactly one played bit in [:40]")
  +        played_ids_mb = torch.argmax(legals_mb[:, :40], dim=1)  # (M)
  +
  +        # Card log-prob restricted to allowed set per sample
  +        B = int(card_logits_all.size(0))
  +        allowed_mask = torch.zeros((B, 40), dtype=torch.bool, device=device_local)
  +        allowed_mask[sample_idx_per_legal, played_ids_mb] = True
  +        masked_logits = torch.where(allowed_mask, card_logits_all, torch.full_like(card_logits_all, float('-inf')))
  +        max_allowed = torch.amax(masked_logits, dim=1)
  +        # Avoid in-place ops on tensors needed for gradient; keep computation out-of-place
  +        exp_shift_allowed = torch.exp(card_logits_all - max_allowed.unsqueeze(1)) * allowed_mask.to(card_logits_all.dtype)
  +        sum_allowed = exp_shift_allowed.sum(dim=1)
  +        lse_allowed = max_allowed + torch.log(torch.clamp_min(sum_allowed, 1e-12))  # (B)
  +        logp_cards_allowed_per_legal = (card_logits_all[sample_idx_per_legal, played_ids_mb] - lse_allowed[sample_idx_per_legal])  # (M)
  +
  +        # Capture logits per legal using action embeddings
  +        cap_logits = (a_emb_mb * state_proj[sample_idx_per_legal]).sum(dim=1)
  +        if not torch.isfinite(cap_logits).all():
  +            raise RuntimeError("compute_two_stage_logp: cap_logits non-finite")
  +        # Group-wise logsumexp over (sample, card)
  +        group_ids = sample_idx_per_legal * 40 + played_ids_mb
  +        num_groups = B * 40
  +        group_max = torch.full((num_groups,), float('-inf'), dtype=cap_logits.dtype, device=device_local)
  +        group_max.scatter_reduce_(0, group_ids, cap_logits, reduce='amax', include_self=True)
  +        gmax_per_legal = group_max[group_ids]
  +        exp_shifted = torch.exp(cap_logits - gmax_per_legal).to(cap_logits.dtype)
  +        group_sum = torch.zeros((num_groups,), dtype=cap_logits.dtype, device=device_local)
  +        group_sum.index_add_(0, group_ids, exp_shifted)
  +        lse_per_legal = gmax_per_legal + torch.log(torch.clamp_min(group_sum[group_ids], 1e-12))
  +        logp_cap_per_legal = cap_logits - lse_per_legal
  +
  +        logp_total_per_legal = (logp_cards_allowed_per_legal + logp_cap_per_legal)
  +        if not torch.isfinite(logp_total_per_legal).all():
  +            raise RuntimeError("compute_two_stage_logp: logp_total_per_legal non-finite")
  +        return logp_total_per_legal
  +
  +
  +class CentralValueNet(torch.nn.Module):
  +    """
  +    Critico condizionato: usa StateEncoderCompact (256) + belief (120→64).
  +    """
--     def __init__(self, obs_dim=10823, state_encoder: StateEncoderCompact = None):
+++    def __init__(self, obs_dim: Optional[int] = None, state_encoder: StateEncoderCompact = None):
  +        super().__init__()
+++        if obs_dim is None:
+++            obs_dim = _get_compact_obs_dim(_default_k_history())
  +        self.state_enc = state_encoder if state_encoder is not None else StateEncoderCompact()
  +        self.belief_head = nn.Sequential(nn.Linear(120, 64), nn.ReLU())
  +        self.belief_net = BeliefNet(in_dim=256, hidden_dim=512)
  +        # Partner-aware belief features (come nell'actor)
  +        self.belief_card_emb = nn.Parameter(torch.randn(40, 32) * 0.02)
  +        self.partner_gate = nn.Sequential(nn.Linear(256, 32), nn.Sigmoid())
  +        self.opp_gate = nn.Sequential(nn.Linear(256, 32), nn.Sigmoid())
  +        # CTDE opzionale: usa others_hands (3x40) per modulare state_feat via FiLM
  +        self.ctde_cond = nn.Sequential(
  +            nn.Linear(120, 128), nn.ReLU(),
  +            nn.Linear(128, 128), nn.ReLU(),
  +        )
  +        self.ctde_scale = nn.Linear(128, 256)
  +        self.ctde_shift = nn.Linear(128, 256)
  +        # Stato 256 + belief 64 + partner 32 + opp 32 = 384
  +        self.head = nn.Sequential(
  +            nn.Linear(384, 256), nn.ReLU(),
  +            nn.Linear(256, 1)
  +        )
  +        self.to(device)
  +
  +    def forward(self, obs: torch.Tensor, seat_team_vec: torch.Tensor = None, others_hands: torch.Tensor = None) -> torch.Tensor:
  +        target_device = next(self.parameters()).device
  +        if torch.is_tensor(obs):
  +            if (obs.device == target_device) and (obs.dtype == torch.float32):
  +                x_obs = obs
  +            elif obs.device == target_device:
  +                x_obs = obs.to(dtype=torch.float32)
  +            else:
  +                x_obs = obs.to(device=target_device, dtype=torch.float32)
  +        else:
  +            x_obs = torch.as_tensor(obs, dtype=torch.float32, device=target_device)
  +        if x_obs.dim() == 1:
  +            x_obs = x_obs.unsqueeze(0)
  +        if seat_team_vec is None:
  +            seat_team_vec = torch.zeros((x_obs.size(0), 6), dtype=torch.float32, device=x_obs.device)
  +        else:
  +            seat_team_vec = (seat_team_vec if torch.is_tensor(seat_team_vec) else torch.as_tensor(seat_team_vec, dtype=torch.float32))
  +            if seat_team_vec.dim() == 1:
  +                seat_team_vec = seat_team_vec.unsqueeze(0)
  +            if (seat_team_vec.device != x_obs.device) or (seat_team_vec.dtype != torch.float32):
  +                seat_team_vec = seat_team_vec.to(x_obs.device, dtype=torch.float32)
  +        # Belief neurale interno (ignora belief_summary esterno)
  +        state_feat = self.state_enc(x_obs, seat_team_vec)
  +        bn_dtype = self.belief_net.fc_in.weight.dtype
  +        if state_feat.dtype != bn_dtype:
  +            state_feat = state_feat.to(dtype=bn_dtype)
  +        # CTDE FiLM gating se others_hands è disponibile (training centralizzato)
  +        if others_hands is not None:
  +            oh = others_hands
  +            if not torch.is_tensor(oh):
  +                oh = torch.as_tensor(oh, dtype=torch.float32, device=x_obs.device)
  +            else:
  +                oh = oh.to(x_obs.device, dtype=torch.float32)
  +            if oh.dim() == 2 and oh.size(1) == 120:
  +                oh_flat = oh
  +            elif oh.dim() == 3 and oh.size(1) == 3 and oh.size(2) == 40:
  +                oh_flat = oh.view(oh.size(0), -1)
  +            else:
  +                from utils.fallback import notify_fallback
  +                notify_fallback('models.critic.forward.others_hands_shape')
  +            cond = self.ctde_cond(oh_flat)
  +            scale = torch.sigmoid(self.ctde_scale(cond))  # (B,256) in (0,1)
  +            shift = torch.tanh(self.ctde_shift(cond)) * 0.1  # small bias
  +            state_feat = state_feat * (0.5 + scale) + shift
  +        hand_table = x_obs[:, :83]
  +        hand_mask = hand_table[:, :40] > 0.5
  +        table_mask = hand_table[:, 43:83] > 0.5
  +        captured = x_obs[:, 83:165]
  +        cap0_mask = captured[:, :40] > 0.5
  +        cap1_mask = captured[:, 40:80] > 0.5
  +        visible_mask = hand_mask | table_mask | cap0_mask | cap1_mask
  +        b_logits = self.belief_net(state_feat)
  +        b_logits = torch.nan_to_num(b_logits, nan=0.0, posinf=1e6, neginf=-1e6).clamp(-30.0, 30.0)
  +        b_probs_flat = self.belief_net.probs(b_logits, visible_mask)
  +        belief_feat = self.belief_head(b_probs_flat)
  +        # Partner/opponent channel split con gating
  +        partner_slice = b_probs_flat[:, 40:80]
  +        opps_slice = b_probs_flat[:, 0:40] + b_probs_flat[:, 80:120]
  +        emb = self.belief_card_emb
  +        partner_feat = torch.matmul(partner_slice, emb)
  +        opp_feat = torch.matmul(opps_slice, emb)
  +        pg = self.partner_gate(state_feat)
  +        og = self.opp_gate(state_feat)
  +        partner_feat = partner_feat * pg
  +        opp_feat = opp_feat * og
  +        B_head = state_feat.size(0)
  +        head_in = torch.empty((B_head, 256 + 64 + 32 + 32), dtype=state_feat.dtype, device=state_feat.device)
  +        hp = 0
  +        head_in[:, hp:hp+256] = state_feat; hp += 256
  +        head_in[:, hp:hp+64] = belief_feat; hp += 64
  +        head_in[:, hp:hp+32] = partner_feat; hp += 32
  +        head_in[:, hp:hp+32] = opp_feat; hp += 32
  +        out = self.head(head_in)
  +        return out.squeeze(-1)
  +
  +    def forward_from_state(self, state_feat: torch.Tensor, x_obs: torch.Tensor,
  +                            others_hands: torch.Tensor = None, visible_mask_40: torch.Tensor = None) -> torch.Tensor:
  +        """Valuta il valore partendo da feature di stato (256) già calcolate.
  +        Usa la stessa testa belief/gating del critico.
  +        """
  +        if x_obs.dim() == 1:
  +            x_obs = x_obs.unsqueeze(0)
  +        sf = state_feat
  +        # CTDE gating opzionale
  +        if others_hands is not None:
  +            oh = others_hands
  +            if not torch.is_tensor(oh):
  +                oh = torch.as_tensor(oh, dtype=torch.float32, device=x_obs.device)
  +            else:
  +                oh = oh.to(x_obs.device, dtype=torch.float32)
  +            if oh.dim() == 2 and oh.size(1) == 120:
  +                oh_flat = oh
  +            elif oh.dim() == 3 and oh.size(1) == 3 and oh.size(2) == 40:
  +                oh_flat = oh.view(oh.size(0), -1)
  +            else:
  +                notify_fallback('models.critic.forward_from_state.others_hands_shape')
  +            cond = self.ctde_cond(oh_flat)
  +            scale = torch.sigmoid(self.ctde_scale(cond))  # (B,256) in (0,1)
  +            shift = torch.tanh(self.ctde_shift(cond)) * 0.1
  +            sf = sf * (0.5 + scale) + shift
  +        # visible mask
  +        hand_table = x_obs[:, :83]
  +        hand_mask = hand_table[:, :40] > 0.5
  +        table_mask = hand_table[:, 43:83] > 0.5
  +        captured = x_obs[:, 83:165]
  +        cap0_mask = captured[:, :40] > 0.5
  +        cap1_mask = captured[:, 40:80] > 0.5
  +        visible_mask_local = hand_mask | table_mask | cap0_mask | cap1_mask
  +        visible_mask = (visible_mask_40 if visible_mask_40 is not None else visible_mask_local)
  +        b_logits = self.belief_net(sf)
  +        b_logits = torch.nan_to_num(b_logits, nan=0.0, posinf=1e6, neginf=-1e6).clamp(-30.0, 30.0)
  +        b_probs_flat = self.belief_net.probs(b_logits, visible_mask)
  +        b_probs_flat = torch.nan_to_num(b_probs_flat, nan=0.0, posinf=0.0, neginf=0.0)
  +        belief_feat = self.belief_head(b_probs_flat)
  +        partner_slice = b_probs_flat[:, 40:80]
  +        opps_slice = b_probs_flat[:, 0:40] + b_probs_flat[:, 80:120]
  +        emb = self.belief_card_emb
  +        partner_feat = torch.matmul(partner_slice, emb)
  +        opp_feat = torch.matmul(opps_slice, emb)
  +        pg = self.partner_gate(sf)
  +        og = self.opp_gate(sf)
  +        partner_feat = partner_feat * pg
  +        opp_feat = opp_feat * og
  +        head_in = torch.empty((sf.size(0), 256 + 64 + 32 + 32), dtype=sf.dtype, device=sf.device)
  +        hp = 0
  +        head_in[:, hp:hp+256] = sf; hp += 256
  +        head_in[:, hp:hp+64] = belief_feat; hp += 64
  +        head_in[:, hp:hp+32] = partner_feat; hp += 32
  +        head_in[:, hp:hp+32] = opp_feat; hp += 32
  +        out = self.head(head_in)
  +        return out.squeeze(-1)
  +
  +    def load_state_dict(self, state_dict, strict=True):  # type: ignore[override]
  +        _ = strict  # keep arg for external callers; force non-strict loading
  +        return super().load_state_dict(state_dict, strict=False)
-- 
diff --cc observation.py
index 14f5f2d,14f5f2d,0000000..edb114f
mode 100644,100644,000000..100644
--- a/observation.py
+++ b/observation.py
@@@@ -1,1053 -1,1053 -1,0 +1,1131 @@@@
  +# observation.py - Versione Torch CUDA (no NumPy)
  +import os
  +from typing import Optional
  +import torch
  +import torch.nn.functional as F
  +import torch._dynamo as _dynamo  # type: ignore
+++from belief.hierarchy import (
+++    compute_level1,
+++    compute_level2,
+++    compute_level3,
+++    compute_belief_hierarchy,
+++)
  +_dynamo_disable = _dynamo.disable  # type: ignore[attr-defined]
  +
  +SUITS = ['denari', 'coppe', 'spade', 'bastoni']
  +RANKS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  +
  +# Valori per la primiera
  +PRIMIERA_VAL = {1:16, 2:12, 3:13, 4:14, 5:15, 6:18, 7:21, 8:10, 9:10, 10:10}
  +
  +# Mappa condivisa per conversione suit → index
  +suit_to_col = {'denari': 0, 'coppe': 1, 'spade': 2, 'bastoni': 3}
  +
  +# ===== Device control (CPU default) =====
  +import os as _os
  +OBS_DEVICE = torch.device(_os.environ.get('OBS_DEVICE', _os.environ.get('SCOPONE_DEVICE', 'cpu')))
  +# Disabilita di default le feature probabilistiche per lasciare che la rete impari il belief
  +OBS_INCLUDE_INFERRED = os.environ.get('OBS_INCLUDE_INFERRED', '0') == '1'
+++OBS_INCLUDE_INFERRED_L2 = os.environ.get('OBS_INCLUDE_INFERRED_L2', '0') == '1'
+++OBS_INCLUDE_INFERRED_L3 = os.environ.get('OBS_INCLUDE_INFERRED_L3', '0') == '1'
  +OBS_INCLUDE_RANK_PROBS = os.environ.get('OBS_INCLUDE_RANK_PROBS', '0') == '1'
  +OBS_INCLUDE_SCOPA_PROBS = os.environ.get('OBS_INCLUDE_SCOPA_PROBS', '0') == '1'
  +OBS_INCLUDE_DEALER = os.environ.get('OBS_INCLUDE_DEALER', '1') == '1'
  +
+++
+++def get_compact_obs_dim(k_history: int) -> int:
+++    """Return the observation dimensionality for the compact encoder."""
+++    fixed = 43 + 40 + 82 + 61 * int(k_history) + 40 + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
+++    total = fixed
+++    if OBS_INCLUDE_INFERRED:
+++        total += 120
+++    if OBS_INCLUDE_INFERRED_L2:
+++        total += 120
+++    if OBS_INCLUDE_INFERRED_L3:
+++        total += 120
+++    if OBS_INCLUDE_SCOPA_PROBS:
+++        total += 10
+++    if OBS_INCLUDE_RANK_PROBS:
+++        total += 150
+++    if OBS_INCLUDE_DEALER:
+++        total += 4
+++    return total
+++
  +# ===== ID/Bitset helpers (device = OBS_DEVICE) =====
  +RANK_OF_ID = torch.tensor([i // 4 + 1 for i in range(40)], dtype=torch.int16, device=OBS_DEVICE)
  +RANK_OF_ID_F32 = RANK_OF_ID.to(torch.float32)
  +SUITCOL_OF_ID = torch.tensor([i % 4 for i in range(40)], dtype=torch.int16, device=OBS_DEVICE)
  +MASK_RANK = [(sum(1 << j for j in range(40) if (j // 4 + 1) == r)) for r in range(1, 11)]  # retained for potential bitset ops
  +PRIMIERA_VAL_T = torch.tensor([0, 16, 12, 13, 14, 15, 18, 21, 10, 10, 10], dtype=torch.float32, device=OBS_DEVICE)
  +IDS_CUDA = torch.arange(40, device=OBS_DEVICE, dtype=torch.int64)
  +IS_DENARI_MASK_40 = (SUITCOL_OF_ID.to(torch.long) == 0)
  +PRIMIERA_PER_ID = PRIMIERA_VAL_T[RANK_OF_ID.to(torch.long)]  # (40,)
  +# Precompute suits one-hot (4,40) for grouping by suit; rebuild on device change
  +SUITS_OH_4x40 = (torch.arange(4, device=OBS_DEVICE, dtype=torch.long).unsqueeze(1) == SUITCOL_OF_ID.to(torch.long).unsqueeze(0))
  +
  +# ===== Runtime flags / small-const caches (CPU-friendly) =====
  +STRICT_CHECKS = (os.environ.get('SCOPONE_STRICT_CHECKS', '0') == '1')
  +
  +_RANK_BINS_CACHE = {}
  +_IDX_1_10_CACHE = {}
  +_BIT_ONE_CACHE = {}
  +_MASK_BITS_CACHE = {}
  +_OTHER_PLAYER_IDX_CACHE = {}
  +_PLAYER_ARANGE_CACHE = {}
  +
  +def _get_rank_bins(device: torch.device) -> torch.Tensor:
  +    key = str(device)
  +    t = _RANK_BINS_CACHE.get(key)
  +    if t is None or t.device != device:
  +        t = torch.arange(1, 11, device=device, dtype=torch.int64).unsqueeze(1)  # (10,1)
  +        _RANK_BINS_CACHE[key] = t
  +    return t
  +
  +def _get_idx_1_10(device: torch.device) -> torch.Tensor:
  +    key = str(device)
  +    t = _IDX_1_10_CACHE.get(key)
  +    if t is None or t.device != device:
  +        t = torch.arange(1, 11, device=device, dtype=torch.int64)
  +        _IDX_1_10_CACHE[key] = t
  +    return t
  +
  +def _get_bit_one(device: torch.device) -> torch.Tensor:
  +    key = str(device)
  +    t = _BIT_ONE_CACHE.get(key)
  +    if t is None or t.device != device:
  +        t = torch.tensor(1, dtype=torch.int64, device=device)
  +        _BIT_ONE_CACHE[key] = t
  +    return t
  +
  +def _get_mask_bits(device: torch.device) -> torch.Tensor:
  +    key = str(device)
  +    t = _MASK_BITS_CACHE.get(key)
  +    if t is None or t.device != device:
  +        t = torch.tensor((1 << 11) - 1, dtype=torch.int64, device=device)
  +        _MASK_BITS_CACHE[key] = t
  +    return t
  +
  +
  +def _get_other_player_idx(device: torch.device) -> torch.Tensor:
  +    key = str(device)
  +    t = _OTHER_PLAYER_IDX_CACHE.get(key)
  +    if t is None or t.device != device:
  +        t = torch.tensor([
  +            [1, 2, 3],
  +            [0, 2, 3],
  +            [0, 1, 3],
  +            [0, 1, 2],
  +        ], dtype=torch.long, device=device)
  +        _OTHER_PLAYER_IDX_CACHE[key] = t
  +    return t
  +
  +
  +def _get_players_arange(device: torch.device) -> torch.Tensor:
  +    key = str(device)
  +    t = _PLAYER_ARANGE_CACHE.get(key)
  +    if t is None or t.device != device:
  +        t = torch.arange(4, device=device, dtype=torch.long)
  +        _PLAYER_ARANGE_CACHE[key] = t
  +    return t
  +
  +def set_obs_device(device: torch.device) -> None:
  +    """Rebuild observation constant tensors on the requested device.
  +    Call this when the environment/device changes to avoid CPU/CUDA mismatches.
  +    """
  +    global OBS_DEVICE, RANK_OF_ID, RANK_OF_ID_F32, SUITCOL_OF_ID, PRIMIERA_VAL_T, IDS_CUDA, IS_DENARI_MASK_40, PRIMIERA_PER_ID, ONE_HOT_PLAYERS, SUITS_OH_4x40
  +    if device == OBS_DEVICE:
  +        return
  +    OBS_DEVICE = torch.device(device)
  +    RANK_OF_ID = torch.tensor([i // 4 + 1 for i in range(40)], dtype=torch.int16, device=OBS_DEVICE)
  +    RANK_OF_ID_F32 = RANK_OF_ID.to(torch.float32)
  +    SUITCOL_OF_ID = torch.tensor([i % 4 for i in range(40)], dtype=torch.int16, device=OBS_DEVICE)
  +    PRIMIERA_VAL_T = torch.tensor([0, 16, 12, 13, 14, 15, 18, 21, 10, 10, 10], dtype=torch.float32, device=OBS_DEVICE)
  +    IDS_CUDA = torch.arange(40, device=OBS_DEVICE, dtype=torch.int64)
  +    IS_DENARI_MASK_40 = (SUITCOL_OF_ID.to(torch.long) == 0)
  +    PRIMIERA_PER_ID = PRIMIERA_VAL_T[RANK_OF_ID.to(torch.long)]
  +    SUITS_OH_4x40 = (torch.arange(4, device=OBS_DEVICE, dtype=torch.long).unsqueeze(1) == SUITCOL_OF_ID.to(torch.long).unsqueeze(0))
  +    ONE_HOT_PLAYERS = {
  +        0: torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32, device=OBS_DEVICE),
  +        1: torch.tensor([0.0, 1.0, 0.0, 0.0], dtype=torch.float32, device=OBS_DEVICE),
  +        2: torch.tensor([0.0, 0.0, 1.0, 0.0], dtype=torch.float32, device=OBS_DEVICE),
  +        3: torch.tensor([0.0, 0.0, 0.0, 1.0], dtype=torch.float32, device=OBS_DEVICE),
  +    }
  +
  +def bitset_popcount(x: int) -> int:
  +    return int(x.bit_count()) if hasattr(int, 'bit_count') else bin(x).count('1')
  +
  +def bitset_rank_counts(bits: int) -> torch.Tensor:
  +    # Rank counts via bit manip on OBS_DEVICE
  +    ids = torch.arange(40, device=OBS_DEVICE, dtype=torch.int64)
  +    bits_t = torch.tensor(int(bits), dtype=torch.int64, device=OBS_DEVICE)
  +    active = ((bits_t >> ids) & 1).to(torch.float32)
  +    ranks = RANK_OF_ID.to(torch.int64) - 1
  +    counts = torch.zeros(10, dtype=torch.float32, device=OBS_DEVICE)
  +    counts.index_add_(0, ranks, active)
  +    return counts.to(torch.int32)
  +
  +def bitset_table_sum(bits: int) -> int:
  +    # Sum of ranks via vectorization on OBS_DEVICE
  +    ids = torch.arange(40, device=OBS_DEVICE, dtype=torch.int64)
  +    bits_t = torch.tensor(int(bits), dtype=torch.int64, device=OBS_DEVICE)
  +    active = ((bits_t >> ids) & 1).to(torch.float32)
  +    ranks = RANK_OF_ID.to(torch.float32)
  +    return int((active * ranks).sum().detach().cpu().item())
  +
  +# ----- OTTIMIZZAZIONE: CACHE PER FUNZIONI COSTOSE -----
  +
  +def encode_cards_as_matrix(cards):
  +    """Codifica un insieme di carte come vettore 40-d su CUDA usando scatter, ID-only."""
  +    if not cards:
  +        return torch.zeros(40, dtype=torch.float32, device=OBS_DEVICE)
  +    if not isinstance(cards[0], int):
  +        raise TypeError("encode_cards_as_matrix expects card IDs (int)")
  +    idx = torch.as_tensor(cards, dtype=torch.long, device=OBS_DEVICE).clamp(0, 39)
  +    one_hot = F.one_hot(idx, num_classes=40).to(torch.float32)
  +    vec = one_hot.sum(dim=0)
  +    # Mantieni semantica originale: presenza (0/1), non conteggio
  +    vec = (vec > 0).to(torch.float32)
  +    return vec
  + 
  +
  +# One-hot encoding per player (pre-calcolato per velocità)
  +ONE_HOT_PLAYERS = {
  +    0: torch.tensor([1.0, 0.0, 0.0, 0.0], dtype=torch.float32, device=OBS_DEVICE),
  +    1: torch.tensor([0.0, 1.0, 0.0, 0.0], dtype=torch.float32, device=OBS_DEVICE),
  +    2: torch.tensor([0.0, 0.0, 1.0, 0.0], dtype=torch.float32, device=OBS_DEVICE),
  +    3: torch.tensor([0.0, 0.0, 0.0, 1.0], dtype=torch.float32, device=OBS_DEVICE)
  +}
  +
  +def encode_current_player(cp):
  +    """
  +    Versione ottimizzata con risultati identici.
  +    Usa array pre-calcolati invece di crearli ogni volta.
  +    """
  +    return ONE_HOT_PLAYERS[cp].clone()  # Copia per evitare modifiche esterne
  +
  +def encode_move(move):
  +    """
  +    Codifica una mossa in un vettore 61-d totalmente tensoriale e compatibile con torch.compile.
  +    Layout: [player(4), rank(10), suit(4), capture_type(3), captured_cards(40)].
  +    """
  +    # Player one-hot (4)
  +    player_idx_py = int(move.get("player", 0))
  +    player_idx_py = 0 if player_idx_py < 0 else 3 if player_idx_py > 3 else player_idx_py
  +    # Use precomputed EYE_4 via ONE_HOT_PLAYERS to avoid one_hot kernel
  +    player_vec = ONE_HOT_PLAYERS[player_idx_py]
  +
  +    # Carta giocata → rank (10) e suit (4)
  +    played = move.get("played_card", None)
  +    if isinstance(played, int):
  +        pid_t = torch.as_tensor(int(played), dtype=torch.long, device=OBS_DEVICE).clamp_(0, 39)
  +        rank_idx_t = (RANK_OF_ID[pid_t].to(torch.long) - 1).clamp_(0, 9)
  +        suit_idx_t = SUITCOL_OF_ID[pid_t].to(torch.long).clamp_(0, 3)
  +    else:
  +        if played is None:
  +            rank_idx_t = torch.zeros((), dtype=torch.long, device=OBS_DEVICE)
  +            suit_idx_t = torch.zeros((), dtype=torch.long, device=OBS_DEVICE)
  +        else:
  +            r_py, s_py = played
  +            r_py = int(r_py)
  +            rank_idx_t = torch.as_tensor(0 if r_py < 1 else 9 if r_py > 10 else (r_py - 1), dtype=torch.long, device=OBS_DEVICE)
  +            suit_idx_t = torch.as_tensor(int(suit_to_col.get(s_py, 0)), dtype=torch.long, device=OBS_DEVICE)
  +    # Replace one_hot with equality against small ranges to avoid launching kernels
  +    # Cached EYE tensors to avoid per-call arange
  +    global _EYE10, _EYE4, _EYE3
  +    if '_EYE10' not in globals() or _EYE10.device != OBS_DEVICE:
  +        _EYE10 = torch.eye(10, dtype=torch.float32, device=OBS_DEVICE)
  +        _EYE4 = torch.eye(4, dtype=torch.float32, device=OBS_DEVICE)
  +        _EYE3 = torch.eye(3, dtype=torch.float32, device=OBS_DEVICE)
  +    rank_vec = _EYE10[rank_idx_t.clamp_(0, 9)]
  +    suit_vec = _EYE4[suit_idx_t.clamp_(0, 3)]
  +
  +    # Capture type (3)
  +    capture_map = {"no_capture": 0, "capture": 1, "scopa": 2}
  +    ctype_idx_py = int(capture_map.get(move.get("capture_type"), 0))
  +    ctype_idx_t = torch.as_tensor(ctype_idx_py, dtype=torch.long, device=OBS_DEVICE)
  +    capture_vec = _EYE3[ctype_idx_t.clamp_(0, 2)]
  +
  +    # Carte catturate (40) – presenza 0/1
  +    captured_cards = move.get("captured_cards") or []
  +    if len(captured_cards) == 0:
  +        captured_vec = torch.zeros(40, dtype=torch.float32, device=OBS_DEVICE)
  +    else:
  +        if isinstance(captured_cards[0], int):
  +            idx = torch.as_tensor(captured_cards, dtype=torch.long, device=OBS_DEVICE)
  +        else:
  +            ids_py = [int((int(r) - 1) * 4 + suit_to_col[s]) for (r, s) in captured_cards]
  +            idx = torch.as_tensor(ids_py, dtype=torch.long, device=OBS_DEVICE)
  +        idx = idx.clamp(0, 39)
  +        one_hot = F.one_hot(idx, num_classes=40).to(torch.float32)
  +        captured_vec = (one_hot.sum(dim=0) > 0).to(torch.float32)
  +
  +    return torch.cat([player_vec, rank_vec, suit_vec, capture_vec, captured_vec.reshape(-1)], dim=0)
  +
  +
  +# Cache condivisa per matrice di carte mancanti
  +missing_cards_cache = {}
  +
  +def compute_missing_cards_matrix(game_state, player_id):
  +    """
  +    Versione ottimizzata con caching ma risultati identici.
  +    """
  +    # Costruisci chiave cache efficiente
  +    # Supporta ID o tuple
  +    player_hand = tuple(sorted(game_state["hands"][player_id]))
  +    table = tuple(sorted(game_state["table"]))
  +    team0_cards = tuple(sorted(game_state["captured_squads"][0]))
  +    team1_cards = tuple(sorted(game_state["captured_squads"][1]))
  +    
  +    cache_key = (player_id, player_hand, table, team0_cards, team1_cards)
  +    
  +    # Controlla cache
  +    if cache_key in missing_cards_cache:
  +        return missing_cards_cache[cache_key].clone()
  +    
  +    # GPU mask-based computation
  +    vis = torch.zeros(40, dtype=torch.bool, device=OBS_DEVICE)
  +    for lst in (game_state["hands"][player_id], game_state["table"], game_state["captured_squads"][0], game_state["captured_squads"][1]):
  +        if lst:
  +            idx = torch.as_tensor(lst, dtype=torch.long, device=OBS_DEVICE)
  +            vis[idx] = True
  +    missing = (~vis).to(torch.float32)
  +    result = missing
  +    
  +    # Salva in cache e ritorna tensore
  +    missing_cards_cache[cache_key] = result.clone()
  +    
  +    # Gestisci dimensione cache
  +    if len(missing_cards_cache) > 100:
  +        import random
  +        keys_to_remove = random.sample(list(missing_cards_cache.keys()), 50)
  +        for k in keys_to_remove:
  +            del missing_cards_cache[k]
  +    
  +    return result
  +
  +# Cache per probabilità inferite
  +inferred_probs_cache = {}
  +
+++
+++def _card_to_id_obs(card) -> int:
+++    if isinstance(card, int):
+++        return int(card)
+++    rank, suit = card
+++    return int((int(rank) - 1) * 4 + suit_to_col[str(suit)])
+++
+++
+++def _played_bits(game_state, player_id: int) -> int:
+++    played = game_state.get('_played_bits_by_player_t')
+++    if torch.is_tensor(played):
+++        return int(played[player_id].item())
+++    if isinstance(played, dict) and player_id in played:
+++        val = played[player_id]
+++        if torch.is_tensor(val):
+++            return int(val.item())
+++        return int(val)
+++    return 0
+++
+++
+++def _hierarchy_cache_key(game_state, player_id: int):
+++    hands = game_state.get('hands', {})
+++    hand_self = tuple(sorted(_card_to_id_obs(c) for c in hands.get(player_id, [])))
+++    table = tuple(sorted(_card_to_id_obs(c) for c in game_state.get('table', [])))
+++    captured = game_state.get('captured_squads', {})
+++    if isinstance(captured, dict):
+++        team0 = tuple(sorted(_card_to_id_obs(c) for c in captured.get(0, [])))
+++        team1 = tuple(sorted(_card_to_id_obs(c) for c in captured.get(1, [])))
+++    else:
+++        team0 = tuple()
+++        team1 = tuple()
+++    hand_sizes = tuple(len(hands.get(p, [])) for p in range(4))
+++    played_bits = tuple(int(_played_bits(game_state, p)) for p in range(4))
+++    return (player_id, hand_self, table, team0, team1, hand_sizes, played_bits)
+++
+++
+++def _get_hierarchy_cached(game_state, player_id: int):
+++    key = _hierarchy_cache_key(game_state, player_id)
+++    cached = inferred_probs_cache.get(key)
+++    if cached is None:
+++        hier = compute_belief_hierarchy(game_state, player_id)
+++        cached = {
+++            'level1': hier['level1'].to(device=OBS_DEVICE, dtype=torch.float32).clone(),
+++            'level2': hier['level2'].to(device=OBS_DEVICE, dtype=torch.float32).clone(),
+++            'level3': hier['level3'].to(device=OBS_DEVICE, dtype=torch.float32).clone(),
+++        }
+++        inferred_probs_cache[key] = cached
+++        if len(inferred_probs_cache) > 128:
+++            import random
+++            for ck in random.sample(list(inferred_probs_cache.keys()), 64):
+++                if ck != key:
+++                    inferred_probs_cache.pop(ck, None)
+++    return cached
+++
+++
  +def compute_inferred_probabilities(game_state, player_id):
--     """
--     Versione mirror-only e compile-friendly: richiede i mirror tensoriali
--     (_hands_bits_t, _table_bits_t, _captured_bits_t, _played_bits_by_player_t).
--     Output: (3*40,) flatten per i tre avversari (10x4 per ciascuno).
--     """
--     hands_bits_t = game_state.get('_hands_bits_t', None)
--     table_bits_t = game_state.get('_table_bits_t', None)
--     captured_bits_t = game_state.get('_captured_bits_t', None)
--     played_bits_by_player_t = game_state.get('_played_bits_by_player_t', None)
--     torch._assert(torch.is_tensor(hands_bits_t), "_hands_bits_t required")
--     torch._assert(torch.is_tensor(table_bits_t), "_table_bits_t required")
--     torch._assert(torch.is_tensor(captured_bits_t), "_captured_bits_t required")
--     torch._assert(torch.is_tensor(played_bits_by_player_t), "_played_bits_by_player_t required")
-- 
--     visible_bits = hands_bits_t[player_id] | table_bits_t | captured_bits_t[0] | captured_bits_t[1]
--     vis = (((visible_bits >> IDS_CUDA) & 1).to(torch.bool))
--     invisible = ~vis
--     total_unknown = invisible.to(torch.float32).sum().clamp(min=1.0)
-- 
--     probs = []
--     other_players = [p for p in range(4) if p != player_id]
--     for p in other_players:
--         hand_size = (((hands_bits_t[p] >> IDS_CUDA) & 1).to(torch.float32).sum())
--         pm = torch.zeros((10, 4), dtype=torch.float32, device=OBS_DEVICE)
--         played_mask = (((played_bits_by_player_t[p] >> IDS_CUDA) & 1).to(torch.bool))
--         possible_mask = invisible & (~played_mask)
--         idx = torch.nonzero(possible_mask, as_tuple=False).flatten()
--         if idx.numel() > 0:
--             rows = (RANK_OF_ID[idx].to(torch.long) - 1).clamp(0, 9)
--             cols = SUITCOL_OF_ID[idx].to(torch.long).clamp(0, 3)
--             pm[rows, cols] = (hand_size / total_unknown)
--         probs.append(pm.reshape(-1))
--     return torch.cat(probs)
+++    """Return level-1 belief probabilities flattened as (120,)."""
+++    cached = _get_hierarchy_cached(game_state, player_id)
+++    return cached['level1'].reshape(-1).clone()
+++
+++
+++def compute_inferred_probabilities_level2(game_state, player_id):
+++    cached = _get_hierarchy_cached(game_state, player_id)
+++    return cached['level2'].reshape(-1).clone()
+++
+++
+++def compute_inferred_probabilities_level3(game_state, player_id):
+++    cached = _get_hierarchy_cached(game_state, player_id)
+++    return cached['level3'].reshape(-1).clone()
  +
  +# Cache per primiera
  +primiera_cache = {}
  +
  +def compute_primiera_status(game_state):
  +    """
  +    Versione ottimizzata con caching ma risultati identici.
  +    """
  +    # Chiave cache semplice ma efficace
  +    team0_cards = tuple(sorted(game_state["captured_squads"][0]))
  +    team1_cards = tuple(sorted(game_state["captured_squads"][1]))
  +    cache_key = (team0_cards, team1_cards)
  +    
  +    # Controlla cache
  +    if cache_key in primiera_cache:
  +        return primiera_cache[cache_key].clone()
  +    
  +    # Calcolo GPU vectorized
  +    def _max_per_suit(ids_list):
  +        if not ids_list:
  +            return torch.zeros(4, dtype=torch.float32, device=OBS_DEVICE)
  +        # Accept tuple cards; convert to IDs if needed
  +        if isinstance(ids_list[0], int):
  +            ids = torch.as_tensor(ids_list, dtype=torch.long, device=OBS_DEVICE)
  +        else:
  +            suit_to_idx = {'denari': 0, 'coppe': 1, 'spade': 2, 'bastoni': 3}
  +            ids_py = [int((r - 1) * 4 + suit_to_idx[s]) for (r, s) in ids_list]
  +            ids = torch.as_tensor(ids_py, dtype=torch.long, device=OBS_DEVICE)
  +        ranks = RANK_OF_ID[ids].to(torch.long)
  +        suits = SUITCOL_OF_ID[ids].to(torch.long)
  +        vals = PRIMIERA_VAL_T[ranks]
  +        out = torch.zeros(4, dtype=torch.float32, device=OBS_DEVICE)
  +        # scatter_reduce for max if available
  +        out.scatter_reduce_(0, suits, vals, reduce='amax', include_self=True)
  +        return out / 21.0
  +    team0_primiera = _max_per_suit(game_state["captured_squads"][0])
  +    team1_primiera = _max_per_suit(game_state["captured_squads"][1])
  +    result = torch.cat([team0_primiera, team1_primiera]).to(dtype=torch.float32)
  +    
  +    # Salva in cache
  +    primiera_cache[cache_key] = result.clone()
  +    
  +    # Gestisci dimensione cache
  +    if len(primiera_cache) > 100:
  +        import random
  +        keys_to_remove = random.sample(list(primiera_cache.keys()), 50)
  +        for k in keys_to_remove:
  +            del primiera_cache[k]
  +    
  +    return result
  +
  +# Cache per denari count
  +denari_cache = {}
  +
  +def compute_denari_count(game_state):
  +    """
  +    Versione ottimizzata con caching ma risultati identici.
  +    """
  +    # Chiave cache
  +    team0_cards = tuple(sorted(game_state["captured_squads"][0]))
  +    team1_cards = tuple(sorted(game_state["captured_squads"][1]))
  +    cache_key = (team0_cards, team1_cards)
  +    
  +    # Controlla cache
  +    if cache_key in denari_cache:
  +        return denari_cache[cache_key].clone()
  +    
  +    cs0 = torch.as_tensor(game_state["captured_squads"][0] or [], dtype=torch.long, device=OBS_DEVICE)
  +    cs1 = torch.as_tensor(game_state["captured_squads"][1] or [], dtype=torch.long, device=OBS_DEVICE)
  +    den0 = (cs0.numel() > 0) and int((SUITCOL_OF_ID[cs0] == 0).sum().detach().cpu().item()) or 0
  +    den1 = (cs1.numel() > 0) and int((SUITCOL_OF_ID[cs1] == 0).sum().detach().cpu().item()) or 0
  +    result = torch.tensor([den0 / 10.0, den1 / 10.0], dtype=torch.float32, device=OBS_DEVICE)
  +    
  +    # Salva in cache
  +    denari_cache[cache_key] = result.clone()
  +    
  +    # Gestisci dimensione cache
  +    if len(denari_cache) > 100:
  +        import random
  +        keys_to_remove = random.sample(list(denari_cache.keys()), 50)
  +        for k in keys_to_remove:
  +            del denari_cache[k]
  +    
  +    return result
  +
  +# Cache per settebello
  +settebello_cache = {}
  +
  +def compute_settebello_status(game_state):
  +    """
  +    Versione ottimizzata con caching ma risultati identici.
  +    """
  +    # Chiave cache
  +    # Lavora su ID o tuple con fast-path ID
  +    SETTEBELLO_ID = 24  # (7-1)*4 + denari(0)
  +    def _has_settebello(seq):
  +        return SETTEBELLO_ID in seq
  +    team0_has_settebello = _has_settebello(game_state["captured_squads"][0])
  +    team1_has_settebello = _has_settebello(game_state["captured_squads"][1])
  +    table_has_settebello = _has_settebello(game_state["table"])
  +    cache_key = (team0_has_settebello, team1_has_settebello, table_has_settebello)
  +    
  +    # Controlla cache
  +    if cache_key in settebello_cache:
  +        return settebello_cache[cache_key].clone()
  +    
  +    # Calcolo originale
  +    if team0_has_settebello:
  +        status = 1
  +    elif team1_has_settebello:
  +        status = 2
  +    elif table_has_settebello:
  +        status = 3
  +    else:
  +        status = 0
  +    
  +    # Normalizza
  +    result = torch.tensor([status / 3.0], dtype=torch.float32, device=OBS_DEVICE)
  +    
  +    # Salva in cache
  +    settebello_cache[cache_key] = result.clone()
  +    
  +    return result
  +
  +# Cache per score estimate
  +score_cache = {}
  +
  +def compute_current_score_estimate(game_state):
  +    """
  +    Versione interamente tensoriale (senza .item()/detach) per compatibilità con torch.compile.
  +    Restituisce un tensor [score_team0, score_team1] normalizzato in [0,1].
  +    """
  +    device = OBS_DEVICE
  +
  +    # Sorgenti: richiede mirror bitset delle carte catturate
  +    cap_bits = game_state.get('_captured_bits_t', None)
  +    torch._assert(torch.is_tensor(cap_bits) and cap_bits.numel() >= 2, "compute_current_score_estimate requires _captured_bits_t")
  +    cap0_mask = (((cap_bits[0] >> IDS_CUDA) & 1).to(torch.bool))
  +    cap1_mask = (((cap_bits[1] >> IDS_CUDA) & 1).to(torch.bool))
  +
  +    # 1) Carte totali (punto a chi ha più carte)
  +    c0 = cap0_mask.to(torch.float32).sum()
  +    c1 = cap1_mask.to(torch.float32).sum()
  +    pt_c0 = (c0 > c1).to(torch.float32)
  +    pt_c1 = (c1 > c0).to(torch.float32)
  +
  +    # 2) Denari (punto a chi ne ha di più)
  +    den_mask = IS_DENARI_MASK_40.to(device=device)
  +    d0 = (cap0_mask & den_mask).to(torch.float32).sum()
  +    d1 = (cap1_mask & den_mask).to(torch.float32).sum()
  +    pt_d0 = (d0 > d1).to(torch.float32)
  +    pt_d1 = (d1 > d0).to(torch.float32)
  +
  +    # 3) Settebello (ID 24)
  +    sette_id = 24
  +    sb0 = cap0_mask[sette_id].to(torch.float32)
  +    sb1 = cap1_mask[sette_id].to(torch.float32)
  +
  +    # 4) Primiera (confronta la somma dei migliori per seme; lo scaling per 21 è costante e si annulla nel confronto)
  +    vals = PRIMIERA_PER_ID.to(device=device, dtype=torch.float32)  # (40)
  +    suits = SUITCOL_OF_ID.to(device=device, dtype=torch.long)      # (40)
  +    suits_oh = (suits.unsqueeze(0) == torch.arange(4, device=device, dtype=torch.long).unsqueeze(1))  # (4,40)
  +    def _primiera_sum(mask_bool):
  +        present = mask_bool.to(torch.bool)
  +        masked_vals = vals.unsqueeze(0) * (suits_oh & present.unsqueeze(0)).to(vals.dtype)
  +        best_per_suit = masked_vals.max(dim=1).values  # (4)
  +        return best_per_suit.sum()  # senza *21, confronto invariato
  +    p0 = _primiera_sum(cap0_mask)
  +    p1 = _primiera_sum(cap1_mask)
  +    pt_p0 = (p0 > p1).to(torch.float32)
  +    pt_p1 = (p1 > p0).to(torch.float32)
  +
  +    # 5) Scopa counts: richiede mirror tensoriale
  +    scp = game_state.get('_scopa_counts_t', None)
  +    torch._assert(torch.is_tensor(scp) and scp.numel() >= 2, "compute_current_score_estimate requires _scopa_counts_t")
  +    scope0_t = scp[0]
  +    scope1_t = scp[1]
  +
  +    # Totale e normalizzazione
  +    total0 = pt_c0 + pt_d0 + sb0 + pt_p0 + scope0_t
  +    total1 = pt_c1 + pt_d1 + sb1 + pt_p1 + scope1_t
  +    result = torch.stack([total0, total1]).to(torch.float32) / 12.0
  +    return result
  +
  +# Cache per table sum
  +table_sum_cache = {}
  +
  +def compute_table_sum(game_state):
  +    """
  +    Versione ottimizzata con caching ma risultati identici.
  +    """
  +    # Chiave cache che funziona sia per ID che per tuple
  +    tbl = game_state["table"]
  +    table_key = tuple(sorted(tbl))
  +    if table_key in table_sum_cache:
  +        return table_sum_cache[table_key].clone()
  +    if len(tbl) > 0:
  +        if isinstance(tbl[0], int):
  +            ids = torch.as_tensor(tbl, dtype=torch.long, device=OBS_DEVICE)
  +        else:
  +            suit_to_idx = {'denari': 0, 'coppe': 1, 'spade': 2, 'bastoni': 3}
  +            ids_py = [int((r - 1) * 4 + suit_to_idx[s]) for (r, s) in tbl]
  +            ids = torch.as_tensor(ids_py, dtype=torch.long, device=OBS_DEVICE)
  +        table_sum = int(RANK_OF_ID[ids].to(torch.int64).sum().detach().cpu().item())
  +    else:
  +        table_sum = 0
  +    result = torch.tensor([table_sum / 30.0], dtype=torch.float32, device=OBS_DEVICE)
  +    table_sum_cache[table_key] = result.clone()
  +    
  +    # Gestisci dimensione cache
  +    if len(table_sum_cache) > 100:
  +        import random
  +        keys_to_remove = random.sample(list(table_sum_cache.keys()), 50)
  +        for k in keys_to_remove:
  +            del table_sum_cache[k]
  +    
  +    return result
  +
  +# Cache per somme possibili sul tavolo (subset-sum 1..10)
  +table_possible_sums_cache = {}
  +
  +def compute_table_possible_sums(game_state):
  +    """
  +    Restituisce un vettore 10-d (rank 1..10) con 1.0 se esiste un sottoinsieme
  +    delle carte sul tavolo la cui somma dei rank è uguale a quel valore.
  +    Risultato normalizzato in [0,1] (booleana) e cache-ato per tavolo.
  +    """
  +    tbl = game_state["table"]
  +    table_key = tuple(sorted(tbl))
  +    cached = table_possible_sums_cache.get(table_key)
  +    if cached is not None:
  +        return cached.clone()
  +    if len(tbl) == 0:
  +        vec = torch.zeros(10, dtype=torch.float32, device=OBS_DEVICE)
  +        table_possible_sums_cache[table_key] = vec.clone()
  +        return vec
  +    # Converte in ID
  +    if isinstance(tbl[0], int):
  +        ids = torch.as_tensor(tbl, dtype=torch.long, device=OBS_DEVICE)
  +    else:
  +        suit_to_idx = {'denari': 0, 'coppe': 1, 'spade': 2, 'bastoni': 3}
  +        ids_py = [int((r - 1) * 4 + suit_to_idx[s]) for (r, s) in tbl]
  +        ids = torch.as_tensor(ids_py, dtype=torch.long, device=OBS_DEVICE)
  +    ranks = RANK_OF_ID[ids].to(torch.long)
  +    n = int(ranks.numel())
  +    possible = torch.zeros(10, dtype=torch.float32, device=OBS_DEVICE)
  +    if n == 0:
  +        table_possible_sums_cache[table_key] = possible.clone()
  +        return possible
  +    # Genera tutte le somme possibili via bitmask (n <= 10 tipicamente)
  +    pos = torch.arange(n, device=OBS_DEVICE, dtype=torch.long)
  +    masks = torch.arange(1, 1 << n, device=OBS_DEVICE, dtype=torch.long)
  +    sel = ((masks.unsqueeze(1) >> pos) & 1).to(torch.long)
  +    sums = (sel * ranks.unsqueeze(0)).sum(dim=1)
  +    # Booleana per tutti i rank 1..10 in un colpo solo
  +    ranks_10 = torch.arange(1, 11, device=OBS_DEVICE, dtype=sums.dtype)  # (10)
  +    # Confronto broadcasting: (M,1) vs (1,10) -> (M,10)
  +    any_per_rank = (sums.unsqueeze(1) == ranks_10.unsqueeze(0)).any(dim=0)
  +    possible = any_per_rank.to(torch.float32)
  +    table_possible_sums_cache[table_key] = possible.clone()
  +    # Limita cache
  +    if len(table_possible_sums_cache) > 100:
  +        import random
  +        for ck in random.sample(list(table_possible_sums_cache.keys()), 50):
  +            del table_possible_sums_cache[ck]
  +    return possible
  +
  +def compute_scopa_counts(game_state):
  +    """
  +    Ritorna un vettore (2,) con il numero di scope per team [team0, team1],
  +    normalizzato dividendo per 10.0.
  +    """
  +    scp = game_state.get('_scopa_counts_t', None)
  +    torch._assert(torch.is_tensor(scp) and scp.numel() >= 2, "compute_scopa_counts requires _scopa_counts_t")
  +    return (scp / 10.0).to(torch.float32)
  +
  +def compute_rank_presence_probs_from_inferred(game_state, player_id):
  +    """
  +    Condensa le inferred probs (3 x 40 carte per altri giocatori) in probabilità per-rank (1..10)
  +    per ciascuno dei 3 giocatori avversari, sommando le probabilità sulle 4 carte del rank.
  +    Output: (30,) flatten: [opp0(10), opp1(10), opp2(10)]. Se OBS_INCLUDE_INFERRED=0,
  +    restituisce zeri.
  +    """
  +    if not OBS_INCLUDE_INFERRED:
  +        return torch.zeros(30, dtype=torch.float32, device=OBS_DEVICE)
  +    probs_3x40 = compute_inferred_probabilities(game_state, player_id)
  +    if torch.is_tensor(probs_3x40):
  +        x = probs_3x40.view(3, 10, 4).sum(dim=2)  # (3,10)
  +        return x.reshape(-1)
  +    else:
  +        t = torch.as_tensor(probs_3x40, dtype=torch.float32, device=OBS_DEVICE).view(3, 10, 4)
  +        return t.sum(dim=2).reshape(-1)
  +
  +# Cache per scopa probabilities
  +scopa_probs_cache = {}
  +
  +def compute_next_player_scopa_probabilities(game_state, player_id, rank_probabilities=None):
  +    """
  +    Versione ottimizzata con caching ma risultati identici.
  +    """
  +    # Chiave cache che cattura lo stato rilevante
  +    player_hand = tuple(sorted(game_state["hands"][player_id]))
  +    table = tuple(sorted(game_state["table"]))
  +    next_player = (player_id + 1) % 4
  +    next_hand_size = len(game_state["hands"].get(next_player, []))
  +    
  +    cache_key = (player_id, next_player, player_hand, table, next_hand_size)
  +    
  +    # Controlla cache
  +    if cache_key in scopa_probs_cache:
  +        return scopa_probs_cache[cache_key].clone()
  +    
  +    # CPU-based computation
  +    next_player_idx = [i for i, p in enumerate([p for p in range(4) if p != player_id]) if p == next_player][0]
  +    scopa_probs = torch.zeros(10, dtype=torch.float32, device=OBS_DEVICE)
  +    if rank_probabilities is None:
  +        rank_probabilities = compute_rank_probabilities_by_player(game_state, player_id)
  +    hand_size = len(game_state["hands"].get(next_player, []))
  +    if hand_size == 0:
  +        return scopa_probs
  +    table = game_state["table"]
  +    if table and not isinstance(table[0], int):
  +        suit_to_idx = {'denari': 0, 'coppe': 1, 'spade': 2, 'bastoni': 3}
  +        table = [int((r - 1) * 4 + suit_to_idx[s]) for (r, s) in table]
  +    table_ids = torch.as_tensor(table or [], dtype=torch.long, device=OBS_DEVICE)
  +    table_ranks = RANK_OF_ID[table_ids].to(torch.long) if table_ids.numel() > 0 else torch.empty(0, dtype=torch.long, device=OBS_DEVICE)
  +    # For each current_rank 1..10
  +    for current_rank in range(1, 11):
  +        # Direct capture removes all of that rank
  +        if table_ranks.numel() > 0:
  +            if (table_ranks == current_rank).any():
  +                remaining = table_ranks[table_ranks != current_rank]
  +            else:
  +                n = int(table_ranks.numel())
  +                if n > 0:
  +                    pos = torch.arange(n, device=OBS_DEVICE, dtype=torch.long)
  +                    masks = torch.arange(1, 1 << n, device=OBS_DEVICE, dtype=torch.long)
  +                    sel = ((masks.unsqueeze(1) >> pos) & 1).to(torch.long)
  +                    sums = (sel * table_ranks.unsqueeze(0)).sum(dim=1)
  +                    good = (sums == current_rank)
  +                    if bool(good.any().detach().cpu().item()):
  +                        # if a subset sums, simulate remove that subset
  +                        gi = int(torch.nonzero(good, as_tuple=False)[0].detach().cpu().item())
  +                        remaining = table_ranks[((masks[gi].unsqueeze(0) >> pos) & 1) == 0]
  +                    else:
  +                        remaining = torch.cat([table_ranks, torch.tensor([current_rank], device=OBS_DEVICE)])
  +                else:
  +                    remaining = table_ranks
  +        else:
  +            remaining = table_ranks
  +        if remaining.numel() == 0:
  +            p_has = (1.0 - rank_probabilities[next_player_idx, 0, :].sum()).clamp_(0.0, 1.0)
  +            scopa_probs[current_rank-1] = p_has
  +            continue
  +        total_sum = int(remaining.sum().detach().cpu().item()) if remaining.numel() > 0 else 0
  +        for next_rank in range(1, 11):
  +            can_capture_all = False
  +            if remaining.numel() > 0:
  +                if bool((remaining == next_rank).all().detach().cpu().item()):
  +                    can_capture_all = True
  +                elif total_sum == next_rank:
  +                    can_capture_all = True
  +            if can_capture_all:
  +                p_zero = rank_probabilities[next_player_idx, 0, next_rank-1]
  +                scopa_probs[current_rank-1] += (1.0 - p_zero)
  +        scopa_probs[current_rank-1] = min(1.0, float(scopa_probs[current_rank-1].detach().cpu().item()))
  +    
  +    # Salva in cache
  +    scopa_probs_cache[cache_key] = scopa_probs.clone()
  +    
  +    # Gestisci dimensione cache
  +    if len(scopa_probs_cache) > 100:
  +        import random
  +        keys_to_remove = random.sample(list(scopa_probs_cache.keys()), 50)
  +        for k in keys_to_remove:
  +            del scopa_probs_cache[k]
  +    
  +    return scopa_probs
  +
  +# Cache per rank probabilities
  +rank_prob_cache = {}
  +
  +def compute_rank_probabilities_by_player(game_state, player_id):
  +    """
  +    Versione ottimizzata con caching ma risultati identici.
  +    """
  +    # Costruisci chiave cache
  +    player_hand = tuple(sorted(game_state["hands"][player_id]))
  +    table = tuple(sorted(game_state["table"]))
  +    team0_cards = tuple(sorted(game_state["captured_squads"][0]))
  +    team1_cards = tuple(sorted(game_state["captured_squads"][1]))
  +    
  +    # Includi dimensioni mani degli altri giocatori
  +    other_hands = tuple((p, len(game_state["hands"].get(p, []))) 
  +                       for p in range(4) if p != player_id)
  +    
  +    # Rappresentazione compatta della history (ID-safe)
  +    def _rank_of_played(pc):
  +        if isinstance(pc, int):
  +            return (pc // 4) + 1
  +        return pc[0]
  +    history_summary = tuple((m["player"], _rank_of_played(m.get("played_card"))) for m in game_state.get("history", []))
  +    
  +    cache_key = (player_id, player_hand, table, team0_cards, team1_cards, 
  +                other_hands, hash(history_summary))
  +    
  +    # Controlla cache
  +    if cache_key in rank_prob_cache:
  +        return rank_prob_cache[cache_key].clone()
  +    
  +    # GPU-based computation
  +    all_probs = torch.zeros((3, 5, 10), dtype=torch.float32, device=OBS_DEVICE)
  +    other_players = [p for p in range(4) if p != player_id]
  +    # Visible rank counts (use bitsets if available)
  +    hands_bits_t = game_state.get('_hands_bits_t', None)
  +    table_bits_t = game_state.get('_table_bits_t', None)
  +    captured_bits_t = game_state.get('_captured_bits_t', None)
  +    visible_rank_counts = torch.zeros(10, dtype=torch.float32, device=OBS_DEVICE)
  +    if hands_bits_t is not None and table_bits_t is not None and captured_bits_t is not None:
  +        visible_bits = hands_bits_t[player_id] | table_bits_t | captured_bits_t[0] | captured_bits_t[1]
  +        mask = (((visible_bits >> IDS_CUDA) & 1).to(torch.bool))
  +        idx = torch.nonzero(mask, as_tuple=False).flatten()
  +        if idx.numel() > 0:
  +            ranks = (RANK_OF_ID[idx].to(torch.long) - 1).clamp_(0, 9)
  +            add = torch.zeros(10, dtype=torch.float32, device=OBS_DEVICE)
  +            add.index_add_(0, ranks, torch.ones_like(ranks, dtype=torch.float32))
  +            visible_rank_counts += add
  +    else:
  +        def _acc_counts(lst):
  +            if lst:
  +                ids = torch.as_tensor(lst, dtype=torch.long, device=OBS_DEVICE)
  +                ranks = (RANK_OF_ID[ids].to(torch.long) - 1)
  +                add = torch.zeros(10, dtype=torch.float32, device=OBS_DEVICE)
  +                add.index_add_(0, ranks, torch.ones_like(ranks, dtype=torch.float32))
  +                return add
  +            return torch.zeros(10, dtype=torch.float32, device=OBS_DEVICE)
  +        visible_rank_counts += _acc_counts(game_state["table"]) + _acc_counts(game_state["hands"][player_id])
  +        for team_cards in game_state["captured_squads"].values():
  +            visible_rank_counts += _acc_counts(team_cards)
  +    total_invisible = 40 - int(visible_rank_counts.sum().detach().cpu().item())
  +    for i, p in enumerate(other_players):
  +        played_rank_counts = torch.zeros(10, dtype=torch.float32, device=OBS_DEVICE)
  +        for move in game_state.get("history", []):
  +            if move.get("player") == p:
  +                pc = move.get("played_card")
  +                rank = (pc // 4) + 1 if isinstance(pc, int) else pc[0]
  +                if 1 <= rank <= 10:
  +                    played_rank_counts[rank-1] += 1
  +        hand_size = len(game_state["hands"].get(p, []))
  +        if hand_size == 0:
  +            all_probs[i, 0, :] = 1.0
  +            continue
  +        total_rank = 4
  +        for rank in range(1, 11):
  +            rank_idx = rank - 1
  +            invisible_rank = total_rank - int(visible_rank_counts[rank_idx].detach().cpu().item())
  +            played_rank = int(played_rank_counts[rank_idx].detach().cpu().item())
  +            remaining_rank = total_rank - played_rank
  +            possible_rank = min(max(remaining_rank, 0), max(invisible_rank, 0))
  +            if possible_rank < 0:
  +                all_probs[i, 0, rank_idx] = 1.0
  +                continue
  +            # k from 0..min(4, hand_size)
  +            k_max = min(possible_rank, hand_size, 4)
  +            k = torch.arange(k_max + 1, device=OBS_DEVICE, dtype=torch.float32)
  +            # Cast to float scalars on device
  +            N = torch.tensor(float(total_invisible), device=OBS_DEVICE)
  +            K = torch.tensor(float(invisible_rank), device=OBS_DEVICE)
  +            n = torch.tensor(float(hand_size), device=OBS_DEVICE)
  +            # Validity mask for hypergeometric terms: 0 <= k <= K and 0 <= n-k <= N-K and N,K,n >= 0
  +            valid = (k >= 0) & (k <= K) & ((n - k) >= 0) & ((n - k) <= (N - K)) & (N >= 0) & (K >= 0) & (n >= 0)
  +            probs_k = torch.zeros_like(k)
  +            if bool(valid.any().detach().cpu().item()):
  +                kv = k[valid]
  +                # log comb using lgamma only on valid entries
  +                def log_comb(a, b):
  +                    return torch.lgamma(a + 1.0) - torch.lgamma(b + 1.0) - torch.lgamma(a - b + 1.0)
  +                log_num = log_comb(K, kv) + log_comb(N - K, n - kv)
  +                log_den = log_comb(N, n)
  +                pv = torch.exp(log_num - log_den)
  +                probs_k[valid] = pv
  +            # Assign; remaining invalid stay 0
  +            all_probs[i, :k.numel(), rank_idx] = torch.nan_to_num(probs_k, nan=0.0, posinf=0.0, neginf=0.0)
  +    
  +    # Salva in cache
  +    rank_prob_cache[cache_key] = all_probs.clone()
  +    
  +    # Gestisci dimensione cache
  +    if len(rank_prob_cache) > 50:  # Cache più piccola perché tensori più grandi
  +        import random
  +        keys_to_remove = random.sample(list(rank_prob_cache.keys()), 25)
  +        for k in keys_to_remove:
  +            del rank_prob_cache[k]
  +    
  +    return all_probs
  +
  +
  +
  +# Funzione per pulizia cache - utile se si vuole liberare memoria
  +def clear_all_caches():
  +    """Pulisce tutte le cache usate per ottimizzare le funzioni"""
  +    missing_cards_cache.clear()
  +    inferred_probs_cache.clear()
  +    primiera_cache.clear()
  +    denari_cache.clear()
  +    settebello_cache.clear()
  +    score_cache.clear()
  +    table_sum_cache.clear()
  +    scopa_probs_cache.clear()
  +    rank_prob_cache.clear()
  + 
  +
  +def encode_recent_history_k(game_state, k=12):
  +    """
  +    Restituisce una codifica compatta delle ultime k mosse (61*k) leggendo direttamente
  +    dal ring buffer senza copie intermedie e senza torch.roll, usando head/len.
  +    """
  +    hb = game_state.get('_hist_buf_t', None)
  +    hlen_t = game_state.get('_history_len_t', None)
  +    head_t = game_state.get('_hist_head_t', None)
  +    device = hb.device
  +    k_int = int(k)
  +    T = int(hb.size(0))
  +    n = int(torch.clamp(hlen_t.to(torch.long), min=0, max=T).item())
  +    if n <= 0:
  +        return torch.zeros(61 * k_int, dtype=hb.dtype, device=device)
  +    head = int(head_t.item()) if torch.is_tensor(head_t) else n % T
  +    take = min(k_int, n)
  +    # calcola start index circolare degli ultimi `take`
  +    start = (head - take) % T
  +    if start + take <= T:
  +        seg = hb[start:start+take]
  +    else:
  +        first = hb[start:]
  +        second = hb[:(start + take) % T]
  +        seg = torch.cat([first, second], dim=0)
  +    if take < k_int:
  +        pad = torch.zeros((k_int - take, 61), dtype=hb.dtype, device=device)
  +        seg = torch.cat([pad, seg], dim=0)
  +    return seg.reshape(-1)
  + 
  +def encode_state_compact_for_player_fast(game_state, player_id, k_history=12, out: Optional[torch.Tensor] = None):
  +    """
  +    Variante interamente GPU-driven che usa i bitset mirror già residenti su CUDA
  +    (inseriti dall'ambiente come _hands_bits_t, _table_bits_t, _captured_bits_t)
  +    per minimizzare i trasferimenti H2D. Richiede che tali chiavi siano presenti.
  +    """
  +    # Use the same device of bitset mirrors (default CPU)
  +    hands_bits_t = game_state.get('_hands_bits_t', None)
  +    table_bits_t = game_state.get('_table_bits_t', None)
  +    captured_bits_t = game_state.get('_captured_bits_t', None)
  +    # Fallback handled below; pick device from any available tensor
  +    device = (hands_bits_t.device if torch.is_tensor(hands_bits_t) else
  +              table_bits_t.device if torch.is_tensor(table_bits_t) else
  +              captured_bits_t.device if torch.is_tensor(captured_bits_t) else
  +              OBS_DEVICE)
  +    if hands_bits_t is None or table_bits_t is None or captured_bits_t is None:
  +        raise ValueError('encode_state_compact_for_player_fast requires bitset mirrors (_hands_bits_t, _table_bits_t, _captured_bits_t)')
  +
  +    # Ensure observation constants live on the same device to avoid per-call tensor copies
  +    if IDS_CUDA.device != device:
  +        set_obs_device(device)
  +
  +    ids = IDS_CUDA
  +
  +    # Expand bitset mirrors once so both boolean and float views can be derived without re-shifting
  +    hands_bits_exp = hands_bits_t.to(torch.int64).unsqueeze(1)
  +    hands_mask_int = (hands_bits_exp >> ids) & 1  # (4,40)
  +    hands_mask_f = hands_mask_int.to(torch.float32)
  +
  +    table_bits_int = table_bits_t.to(torch.int64)
  +    table_mask_int = (table_bits_int >> ids) & 1  # (40)
  +    table_mask_f = table_mask_int.to(torch.float32)
  +
  +    captured_bits_exp = captured_bits_t.to(torch.int64).unsqueeze(1)
  +    captured_mask_int = (captured_bits_exp >> ids) & 1  # (2,40)
  +    captured_mask_f = captured_mask_int.to(torch.float32)
  +    captured_mask_bool = captured_mask_int.to(torch.bool)
  +
  +    # 1) Mani (43): 40 one-hot + 3 conteggi altri giocatori
  +    hand_vec = hands_mask_f[player_id]
  +    if STRICT_CHECKS and (not torch.isfinite(hand_vec).all()):
  +        raise RuntimeError("Observation hand_vec contains non-finite values")
  +    hand_counts = (hands_mask_f.sum(dim=1) / 10.0)
  +    other_idx = _get_other_player_idx(device)[player_id]
  +    other_counts_t = hand_counts.index_select(0, other_idx)
  +    hands_enc = torch.cat([hand_vec.reshape(-1), other_counts_t.reshape(-1)], dim=0)
  +
  +    # 2) Tavolo (40)
  +    table_enc = table_mask_f
  +    if STRICT_CHECKS and (not torch.isfinite(table_enc).all()):
  +        raise RuntimeError("Observation table_enc contains non-finite values")
  +
  +    # 3) Catture squadre (82): 40 + 40 + 2
  +    team0_vec = captured_mask_f[0]
  +    team1_vec = captured_mask_f[1]
  +    team_counts = captured_mask_f.sum(dim=1) / 40.0
  +    captured_enc = torch.cat([team0_vec, team1_vec, team_counts], dim=0)
  +    if STRICT_CHECKS and (not torch.isfinite(captured_enc).all()):
  +        raise RuntimeError("Observation captured_enc contains non-finite values")
  +
  +    # 4) History compatta (61*k)
  +    hist_k = encode_recent_history_k(game_state, k=k_history)
  +    if STRICT_CHECKS and (not torch.isfinite(hist_k).all()):
  +        raise RuntimeError("Observation history encoding contains non-finite values")
  +
  +    # 5) Missing cards (40): inverti visibilità (mano osservatore + tavolo + captured)
  +    visible_bits = (hands_bits_t[player_id] | table_bits_t | captured_bits_t[0] | captured_bits_t[1]).to(torch.int64)
  +    missing_vec = 1.0 - ((visible_bits >> ids) & 1).to(torch.float32)
  +    if STRICT_CHECKS and (not torch.isfinite(missing_vec).all()):
  +        raise RuntimeError("Observation missing_vec contains non-finite values")
  +
  +    # 6) Inferred probs (120) - opzionale
  +    inferred_probs = (compute_inferred_probabilities(game_state, player_id)
  +                      if OBS_INCLUDE_INFERRED else None)
+++    inferred_probs_l2 = (compute_inferred_probabilities_level2(game_state, player_id)
+++                         if OBS_INCLUDE_INFERRED_L2 else None)
+++    inferred_probs_l3 = (compute_inferred_probabilities_level3(game_state, player_id)
+++                         if OBS_INCLUDE_INFERRED_L3 else None)
  +
  +    suits_oh = SUITS_OH_4x40
  +    prim_vals = PRIMIERA_PER_ID
  +    prim_mask = captured_mask_bool.unsqueeze(1) & suits_oh.unsqueeze(0)  # (2,4,40)
  +    prim_masked_vals = prim_mask.to(torch.float32) * prim_vals.view(1, 1, 40)
  +    primiera_team = prim_masked_vals.max(dim=2).values / 21.0
  +    primiera_status = primiera_team.reshape(-1)
  +
  +    # 8) Denari count (2)
  +    denari_mask = IS_DENARI_MASK_40
  +    denari_raw = (captured_mask_bool & denari_mask).sum(dim=1).to(torch.float32)
  +    denari_count = denari_raw / 10.0
  +
  +    # 9) Settebello (1)
  +    settebello_id = 24
  +    have = captured_mask_f[:, settebello_id]
  +    on_tbl = table_mask_f[settebello_id]
  +    settebello_status = (have[0] + have[1] * 2.0 + on_tbl * 3.0).unsqueeze(0) / 3.0
  +
  +    # 10) Score estimate (2) - tensor path using mirrors to stay compilable
  +    scopa_counts = game_state.get('_scopa_counts_t', None)
  +    if scopa_counts is None:
  +        scopa_counts = torch.zeros(2, dtype=torch.float32, device=device)
  +    # total cards winner
  +    team_card_counts = captured_mask_f.sum(dim=1)
  +    pt_c0 = (team_card_counts[0] > team_card_counts[1]).to(torch.float32)
  +    pt_c1 = (team_card_counts[1] > team_card_counts[0]).to(torch.float32)
  +    # denari winner
  +    pt_d0 = (denari_raw[0] > denari_raw[1]).to(torch.float32)
  +    pt_d1 = (denari_raw[1] > denari_raw[0]).to(torch.float32)
  +    # settebello
  +    sb0 = have[0]
  +    sb1 = have[1]
  +    # primiera winner using per-suit maxima sums
  +    prim_sums = primiera_team.sum(dim=1)
  +    prim0 = prim_sums[0]
  +    prim1 = prim_sums[1]
  +    pt_p0 = (prim0 > prim1).to(torch.float32)
  +    pt_p1 = (prim1 > prim0).to(torch.float32)
  +    total0 = pt_c0 + pt_d0 + sb0 + pt_p0 + scopa_counts[0]
  +    total1 = pt_c1 + pt_d1 + sb1 + pt_p1 + scopa_counts[1]
  +    score_estimate = torch.stack([total0, total1]).to(torch.float32) / 12.0
  +
  +    # 11) Table sum (1) via bitset
  +    table_sum = (RANK_OF_ID_F32 * table_mask_f).sum().unsqueeze(0) / 30.0
  +
  +    # 12) Scopa probs next (10) - opzionale; prefer mirror to stay compilable
  +    if OBS_INCLUDE_SCOPA_PROBS:
  +        scopa_probs = game_state.get('_scopa_probs_t', torch.zeros(10, dtype=torch.float32, device=device))
  +    else:
  +        scopa_probs = torch.zeros(10, dtype=torch.float32, device=device)
  +
  +    # 13) Rank probs by player (150) - opzionale; prefer mirror
  +    if OBS_INCLUDE_RANK_PROBS:
  +        rpb = game_state.get('_rank_probs_by_player_t', None)
  +        rank_probs_by_player = (rpb.flatten() if torch.is_tensor(rpb) else torch.zeros(150, dtype=torch.float32, device=device))
  +    else:
  +        rank_probs_by_player = torch.zeros(150, dtype=torch.float32, device=device)
  +
  +    # 14) Scopa counts (2)
  +    scopa_counts = compute_scopa_counts(game_state)
  +
  +    # 15) Table possible sums (10) — reuse cached subset computation (CPU fast-path)
  +    table_possible_sums = compute_table_possible_sums(game_state)
  +    if table_possible_sums.device != device:
  +        table_possible_sums = table_possible_sums.to(device=device)
  +    # 16) Progress (1) e 17) Last capturing team (2)
  +    p_m = game_state.get('_progress_t', None)
  +    lct_m = game_state.get('_last_capturing_team_t', None)
  +    progress = p_m if (p_m.device == device and p_m.dtype == torch.float32) else p_m.to(device=device, dtype=torch.float32)
  +    last_capturing_team = (lct_m if (lct_m.device == device and lct_m.dtype == torch.float32)
  +                           else lct_m.to(device=device, dtype=torch.float32))
  +
  +    # Prealloc result and write slices to reduce cat overhead
  +    include_scopa = OBS_INCLUDE_SCOPA_PROBS
  +    include_rank = OBS_INCLUDE_RANK_PROBS
  +    include_inferred = OBS_INCLUDE_INFERRED
--     expected_dim = (43 + 40 + 82 + 61 * k_history + 40 + (120 if include_inferred else 0) + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
--                     + (10 if include_scopa else 0) + (150 if include_rank else 0) + (4 if OBS_INCLUDE_DEALER else 0))
+++    include_inferred_l2 = OBS_INCLUDE_INFERRED_L2
+++    include_inferred_l3 = OBS_INCLUDE_INFERRED_L3
+++    expected_dim = (43 + 40 + 82 + 61 * k_history + 40
+++                    + (120 if include_inferred else 0)
+++                    + (120 if include_inferred_l2 else 0)
+++                    + (120 if include_inferred_l3 else 0)
+++                    + 8 + 2 + 1 + 2 + 1 + 10 + 2 + 30 + 3
+++                    + (10 if include_scopa else 0)
+++                    + (150 if include_rank else 0)
+++                    + (4 if OBS_INCLUDE_DEALER else 0))
  +    if (out is not None) and torch.is_tensor(out) and out.shape == (expected_dim,) and out.dtype == torch.float32 and out.device == device:
  +        result = out
  +    else:
  +        result = torch.empty((expected_dim,), dtype=torch.float32, device=device)
  +    pos = 0
  +    def _w(t):
  +        nonlocal pos
  +        # Assume all tensors are already float32 on correct device to avoid per-slice conversions
  +        n = int(t.numel())
  +        result[pos:pos+n] = t.reshape(-1)
  +        pos += n
  +    _w(hands_enc)
  +    _w(table_enc)
  +    _w(captured_enc)
  +    _w(hist_k)
  +    _w(missing_vec)
  +    if include_inferred and inferred_probs is not None:
  +        _w(inferred_probs if torch.is_tensor(inferred_probs) else torch.as_tensor(inferred_probs, dtype=torch.float32, device=device))
+++    if include_inferred_l2 and inferred_probs_l2 is not None:
+++        _w(inferred_probs_l2 if torch.is_tensor(inferred_probs_l2) else torch.as_tensor(inferred_probs_l2, dtype=torch.float32, device=device))
+++    if include_inferred_l3 and inferred_probs_l3 is not None:
+++        _w(inferred_probs_l3 if torch.is_tensor(inferred_probs_l3) else torch.as_tensor(inferred_probs_l3, dtype=torch.float32, device=device))
  +    _w(primiera_status)
  +    _w(denari_count)
  +    _w(settebello_status)
  +    _w(score_estimate)
  +    _w(table_sum)
  +    _w(table_possible_sums)
  +    _w(scopa_counts)
  +    if include_scopa:
  +        _w(scopa_probs)
  +    if include_rank:
  +        _w(rank_probs_by_player)
  +    _w(progress)
  +    _w(last_capturing_team)
  +    if OBS_INCLUDE_DEALER:
  +        # Compute dealer one-hot purely with tensor ops to stay compile-friendly
  +        curr = torch.as_tensor(int(game_state.get('current_player', -1)), dtype=torch.long, device=device)
  +        hlen_t = game_state.get('_history_len_t', None)
  +        hlen_mod4 = (hlen_t.to(torch.long) % 4) if torch.is_tensor(hlen_t) else torch.zeros((), dtype=torch.long, device=device)
  +        starting_seat = torch.remainder(curr - hlen_mod4, 4)
  +        dealer_idx = torch.remainder(starting_seat - 1, 4)
  +        dealer_vec = (_get_players_arange(device) == dealer_idx).to(torch.float32)
  +        _w(dealer_vec)
  +    # rank_presence_from_inferred (30) always appended at end for fast-path coherence
  +    rpf = game_state.get('_rank_presence_from_inferred_t', None)
  +    _w(rpf)
  +    return result
diff --cc tests/test_code.py
index 881403c,881403c,0000000..9e2001e
mode 100644,100644,000000..100644
--- a/tests/test_code.py
+++ b/tests/test_code.py
@@@@ -1,1178 -1,1178 -1,0 +1,1178 @@@@
  +import pytest
  +import random
  +import numpy as np
  +import torch
  +from environment import ScoponeEnvMA
  +from actions import decode_action_ids, encode_action_from_ids_tensor
  +from state import initialize_game
  +from rewards import compute_final_score_breakdown
  +from observation import compute_table_sum, compute_denari_count, compute_settebello_status
  +from models.action_conditioned import ActionConditionedActor
  +import os
  +
  +
  +def test_initialize_game_ids_only():
  +    gs = initialize_game()
  +    # 4 mani da 10 ID ciascuna
  +    assert all(isinstance(cid, int) for p in range(4) for cid in gs['hands'][p])
  +    assert isinstance(gs['table'], list)
  +    assert all(isinstance(cid, int) for cid in gs['table'])
  +
  +
  +def test_env_reset_and_compact_obs_shape():
  +    env = ScoponeEnvMA(k_history=4)
  +    obs = env.reset()
  +    assert obs.ndim == 1 and obs.shape[0] == env.observation_space.shape[0]
  +
  +
  +def test_valid_actions_and_decode_ids_roundtrip():
  +    env = ScoponeEnvMA(k_history=4)
  +    env.reset()
  +    legals = env.get_valid_actions()
  +    assert len(legals) > 0
  +    pid, cap = decode_action_ids(legals[0])
  +    assert isinstance(pid, int)
  +    assert all(isinstance(c, int) for c in cap)
  +
  +
  +def test_step_random_until_done_or_cap():
  +    env = ScoponeEnvMA(k_history=4)
  +    env.reset()
  +    done, steps = False, 0
  +    while not done and steps < 200:
  +        legals = env.get_valid_actions()
  +        is_empty = (legals.numel() == 0) if torch.is_tensor(legals) else (len(legals) == 0)
  +        if is_empty:
  +            break
  +        if torch.is_tensor(legals):
  +            idx = random.randrange(legals.size(0))
  +            act = legals[idx]
  +        else:
  +            act = random.choice(legals)
  +        _, _, done, _ = env.step(act)
  +        steps += 1
  +    assert steps > 0
  +
  +
  +def test_compute_final_score_breakdown_id_input_v1():
  +    gs = initialize_game()
  +    # settebello (24), due denari (0,4), una non-denari (2)
  +    gs['captured_squads'][0] = [24, 0, 4, 2]
  +    gs['captured_squads'][1] = [1, 3]
  +    bd = compute_final_score_breakdown(gs, rules={})
  +    assert isinstance(bd, dict) and 0 in bd and 1 in bd
  +
  +
  +def test_observation_helpers_id_only():
  +    gs = initialize_game()
  +    gs['table'] = [0, 4, 8]  # 1+2+3 denari
  +    tsum = compute_table_sum(gs)
  +    v = float(tsum[0].detach().cpu().item()) if torch.is_tensor(tsum) else float(tsum[0])
  +    assert tsum.shape == (1,) and np.isclose(v, (1+2+3)/30.0)
  +    gs['captured_squads'][0] = [24, 0]  # settebello + un denari
  +    gs['captured_squads'][1] = [1, 2]
  +    d = compute_denari_count(gs)
  +    d0 = float(d[0].detach().cpu().item()) if torch.is_tensor(d) else float(d[0])
  +    assert d.shape == (2,) and np.isclose(d0, 2/10.0)
  +    sb = compute_settebello_status(gs)
  +    assert sb.shape == (1,)
  +
  +import pytest
  +import random
  +import torch
  +from environment import ScoponeEnvMA
  +from actions import decode_action_ids, encode_action_from_ids_tensor
  +from state import initialize_game
  +from rewards import compute_final_score_breakdown
  +
  +
  +def test_env_reset_and_shapes():
  +    env = ScoponeEnvMA(k_history=4)
  +    obs = env.reset()
  +    assert obs.ndim == 1 and obs.shape[0] == env.observation_space.shape[0]
  +
  +
  +def test_valid_actions_and_decode_ids():
  +    env = ScoponeEnvMA(k_history=4)
  +    env.reset()
  +    legals = env.get_valid_actions()
  +    assert len(legals) > 0
  +    played, captured = decode_action_ids(legals[0])
  +    assert isinstance(played, int)
  +    assert all(isinstance(c, int) for c in captured)
  +
  +
  +def test_step_and_final_breakdown():
  +    env = ScoponeEnvMA(k_history=4)
  +    env.reset()
  +    done = False
  +    info = {}
  +    while not done:
  +        legals = env.get_valid_actions()
  +        is_empty = (legals.numel() == 0) if torch.is_tensor(legals) else (len(legals) == 0)
  +        if is_empty:
  +            break
  +        action = (legals[random.randrange(legals.size(0))] if torch.is_tensor(legals) else random.choice(legals))
  +        _, _, done, info = env.step(action)
  +    if done:
  +        assert 'score_breakdown' in info and 'team_rewards' in info
  +
  +
  +def test_compute_final_score_breakdown_id_input_v2():
  +    gs = initialize_game()
  +    # Assegna alcune carte catturate come ID
  +    gs['captured_squads'][0] = [24, 9, 3]  # settebello + 3 denari + 1 spade
  +    gs['captured_squads'][1] = [1, 2]
  +    bd = compute_final_score_breakdown(gs, rules={})
  +    assert 0 in bd and 1 in bd
  +import random
  +import torch
  +# Rimuovi import legacy non più presenti
  +from observation import (
  +    compute_primiera_status,
  +    compute_missing_cards_matrix,
  +    compute_table_sum, 
  +    compute_settebello_status,
  +    compute_denari_count,
  +    compute_next_player_scopa_probabilities
  +)
  +
  +# Helpers ID for tests
  +SUIT_TO_COL = {'denari': 0, 'coppe': 1, 'spade': 2, 'bastoni': 3}
  +def tid(card_tuple):
  +    r, s = card_tuple
  +    return (r - 1) * 4 + SUIT_TO_COL[s]
  +
  +# Importa i moduli modificati
  +from environment import ScoponeEnvMA
  +from actions import decode_action_ids
  +from state import create_deck, initialize_game, SUITS, RANKS
  +# game_logic.update_game_state deprecated in favor of env.step; keep import for compatibility in this test
  +# update_game_state deprecated; tests now use env.step exclusively
  +from rewards import compute_final_score_breakdown, compute_final_reward_from_breakdown
  +
  +@pytest.fixture
  +def env_fixture():
  +    """
  +    Fixture che crea e restituisce l'ambiente aggiornato.
  +    """
  +    return ScoponeEnvMA()
  +
  +
  +def test_create_deck():
  +    """
  +    Test su create_deck() per verificare che il mazzo contenga 40 carte distinte
  +    e le carte siano effettivamente una combinazione di SUITS x RANKS.
  +    """
  +    deck = create_deck()
  +    assert len(deck) == 40, "Il mazzo deve contenere 40 carte"
  +    assert len(set(deck)) == 40, "Non devono esserci duplicati nel mazzo"
  +
  +    # Verifica che ogni carta appartenga a SUITS x RANKS
  +    for cid in deck:
  +        assert isinstance(cid, int)
  +        r = cid // 4 + 1
  +        sidx = cid % 4
  +        assert r in RANKS
  +        assert sidx in [0,1,2,3]
  +
  +
  +def test_initialize_game():
  +    """
  +    Verifica che initialize_game() crei uno stato con:
  +      - 4 mani da 10 carte l'una
  +      - table vuoto
  +      - captured_squads = {0:[], 1:[]}
  +      - history vuota
  +    E che le carte siano correttamente "mischiate".
  +    """
  +    state = initialize_game()
  +    assert len(state["hands"]) == 4
  +    for p in range(4):
  +        assert len(state["hands"][p]) == 10
  +    assert state["table"] == []
  +    assert len(state["captured_squads"][0]) == 0
  +    assert len(state["captured_squads"][1]) == 0
  +    assert len(state["history"]) == 0
  +
  +    # Controllo "shuffle": non è un test rigoroso,
  +    # ma almeno controllo che la prima carta di due giocatori diversi non sia identica.
  +    assert state["hands"][0][0] != state["hands"][1][0], \
  +        "Le prime carte di P0 e P1 non dovrebbero coincidere (mazzo non mescolato?)."
  +
  +
  +# Removed test_encode_card_onehot as this function no longer exists
  +
  +
  +def test_encode_action_decode_action():
  +    """
  +    Verifica che encode_action+decode_action siano inversi con diverse combinazioni di carte
  +    usando la rappresentazione a matrice.
  +    """
  +    # Esempio di codifica: carta (7, 'denari') cattura [(3, 'spade'), (4, 'coppe')]
  +    card = (7, 'denari')
  +    cards_to_capture = [(3, 'spade'), (4, 'coppe')]
  +    import torch
  +    action_vec = encode_action_from_ids_tensor(torch.tensor(tid(card), dtype=torch.long), torch.tensor([tid(x) for x in cards_to_capture], dtype=torch.long))
  +    
  +    # Verifichiamo la dimensione del vettore di azione
  +    assert action_vec.shape == (80,), "Il vettore di azione deve avere 80 dimensioni"
  +    
  +    # Decodifichiamo e verifichiamo che otteniamo la stessa carta e carte da catturare
  +    dec_card, dec_captured = decode_action_ids(action_vec)
  +    assert dec_card == tid(card)
  +    assert set(dec_captured) == set(tid(x) for x in cards_to_capture)
  +    
  +    # Test con subset vuoto: carta (5, 'bastoni') senza catture
  +    card2 = (5, 'bastoni')
  +    cards_to_capture2 = []
  +    import torch
  +    action_vec2 = encode_action_from_ids_tensor(torch.tensor(tid(card2), dtype=torch.long), torch.tensor([], dtype=torch.long))
  +    dec_card2, dec_captured2 = decode_action_ids(action_vec2)
  +    assert dec_card2 == tid(card2)
  +    assert dec_captured2 == []
  +    
  +    # Test con più carte da catturare: (10, 'coppe') cattura [(2, 'denari'), (3, 'spade'), (5, 'bastoni')]
  +    card3 = (10, 'coppe')
  +    cards_to_capture3 = [(2, 'denari'), (3, 'spade'), (5, 'bastoni')]
  +    import torch
  +    action_vec3 = encode_action_from_ids_tensor(torch.tensor(tid(card3), dtype=torch.long), torch.tensor([tid(x) for x in cards_to_capture3], dtype=torch.long))
  +    dec_card3, dec_captured3 = decode_action_ids(action_vec3)
  +    assert dec_card3 == tid(card3)
  +    assert set(dec_captured3) == set(tid(x) for x in cards_to_capture3)
  +
  +
  +def test_decode_action_invalid_vector():
  +    """Verifica che decode_action sollevi ValueError quando la carta giocata non è specificata."""
  +    invalid_vec = torch.zeros(80, dtype=torch.float32)
  +    import pytest
  +    with pytest.raises(ValueError):
  +        decode_action_ids(invalid_vec)
  +
  +
  +def test_get_valid_actions_direct_capture(env_fixture):
  +    """
  +    Test 'get_valid_actions' in un caso dove esiste la cattura diretta.
  +    Se c'è una carta sul tavolo con rank == carta giocata, DEVO catturare e
  +    non posso buttare la carta (né fare somme).
  +    """
  +    env = env_fixture
  +    env.reset()
  +    # Forziamo scenario in ID
  +    env.game_state["hands"][0] = [tid((4,'denari')), tid((7,'spade'))]
  +    env.game_state["table"] = [tid((7,'denari')), tid((3,'spade')), tid((4,'coppe'))]
  +    env.current_player = 0
  +    # Sync mirrors for GPU-only environment after manual mutation
  +    env._rebuild_id_caches()
  +
  +    valids = env.get_valid_actions()
  +    plays = []
  +    for v in valids:
  +        pid, caps = decode_action_ids(v)
  +        plays.append((pid, tuple(sorted(caps))))
  +    expected = {
  +        (tid((4,'denari')), (tid((4,'coppe')),)),
  +        (tid((7,'spade')), (tid((7,'denari')),)),
  +    }
  +    assert set(plays) >= expected
  +
  +
  +def test_get_valid_actions_no_direct_capture(env_fixture):
  +    """
  +    Caso in cui NON c'è cattura diretta, ma c'è una somma possibile.
  +    Oppure si butta la carta se nessuna somma è possibile.
  +    """
  +    env = env_fixture
  +    env.reset()
  +    env.game_state["hands"][0] = [tid((6,'denari')), tid((7,'spade'))]
  +    env.game_state["table"] = [tid((1,'coppe')), tid((3,'spade')), tid((2,'bastoni'))]
  +    env.current_player = 0
  +    env._rebuild_id_caches()
  +
  +    valids = env.get_valid_actions()
  +    plays = []
  +    for v in valids:
  +        pid, caps = decode_action_ids(v)
  +        plays.append((pid, tuple(sorted(caps))))
  +    expected = {
  +        (tid((6,'denari')), tuple(sorted([tid((1,'coppe')), tid((3,'spade')), tid((2,'bastoni'))]))),
  +        (tid((7,'spade')), tuple()),
  +    }
  +    assert set(plays) >= expected
  +
  +
  +
  +def test_step_basic(env_fixture):
  +    """
  +    Verifica che un 'step' con un'azione valida non sollevi eccezioni e
  +    restituisca (next_obs, reward=0.0, done=False) se la partita non è finita.
  +    """
  +    env = env_fixture
  +    env.reset()
  +    valids = env.get_valid_actions()
  +    assert len(valids) > 0, "Appena dopo reset, dovrebbero esserci azioni valide"
  +    first_action = valids[0]
  +
  +    next_obs, reward, done, info = env.step(first_action)
  +    assert reward == 0.0
  +    assert done == False
  +    assert "team_rewards" not in info
  +
  +
  +def test_step_invalid_action(env_fixture):
  +    """
  +    Verifica che se provo uno step con un'azione NON valida, venga sollevata una ValueError.
  +    """
  +    env = env_fixture
  +    env.reset()
  +    
  +    # Forziamo una situazione più controllata
  +    env.game_state["hands"][0] = [(7, 'denari'), (3, 'coppe')]
  +    env.game_state["table"] = [(4, 'bastoni'), (3, 'bastoni')]
  +    env.current_player = 0
  +    
  +    # Prendi le azioni valide in questo stato
  +    valids = env.get_valid_actions()
  +    
  +    # Creiamo un'azione chiaramente invalida: giocare una carta non presente nella mano
  +    invalid_card = (5, 'spade')  # Una carta che non è nella mano del giocatore
  +    
  +    # Creiamo un'azione che tenta di giocare questa carta non presente in mano
  +    import torch
  +    invalid_action = encode_action_from_ids_tensor(torch.tensor(tid(invalid_card), dtype=torch.long), torch.tensor([], dtype=torch.long))
  +    
  +    # Questo dovrebbe sollevare ValueError perché la carta non è nella mano
  +    with pytest.raises(ValueError):
  +        env.step(invalid_action)
  +
  +
  +def test_done_and_final_reward(env_fixture):
  +    """
  +    Esegue step finché la partita non finisce. A fine partita (done=True),
  +    controlla che in info ci sia "team_rewards" e che la lunghezza sia 2.
  +    """
  +    env = env_fixture
  +    env.reset()
  +    done = False
  +    info = {}
  +    while not done:
  +        valids = env.get_valid_actions()
  +        is_empty = (valids.numel() == 0) if torch.is_tensor(valids) else (len(valids) == 0)
  +        if is_empty:
  +            break
  +        action = (valids[random.randrange(valids.size(0))] if torch.is_tensor(valids) else random.choice(valids))
  +        obs, r, done, info = env.step(action)
  +
  +    assert done is True, "La partita dovrebbe risultare finita."
  +    assert "team_rewards" in info, "L'info finale dovrebbe contenere team_rewards."
  +    assert len(info["team_rewards"]) == 2, "team_rewards dev'essere un array di 2 (team0, team1)."
  +
  +
  +def test_scopa_case(env_fixture):
  +    """
  +    Test di un caso specifico per verificare la "scopa" e che la reward di scopa sia assegnata correttamente.
  +    Ricordiamo che la scopa in questa implementazione aggiunge 1 punto nel breakdown, se non è l'ultima giocata.
  +    """
  +    # Costruiamo manualmente uno scenario:
  +    #   - P0 ha in mano solo (7,'denari'), e ci sono es. 2 carte sul tavolo che sommano 7.
  +    #   - Ci sono ancora carte in mano ad altri giocatori, cosicché la scopa sia valida.
  +    env = env_fixture
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["hands"][0] = [tid((7,'denari'))]
  +    env.game_state["hands"][1] = [tid((5,'coppe'))]
  +    env.game_state["hands"][2] = []
  +    env.game_state["hands"][3] = []
  +    env.game_state["table"] = [tid((3,'bastoni')), tid((4,'denari'))]
  +    env.game_state["captured_squads"][0] = []
  +    env.game_state["captured_squads"][1] = []
  +    env.game_state["history"] = []
  +    env._rebuild_id_caches()
  +
  +    import torch
  +    action_vec = encode_action_from_ids_tensor(torch.tensor(tid((7,'denari')), dtype=torch.long), torch.tensor([tid((3,'bastoni')), tid((4,'denari'))], dtype=torch.long))
  +    obs_after, r, done, info = env.step(action_vec)
  +    new_gs = env.game_state
  +    rw_array = info.get("team_rewards", [0.0, 0.0]) if done else [0.0, 0.0]
  +
  +    # Non dovrebbe essere done, perché P1 ha ancora 1 carta
  +    assert done is False, "Non è l'ultima mossa: P1 ha carte."
  +    # Ricontrolliamo la history: l'ultima mossa dev'essere "scopa".
  +    last_move = new_gs["history"][-1]
  +    assert last_move["capture_type"] == "scopa", "Dovrebbe risultare scopa avendo svuotato il tavolo."
  +
  +    # Ricontrolliamo che la cattura sia finita in captured_squads del team0
  +    # (team0 = giocatori 0 e 2)
  +    assert len(new_gs["captured_squads"][0]) == 3, "Team0 deve aver preso (7,'denari'), (3,'bastoni'), (4,'denari')"
  +    # E il tavolo ora deve essere vuoto
  +    assert len(new_gs["table"]) == 0
  +
  +    # Poiché non è finita la partita, la reward dev'essere [0,0]
  +    assert rw_array == [0.0, 0.0]
  +
  +    # Non forziamo più la fine con utility: il test si ferma qui dopo aver verificato la scopa
  +
  +
  +def test_full_match_random(env_fixture):
  +    """
  +    Test finale in cui giochiamo una partita random completa con l'Env:
  +    a) Controlliamo che si arrivi a done=True senza errori.
  +    b) Controlliamo che info['team_rewards'] abbia valori coerenti.
  +    """
  +    env = env_fixture
  +    obs = env.reset()
  +    done = False
  +    while not done:
  +        valids = env.get_valid_actions()
  +        is_empty = (valids.numel() == 0) if torch.is_tensor(valids) else (len(valids) == 0)
  +        if is_empty:
  +            # A volte può succedere che un player abbia finito le carte
  +            # ma non è done perché altri player hanno ancora carte.
  +            # Ma in questa implementazione se 'valids' è vuoto => ValueError se step con azione non valida.
  +            # Ci basterebbe passare la mano. Oppure passare al successivo.
  +            # Per semplicità, break che simula "non si può far nulla".
  +            break
  +
  +        a = valids[random.randrange(valids.size(0))]
  +        obs, rew, done, info = env.step(a)
  +
  +    if done:
  +        assert "team_rewards" in info
  +        r0, r1 = info["team_rewards"]
  +        print("Partita terminata. Ricompense finali:", r0, r1)
  +
  +        # Se r0>0 => Team0 vince, se <0 => Team1 vince, se =0 => pareggio
  +        # Non facciamo un assert specifico sul segno, potrebbe uscire pareggio.
  +        # Basta controllare la coerenza:
  +        diff = r0 - r1
  +        # Se diff>0 => r1 deve essere <0. Se diff=0 => r0=r1=0, etc.
  +        # In base alla formula diff = breakdown0 - breakdown1, rewardTeam0= diff*10, rewardTeam1= -diff*10
  +        # => r0 + r1 deve essere 0 in ogni caso
  +        assert abs(r0 + r1) < 1e-9, "Le ricompense di due team devono essere opposte"
  +
  +    else:
  +        # Se usciamo dal while senza done => la partita non è completata
  +        # Non è necessariamente un bug, ma potremmo segnalarlo.
  +        pytest.skip("La partita random non si è conclusa entro i passaggi eseguiti.")
  +
  +
  +
  +
  +
  +def test_compute_primiera_status():
  +    """
  +    Test the compute_primiera_status function that calculates primiera scores.
  +    """
  +    # Create a test game state with known cards
  +    game_state = {
  +        "captured_squads": {
  +            0: [
  +                (7, 'denari'),  # 21 points
  +                (6, 'coppe'),   # 18 points
  +                (5, 'spade'),   # 15 points
  +                (4, 'bastoni')  # 14 points
  +            ],
  +            1: [
  +                (1, 'denari'),  # 16 points
  +                (3, 'coppe'),   # 13 points
  +                (2, 'spade'),   # 12 points
  +                (10, 'bastoni') # 10 points
  +            ]
  +        }
  +    }
  +    
  +    # Calculate primiera status
  +    primiera_status = compute_primiera_status(game_state)
  +    
  +    # Should return 8 values (4 for each team)
  +    assert len(primiera_status) == 8, f"Expected 8 values, got {len(primiera_status)}"
  +    
  +    # Check team 0 values (normalized by 21.0)
  +    assert primiera_status[0] == 21.0/21.0, f"Expected 1.0 for denari, got {primiera_status[0]}"
  +    assert primiera_status[1] == 18.0/21.0, f"Expected 0.857 for coppe, got {primiera_status[1]}"
  +    assert primiera_status[2] == 15.0/21.0, f"Expected 0.714 for spade, got {primiera_status[2]}"
  +    assert primiera_status[3] == 14.0/21.0, f"Expected 0.667 for bastoni, got {primiera_status[3]}"
  +    
  +    # Check team 1 values
  +    assert primiera_status[4] == 16.0/21.0, f"Expected 0.762 for denari, got {primiera_status[4]}"
  +    assert primiera_status[5] == 13.0/21.0, f"Expected 0.619 for coppe, got {primiera_status[5]}"
  +    assert primiera_status[6] == 12.0/21.0, f"Expected 0.571 for spade, got {primiera_status[6]}"
  +    assert primiera_status[7] == 10.0/21.0, f"Expected 0.476 for bastoni, got {primiera_status[7]}"
  +
  +
  +def test_compute_missing_cards_matrix():
  +    """
  +    Test the compute_missing_cards_matrix function that identifies cards not visible to a player.
  +    """
  +    # Create a test game state where some cards are visible
  +    game_state = {
  +        "hands": {
  +            0: [tid((1, 'denari')), tid((2, 'coppe'))],
  +            1: [tid((3, 'spade')), tid((4, 'bastoni'))],
  +            2: [tid((5, 'denari')), tid((6, 'coppe'))],
  +            3: [tid((7, 'spade')), tid((8, 'bastoni'))]
  +        },
  +        "table": [tid((9, 'denari')), tid((10, 'coppe'))],
  +        "captured_squads": {
  +            0: [tid((1, 'coppe')), tid((2, 'spade'))],
  +            1: [tid((3, 'bastoni')), tid((4, 'denari'))]
  +        }
  +    }
  +    
  +    # Test for player 0
  +    missing_cards = compute_missing_cards_matrix(game_state, 0)
  +    
  +    # Should return a flattened 10x4 matrix (40 dimensions)
  +    assert missing_cards.shape == (40,), f"Expected 40 dimensions, got {missing_cards.shape}"
  +    
  +    # Calculate how many cards are missing (should be 40 - visible cards)
  +    visible_count = (
  +        len(game_state["hands"][0]) +
  +        len(game_state["table"]) +
  +        len(game_state["captured_squads"][0]) +
  +        len(game_state["captured_squads"][1])
  +    )
  +    
  +    # Total non-zero values in the matrix should be 40 - visible_count
  +    # Because our missing cards matrix has 1s for missing cards
  +    # Handle torch Tensor on CUDA safely for numpy ops
  +    non_zero_count = int(torch.count_nonzero(missing_cards).detach().cpu().item())
  +    assert non_zero_count == 40 - visible_count, f"Expected {40 - visible_count} missing cards, found {non_zero_count}"
  +
  +
  +def test_compute_table_sum():
  +    """
  +    Test the compute_table_sum function that calculates the sum of ranks on the table.
  +    """
  +    # Create a test game state with known cards on the table
  +    game_state = {
  +        "table": [(1, 'denari'), (2, 'coppe'), (3, 'spade'), (4, 'bastoni')]
  +    }
  +    
  +    # Calculate table sum
  +    table_sum = compute_table_sum(game_state)
  +    
  +    # Should return a single value (normalized by 30.0)
  +    assert table_sum.shape == (1,), f"Expected 1 dimension, got {table_sum.shape}"
  +    
  +    # Sum should be (1+2+3+4)/30 = 10/30 = 0.333...
  +    expected_sum = 10.0 / 30.0
  +    assert np.isclose(float(table_sum[0].item()), expected_sum), f"Expected {expected_sum}, got {table_sum[0]}"
  +    
  +    # Test with empty table
  +    empty_game_state = {"table": []}
  +    empty_table_sum = compute_table_sum(empty_game_state)
  +    assert empty_table_sum[0] == 0.0, f"Expected 0.0 for empty table, got {empty_table_sum[0]}"
  +
  +
  +def test_compute_settebello_status():
  +    """
  +    Test the compute_settebello_status function that tracks where the 7 of denari is.
  +    """
  +    # Test when settebello is captured by team 0
  +    game_state_team0 = {"captured_squads": {0: [24], 1: []}, "table": []}
  +    settebello_team0 = compute_settebello_status(game_state_team0)
  +    assert settebello_team0[0] == 1.0/3.0, f"Expected 1/3 for team 0 capture, got {settebello_team0[0]}"
  +    
  +    # Test when settebello is captured by team 1
  +    game_state_team1 = {"captured_squads": {0: [], 1: [24]}, "table": []}
  +    settebello_team1 = compute_settebello_status(game_state_team1)
  +    assert settebello_team1[0] == 2.0/3.0, f"Expected 2/3 for team 1 capture, got {settebello_team1[0]}"
  +    
  +    # Test when settebello is on the table
  +    game_state_table = {"captured_squads": {0: [], 1: []}, "table": [24]}
  +    settebello_table = compute_settebello_status(game_state_table)
  +    assert settebello_table[0] == 3.0/3.0, f"Expected 3/3 for table, got {settebello_table[0]}"
  +    
  +    # Test when settebello is not visible
  +    game_state_hidden = {
  +        "captured_squads": {
  +            0: [],
  +            1: []
  +        },
  +        "table": []
  +    }
  +    settebello_hidden = compute_settebello_status(game_state_hidden)
  +    assert settebello_hidden[0] == 0.0, f"Expected 0 for hidden, got {settebello_hidden[0]}"
  +
  +
  +def test_compute_next_player_scopa_probabilities(monkeypatch):
  +    """
  +    Test the compute_next_player_scopa_probabilities function that assesses scopa chances.
  +    Uses a simplified approach that doesn't rely on the mocked function behavior.
  +    """
  +    # Create a test game state with known conditions
  +    game_state = {
  +        "hands": {
  +            0: [(1, 'denari')],  # Current player has a 1 of denari
  +            1: [(7, 'denari')]   # Next player has a 7 of denari (we'll see this in the test)
  +        },
  +        "table": [(1, 'coppe')]  # Table has 1 of coppe - will cause empty table if captured
  +    }
  +    
  +    # Define a simplified mock that returns predictable values
  +    def mock_compute_rank_probabilities(game_state, player_id):
  +        # Return a simple torch tensor of zeros with shape (3,5,10)
  +        import torch as _torch
  +        probs_t = _torch.zeros((3, 5, 10), dtype=_torch.float32)
  +        return probs_t
  +    
  +    # Apply the monkeypatch
  +    monkeypatch.setattr('observation.compute_rank_probabilities_by_player', mock_compute_rank_probabilities)
  +    
  +    # Calculate scopa probabilities
  +    scopa_probs = compute_next_player_scopa_probabilities(game_state, 0, rank_probabilities=mock_compute_rank_probabilities(game_state, 0))
  +    
  +    # Should return a 10-element array for each rank
  +    assert scopa_probs.shape == (10,), f"Expected 10 dimensions, got {scopa_probs.shape}"
  +    
  +    # Since the table contains a 1 of coppe, playing a 1 of denari would create a scopa opportunity
  +    # Our mock makes p_at_least_one = 1.0 for all ranks
  +    # So there should be a non-zero probability for rank 1
  +    assert scopa_probs[0] > 0, f"Expected non-zero probability for rank 1, got {scopa_probs[0]}"
  +
  +
  +
  +
  +def test_compute_denari_count():
  +    """
  +    Test the compute_denari_count function that counts denari cards.
  +    """
  +    # Create a test game state
  +    game_state = {"captured_squads": {0: [0,4,6], 1: [12, 17, 22]}}
  +    
  +    # Calculate denari count
  +    denari_count = compute_denari_count(game_state)
  +    
  +    # Should return 2 values (one for each team)
  +    assert denari_count.shape == (2,), f"Expected 2 dimensions, got {denari_count.shape}"
  +    
  +    # Team 0 has 2 denari out of 10 possible = 0.2
  +    assert denari_count[0] == 2.0/10.0, f"Expected 0.2 for team 0, got {denari_count[0]}"
  +    
  +    # Team 1 has 1 denari out of 10 possible = 0.1
  +    assert denari_count[1] == 1.0/10.0, f"Expected 0.1 for team 1, got {denari_count[1]}"
  +
  +
  +
  +
  +def test_reset_starting_player():
  +    """
  +    Verify that reset(starting_player=idx) sets the current player correctly.
  +    """
  +    env = ScoponeEnvMA(k_history=4)
  +    env.reset(starting_player=2)
  +    assert env.current_player == 2
  +
  +
  +def test_decode_action_wrong_length_raises():
  +    """
  +    Validazione ora avviene in env.step; un vettore di lunghezza errata deve causare errore a step.
  +    """
  +    env = ScoponeEnvMA()
  +    env.reset()
  +    env.current_player = 0
  +    bad_vec = np.zeros(79, dtype=np.float32)
  +    with pytest.raises(Exception):
  +        env.step(bad_vec)
  +
  +
  +def test_ace_take_all_valid_action_added():
  +    """
  +    With asso_piglia_tutto enabled, valid actions should include the ace capturing the entire table.
  +    """
  +    env = ScoponeEnvMA(rules={"asso_piglia_tutto": True})
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["hands"][0] = [tid((1, 'denari')), tid((5, 'spade'))]
  +    env.game_state["table"] = [tid((2, 'coppe')), tid((3, 'spade'))]
  +    env._rebuild_id_caches()
  +
  +    valids = env.get_valid_actions()
  +
  +    found_take_all = False
  +    for v in valids:
  +        pc, cc = decode_action_ids(v)
  +        if pc == tid((1, 'denari')) and set(cc) == set(env.game_state["table"]):
  +            found_take_all = True
  +            break
  +    assert found_take_all, "Ace take-all action should be present among valid actions"
  +
  +    # Ensure no ace place action [] when posability is disabled by default
  +    assert not any(decode_action_ids(v)[0] == tid((1, 'denari')) and decode_action_ids(v)[1] == [] for v in valids)
  +
  +
  +def test_ace_place_only_when_allowed_by_rules():
  +    """
  +    Ace placement (no capture) must be allowed only when rules permit it and, if only_empty, only on empty table.
  +    """
  +    rules = {
  +        "asso_piglia_tutto": True,
  +        "asso_piglia_tutto_posabile": True,
  +        "asso_piglia_tutto_posabile_only_empty": True,
  +    }
  +    env = ScoponeEnvMA(rules=rules)
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["hands"][0] = [tid((1, 'coppe'))]
  +
  +    # Non-empty table: no place action unless allowed-only-empty (we set only_empty True)
  +    env.game_state["table"] = [tid((4, 'denari'))]
  +    env._rebuild_id_caches()
  +    valids = env.get_valid_actions()
  +    assert not any(decode_action_ids(v)[0] == tid((1, 'coppe')) and decode_action_ids(v)[1] == [] for v in valids)
  +
  +    # Empty table: place action may or may not exist in new core; just ensure at least one action exists for the ace
  +    env.game_state["table"] = []
  +    env._rebuild_id_caches()
  +    valids2 = env.get_valid_actions()
  +    assert any(decode_action_ids(v)[0] == tid((1, 'coppe')) for v in valids2)
  +
  +
  +def test_step_forced_ace_capture_on_nonempty_table():
  +    """
  +    If ace placement is not allowed and table is not empty, stepping with ace+[] should force take-all.
  +    Also verify scopa demotion when scopa_on_asso_piglia_tutto is False.
  +    """
  +    env = ScoponeEnvMA(rules={"asso_piglia_tutto": True, "scopa_on_asso_piglia_tutto": False})
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["hands"][0] = [tid((1, 'denari'))]
  +    env.game_state["hands"][1] = [tid((2, 'coppe'))]
  +    env.game_state["hands"][2] = []
  +    env.game_state["hands"][3] = []
  +    env.game_state["table"] = [tid((2, 'coppe')), tid((3, 'spade'))]
  +    env._rebuild_id_caches()
  +
  +    # If we try to place ace with [], should force take-all
  +    import torch
  +    act = encode_action_from_ids_tensor(torch.tensor(tid((1, 'denari')), dtype=torch.long), torch.tensor([], dtype=torch.long))
  +    obs_after, r, done, info = env.step(act)
  +    assert info["last_move"]["capture_type"] == "capture"
  +
  +
  +def test_scopa_on_last_capture_toggle():
  +    """
  +    On last capture, capture_type should depend on scopa_on_last_capture rule.
  +    """
  +    # Case 1: scopa_on_last_capture = False -> capture
  +    env = ScoponeEnvMA(rules={"scopa_on_last_capture": False})
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["hands"][0] = [tid((3, 'denari'))]
  +    env.game_state["hands"][1] = []
  +    env.game_state["hands"][2] = []
  +    env.game_state["hands"][3] = []
  +    env.game_state["table"] = [tid((1, 'spade')), tid((2, 'coppe'))]
  +    env._rebuild_id_caches()
  +
  +    import torch
  +    act = encode_action_from_ids_tensor(torch.tensor(tid((3, 'denari')), dtype=torch.long), torch.tensor([tid((1, 'spade')), tid((2, 'coppe'))], dtype=torch.long))
  +    obs_after, r, done, info = env.step(act)
  +    assert done is True
  +    assert env.game_state["history"][-1]["capture_type"] == "capture"
  +
  +    # Case 2: scopa_on_last_capture = True -> scopa
  +    env2 = ScoponeEnvMA(rules={"scopa_on_last_capture": True})
  +    env2.reset()
  +    env2.current_player = 0
  +    env2.game_state["hands"][0] = [tid((3, 'denari'))]
  +    env2.game_state["hands"][1] = []
  +    env2.game_state["hands"][2] = []
  +    env2.game_state["hands"][3] = []
  +    env2.game_state["table"] = [tid((1, 'spade')), tid((2, 'coppe'))]
  +    env2._rebuild_id_caches()
  +    env2._rebuild_id_caches()
  +
  +    import torch
  +    act2 = encode_action_from_ids_tensor(torch.tensor(tid((3, 'denari')), dtype=torch.long), torch.tensor([tid((1, 'spade')), tid((2, 'coppe'))], dtype=torch.long))
  +    obs_after2, r2, done2, info2 = env2.step(act2)
  +    assert done2 is True
  +    assert env2.game_state["history"][-1]["capture_type"] == "scopa"
  +
  +
  +def test_last_cards_to_dealer_toggle():
  +    """
  +    Verify last cards assignment to the last capturing team depending on the rule.
  +    """
  +    # Enabled: leftover table cards go to last capturing team
  +    env = ScoponeEnvMA(rules={"last_cards_to_dealer": True})
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["hands"] = {0: [tid((5, 'denari'))], 1: [], 2: [], 3: []}
  +    env.game_state["table"] = [tid((9, 'coppe'))]
  +    env.game_state["captured_squads"] = {0: [], 1: []}
  +    env.game_state["history"] = [{"player": 1, "played_card": tid((2, 'denari')), "capture_type": "capture", "captured_cards": [tid((2, 'spade'))]}]
  +
  +    import torch
  +    act = encode_action_from_ids_tensor(torch.tensor(tid((5, 'denari')), dtype=torch.long), torch.tensor([], dtype=torch.long))
  +    obs_after, r, done, info = env.step(act)
  +    # Se non è done per un edge ordering, forza un ulteriore controllo
  +    if not done:
  +        # Con tutte le altre mani vuote, dopo questa giocata dovrebbe chiudersi
  +        # Verifica derivata: tutte le mani sono vuote
  +        assert all(len(env.game_state["hands"][p]) == 0 for p in range(4))
  +        done = True
  +    assert tid((9, 'coppe')) in env.game_state["captured_squads"][1]
  +    assert tid((5, 'denari')) in env.game_state["captured_squads"][1]
  +
  +    # Disabled: leftover table cards are not assigned
  +    env2 = ScoponeEnvMA(rules={"last_cards_to_dealer": False})
  +    env2.reset()
  +    env2.current_player = 0
  +    env2.game_state["hands"] = {0: [tid((5, 'denari'))], 1: [], 2: [], 3: []}
  +    env2.game_state["table"] = [tid((9, 'coppe'))]
  +    env2.game_state["captured_squads"] = {0: [], 1: []}
  +    env2.game_state["history"] = [{"player": 1, "played_card": tid((2, 'denari')), "capture_type": "capture", "captured_cards": [tid((2, 'spade'))]}]
  +
  +    import torch
  +    act2 = encode_action_from_ids_tensor(torch.tensor(tid((5, 'denari')), dtype=torch.long), torch.tensor([], dtype=torch.long))
  +    obs_after2, r2, done2, info2 = env2.step(act2)
  +    assert done2 is True
  +    assert len(env2.game_state["captured_squads"][1]) == 0
  +
  +
  +def test_valid_actions_cache_hit_increments():
  +    """
  +    Repeated get_valid_actions() calls without state change should hit the cache.
  +    """
  +    env = ScoponeEnvMA()
  +    env.reset()
  +    _ = env.get_valid_actions()
  +    hits_before = env._cache_hits
  +    # Second call may or may not hit depending on state hashing; allow equality
  +    _ = env.get_valid_actions()
  +    assert env._cache_hits >= hits_before
  +
  +
  +def test_table_empty_only_throw_actions():
  +    """
  +    If the table is empty, valid actions should be only 'throw' actions for each card in hand.
  +    """
  +    env = ScoponeEnvMA()
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["table"] = []
  +    env.game_state["hands"][0] = [tid((2, 'coppe')), tid((4, 'bastoni'))]
  +
  +    env._rebuild_id_caches()
  +    valids = env.get_valid_actions()
  +    # In new core, extra variants may add actions; ensure at least throw exists for each card
  +    hand_ids = set(env.game_state["hands"][0])
  +    throw_set = {(pid, tuple()) for pid in hand_ids}
  +    plays = set()
  +    for v in valids:
  +        pid, caps = decode_action_ids(v)
  +        if len(caps) == 0:
  +            plays.add((pid, tuple()))
  +    assert throw_set.issubset(plays)
  +
  +
  +def test_step_sum_mismatch_raises():
  +    """
  +    Stepping with a capture set whose sum doesn't match played rank should raise ValueError.
  +    """
  +    env = ScoponeEnvMA()
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["hands"][0] = [tid((6, 'denari'))]
  +    env.game_state["table"] = [tid((1, 'spade')), tid((2, 'coppe'))]
  +
  +    # Attempt to capture 1+2 with a 6 -> invalid
  +    import torch
  +    bad_action = encode_action_from_ids_tensor(torch.tensor(tid((6, 'denari')), dtype=torch.long), torch.tensor([tid((1, 'spade')), tid((2, 'coppe'))], dtype=torch.long))
  +    with pytest.raises(ValueError):
  +        env.step(bad_action)
  +
  +
  +def test_variant_non_scientifico_deal(env_fixture):
  +    """
  +    The 'scopone_non_scientifico' variant must deal 9 cards to each player and 4 cards face-up on the table.
  +    """
  +    env = ScoponeEnvMA(rules={"variant": "scopone_non_scientifico"})
  +    obs = env.reset()
  +    # New core uses compact obs; simply assert reset returns 1-D observation
  +    assert obs.ndim == 1 and obs.shape[0] == env.observation_space.shape[0]
  +    # Hands should have 9 cards each, table 4 cards
  +    for p in range(4):
  +        assert len(env.game_state["hands"][p]) == 9
  +    assert len(env.game_state["table"]) == 4
  +
  +
  +def test_direct_capture_takes_precedence_over_sum(env_fixture):
  +    """
  +    When same-rank cards exist on table, valid actions for that rank must be only direct captures, not sums.
  +    """
  +    env = env_fixture
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["hands"][0] = [tid((6, 'denari'))]
  +    # Table contains a 6 (direct) and also 1+5 that sum to 6
  +    env.game_state["table"] = [tid((6, 'coppe')), tid((1, 'spade')), tid((5, 'bastoni'))]
  +
  +    env._rebuild_id_caches()
  +    valids = env.get_valid_actions()
  +    # Ensure at least one direct-capture action exists for playing 6 denari
  +    pid_target = tid((6, 'denari'))
  +    has_direct = False
  +    for v in valids:
  +        pid, caps = decode_action_ids(v)
  +        if pid == pid_target and len(caps) == 1 and ((caps[0] // 4) + 1) == 6:
  +            has_direct = True
  +            break
  +    assert has_direct
  +
  +
  +def test_max_consecutive_scope_rule_limiting():
  +    """
  +    With max_consecutive_scope=1, a second consecutive scopa by the same team should be demoted to 'capture'.
  +    """
  +    env = ScoponeEnvMA(rules={"max_consecutive_scope": 1})
  +    env.reset()
  +    env.current_player = 0
  +    # Prepare history: last move was a scopa by team 0 (player 2)
  +    env.game_state["history"] = [
  +        {"player": 2, "played_card": tid((7, 'spade')), "capture_type": "scopa", "captured_cards": [tid((3, 'denari')), tid((4, 'coppe'))]}
  +    ]
  +    # Now current player 0 can also make a scopa
  +    env.game_state["hands"][0] = [tid((7, 'denari'))]
  +    env.game_state["hands"][1] = [tid((1, 'coppe'))]
  +    env.game_state["hands"][2] = []
  +    env.game_state["hands"][3] = []
  +    env.game_state["table"] = [tid((3, 'spade')), tid((4, 'bastoni'))]
  +    env._rebuild_id_caches()
  +
  +    import torch
  +    act = encode_action_from_ids_tensor(torch.tensor(tid((7, 'denari')), dtype=torch.long), torch.tensor([tid((3, 'spade')), tid((4, 'bastoni'))], dtype=torch.long))
  +    obs_after, r, done, info = env.step(act)
  +    assert env.game_state["history"][-1]["capture_type"] == "capture"
  +
  +
  +def test_compute_final_score_breakdown_rules():
  +    """
  +    Validate correctness of scoring with re_bello and napola variants.
  +    """
  +    game_state = {
  +        "captured_squads": {
  +            0: [tid((10, 'denari')), tid((7, 'denari')), tid((2, 'coppe')), tid((3, 'spade')), tid((1, 'denari')), tid((2, 'denari')), tid((3, 'denari'))],
  +            1: [tid((4, 'bastoni')), tid((5, 'spade'))]
  +        },
  +        "history": [
  +            {"player": 1, "played_card": tid((7, 'coppe')), "capture_type": "scopa", "captured_cards": [tid((7, 'spade'))]},
  +            {"player": 0, "played_card": tid((1, 'denari')), "capture_type": "capture", "captured_cards": [tid((1, 'coppe'))]},
  +            {"player": 3, "played_card": tid((2, 'bastoni')), "capture_type": "no_capture", "captured_cards": []}
  +        ]
  +    }
  +
  +    # Base scoring without variants
  +    b0 = compute_final_score_breakdown(game_state, rules={})
  +    base0 = b0[0]["total"]
  +    base1 = b0[1]["total"]
  +
  +    # re_bello adds +1 to team 0 because it has (10, 'denari')
  +    b1 = compute_final_score_breakdown(game_state, rules={"re_bello": True})
  +    assert b1[0]["re_bello"] == 1
  +    assert b1[0]["total"] == base0 + 1
  +
  +    # napola fixed3 adds +3 if team 0 has at least A-2-3 of denari
  +    b2 = compute_final_score_breakdown(game_state, rules={"napola": True, "napola_scoring": "fixed3"})
  +    assert b2[0]["napola"] == 3
  +    assert b2[0]["total"] == base0 + 3
  +
  +    # napola length counts the length of the run from A upward; here A-2-3 present -> 3 points
  +    b3 = compute_final_score_breakdown(game_state, rules={"napola": True, "napola_scoring": "length"})
  +    assert b3[0]["napola"] == 3
  +    assert b3[0]["total"] == base0 + 3
  +
  +    # Combine rules
  +    b4 = compute_final_score_breakdown(game_state, rules={"re_bello": True, "napola": True, "napola_scoring": "fixed3"})
  +    assert b4[0]["total"] == base0 + 1 + 3
  +
  +
  +def test_leftover_cards_go_to_last_capturing_team_on_done():
  +    """
  +    Ensure leftover table cards go to last capturing team at end of hand.
  +    """
  +    env = ScoponeEnvMA(rules={"last_cards_to_dealer": True})
  +    env.reset()
  +    env.current_player = 0
  +    env.game_state["hands"] = {0: [tid((5, 'denari'))], 1: [], 2: [], 3: []}
  +    env.game_state["table"] = [tid((9, 'coppe'))]
  +    env.game_state["captured_squads"] = {0: [], 1: []}
  +    # Last capture was by team 1 (player 3)
  +    env.game_state["history"] = [{"player": 3, "played_card": tid((7, 'spade')), "capture_type": "capture", "captured_cards": [tid((7, 'bastoni'))]}]
  +    env._rebuild_id_caches()
  +
  +    import torch
  +    act = encode_action_from_ids_tensor(torch.tensor(tid((5, 'denari')), dtype=torch.long), torch.tensor([], dtype=torch.long))
  +    obs_after, r, done, info = env.step(act)
  +    assert done is True
  +    assert tid((9, 'coppe')) in env.game_state["captured_squads"][1]
  +
  +
  +def test_policy_prefers_optimal_ace_king_sequence_with_checkpoint():
  +    """
  +    Scenario consecutivo in un'unica partita per valutare le preferenze della policy:
  +    - P0 ha due assi (uno di denari e uno non di denari) e nessun altro doppio rank rilevante.
  +      Atteso: posare l'asso non di denari per preservare denari.
  +    - P1 ha un asso: atteso: fare scopa catturando l'asso sul tavolo.
  +    - P2 ha un asso e nessun re: atteso: posare l'asso.
  +    - P3 ha 4 re: atteso: posare un re.
  +    Si verifica che il modello (caricato da BEST_ACTOR_CKPT) assegni logit/probabilità maggiore
  +    all'azione prevista rispetto alle alternative legali in ciascuna delle 4 mosse.
  +    """
  +    import torch
  +    import os
  +    import pytest
  +
  +    # Auto-discovery del checkpoint migliore
  +    ckpt_path = os.getenv("BEST_ACTOR_CKPT")
  +    if not ckpt_path:
  +        candidates = [
  +            os.path.join('checkpoints', 'ppo_ac_best.pth'),      # priorità: best generale
  +            os.path.join('checkpoints', 'ppo_ac_bestwr.pth'),    # poi best per win-rate
  +            os.path.join('checkpoints', 'ppo_ac.pth'),           # poi ultimo checkpoint standard
  +        ]
  +        ckpt_path = next((p for p in candidates if os.path.isfile(p)), None)
  +        # heuristic: cerca l'ultimo .pth in checkpoints/
  +        if ckpt_path is None:
  +            try:
  +                import glob
  +                all_pth = glob.glob(os.path.join('checkpoints', '*.pth'))
  +                if all_pth:
  +                    all_pth.sort(key=lambda p: os.path.getmtime(p), reverse=True)
  +                    ckpt_path = all_pth[0]
  +            except Exception:
  +                ckpt_path = None
  +    # Se non c'è alcun checkpoint, salta questo test (non è un errore funzionale del core)
  +    if not ckpt_path:
  +        pytest.skip("Nessun checkpoint disponibile per il test di policy; saltiamo questo test.")
  +    device = torch.device(os.environ.get(
  +        'SCOPONE_DEVICE',
  +        ('cuda' if torch.cuda.is_available() and os.environ.get('TESTS_FORCE_CPU') != '1' else 'cpu')
  +    ))
--     actor = ActionConditionedActor(obs_dim=10823, action_dim=80)
+++    actor = ActionConditionedActor(action_dim=80)
  +    if ckpt_path and os.path.isfile(ckpt_path):
  +        try:
  +            state = torch.load(ckpt_path, map_location=device)
  +            # Estrai esclusivamente lo state_dict dell'attore
  +            sd = None
  +            if isinstance(state, dict):
  +                if 'actor' in state and isinstance(state['actor'], dict):
  +                    sd = state['actor']
  +                elif 'actor_state_dict' in state and isinstance(state['actor_state_dict'], dict):
  +                    sd = state['actor_state_dict']
  +            if sd is None:
  +                raise RuntimeError("Checkpoint trovato ma manca la chiave 'actor'/'actor_state_dict'.")
  +            actor.load_state_dict(sd)
  +            actor.eval()
  +        except Exception as e:
  +            pytest.fail(f"Impossibile caricare il checkpoint attore da {ckpt_path}: {e}")
  +    else:
  +        pytest.skip("Checkpoint non trovato sul filesystem; saltiamo questo test di integrazione attore.")
  +
  +    env = ScoponeEnvMA()
  +    env.reset()
  +    env.current_player = 0
  +    # Setup scenario (ID-only)
  +    # P0: two aces -> (1,'denari') and (1,'spade')
  +    ace_den = tid((1,'denari'))
  +    ace_spa = tid((1,'spade'))
  +    env.game_state["hands"][0] = [ace_den, ace_spa]
  +    # P1: one ace to capture later
  +    ace_cop = tid((1,'coppe'))
  +    env.game_state["hands"][1] = [ace_cop]
  +    # P2: one ace to place later
  +    ace_bas = tid((1,'bastoni'))
  +    env.game_state["hands"][2] = [ace_bas]
  +    # P3: four kings (rank 10)
  +    k_den, k_cop, k_spa, k_bas = tid((10,'denari')), tid((10,'coppe')), tid((10,'spade')), tid((10,'bastoni'))
  +    env.game_state["hands"][3] = [k_den, k_cop, k_spa, k_bas]
  +    env.game_state["table"] = []
  +    env._rebuild_id_caches()
  +
  +    # Turno P0: preferire posare asso non di denari (ace_spa)
  +    obs0 = env._get_observation(0)
  +    legals0 = env.get_valid_actions()
  +    actions0 = legals0
  +    if actions0.dim() == 1:
  +        actions0 = actions0.unsqueeze(0)
  +    actions0 = actions0.to(device=device, dtype=torch.float32)
  +    obs0_t = torch.as_tensor(obs0, dtype=torch.float32, device=device).unsqueeze(0)
  +    seat0 = torch.zeros(6, dtype=torch.float32, device=device)
  +    seat0[0] = 1.0; seat0[4] = 1.0  # player 0, team 0/2
  +    with torch.no_grad():
  +        logits0 = actor(obs0_t, actions0, seat0.unsqueeze(0))
  +    # Identifica gli indici delle due azioni di posa ace_spa e ace_den
  +    def is_action(vec, pid_expected, caps_expected):
  +        pid, caps = decode_action_ids(vec)
  +        return pid == pid_expected and set(caps) == set(caps_expected)
  +    idx_spa = next(i for i,v in enumerate(legals0) if is_action(v, ace_spa, []))
  +    idx_den = next(i for i,v in enumerate(legals0) if is_action(v, ace_den, []))
  +    # Richiedi margine minimo per robustezza su checkpoint diversi; se troppo vicino, skip
  +    diff0 = (logits0[idx_spa] - logits0[idx_den]).item()
  +    if diff0 <= 1e-3:
  +        pytest.skip(f"Checkpoint non informativo su P0 (margine={diff0:.4g})")
  +    assert diff0 > 0, "P0 dovrebbe preferire posare l'asso NON di denari"
  +    # Esegui l'azione preferita (posa ace_spa)
  +    env.step(legals0[idx_spa])
  +
  +    # Turno P1: preferire cattura con asso (scopa)
  +    obs1 = env._get_observation(1)
  +    legals1 = env.get_valid_actions()
  +    actions1 = legals1
  +    if actions1.dim() == 1:
  +        actions1 = actions1.unsqueeze(0)
  +    actions1 = actions1.to(device=device, dtype=torch.float32)
  +    obs1_t = torch.as_tensor(obs1, dtype=torch.float32, device=device).unsqueeze(0)
  +    seat1 = torch.zeros(6, dtype=torch.float32, device=device)
  +    seat1[1] = 1.0; seat1[5] = 1.0  # player 1, team 1/3
  +    with torch.no_grad():
  +        logits1 = actor(obs1_t, actions1, seat1.unsqueeze(0))
  +    # azione attesa: played=ace_cop, captured=[ace_spa]
  +    idx_capture_ace = next(i for i,v in enumerate(legals1) if is_action(v, ace_cop, [ace_spa]))
  +    # Trova un'alternativa (es. posa ace_cop senza cattura) se presente
  +    idx_place_ace = None
  +    for i,v in enumerate(legals1):
  +        if is_action(v, ace_cop, []):
  +            idx_place_ace = i
  +            break
  +    top_idx1 = int(torch.argmax(logits1).item())
  +    assert top_idx1 == idx_capture_ace, "P1 dovrebbe preferire catturare con l'asso (scopa)"
  +    # Esegui cattura
  +    env.step(legals1[idx_capture_ace])
  +
  +    # Turno P2: preferire posare l'asso (tavolo vuoto dopo scopa)
  +    obs2 = env._get_observation(2)
  +    legals2 = env.get_valid_actions()
  +    actions2 = legals2
  +    if actions2.dim() == 1:
  +        actions2 = actions2.unsqueeze(0)
  +    actions2 = actions2.to(device=device, dtype=torch.float32)
  +    obs2_t = torch.as_tensor(obs2, dtype=torch.float32, device=device).unsqueeze(0)
  +    seat2 = torch.zeros(6, dtype=torch.float32, device=device)
  +    seat2[2] = 1.0; seat2[4] = 1.0  # player 2, team 0/2
  +    with torch.no_grad():
  +        logits2 = actor(obs2_t, actions2, seat2.unsqueeze(0))
  +    idx_place_ace2 = next(i for i,v in enumerate(legals2) if is_action(v, ace_bas, []))
  +    top_idx2 = int(torch.argmax(logits2).item())
  +    assert top_idx2 == idx_place_ace2, "P2 dovrebbe preferire posare l'asso"
  +    env.step(legals2[idx_place_ace2])
  +
  +    # Turno P3: preferire posare un re
  +    obs3 = env._get_observation(3)
  +    legals3 = env.get_valid_actions()
  +    actions3 = legals3
  +    if actions3.dim() == 1:
  +        actions3 = actions3.unsqueeze(0)
  +    actions3 = actions3.to(device=device, dtype=torch.float32)
  +    obs3_t = torch.as_tensor(obs3, dtype=torch.float32, device=device).unsqueeze(0)
  +    seat3 = torch.zeros(6, dtype=torch.float32, device=device)
  +    seat3[3] = 1.0; seat3[5] = 1.0  # player 3, team 1/3
  +    with torch.no_grad():
  +        logits3 = actor(obs3_t, actions3, seat3.unsqueeze(0))
  +    # trova indici posa re
  +    idx_k = [i for i,v in enumerate(legals3) if any(is_action(v, kid, []) for kid in [k_den,k_cop,k_spa,k_bas])]
  +    assert len(idx_k) > 0
  +    top_idx3 = int(torch.argmax(logits3).item())
--     assert top_idx3 in idx_k, "P3 dovrebbe preferire posare un re"
+++    assert top_idx3 in idx_k, "P3 dovrebbe preferire posare un re"
diff --cc tools/benchmark_ac.py
index b133a16,b133a16,0000000..586da8d
mode 100644,100644,000000..100644
--- a/tools/benchmark_ac.py
+++ b/tools/benchmark_ac.py
@@@@ -1,235 -1,235 -1,0 +1,194 @@@@
  +#!/usr/bin/env python3
  +"""
  +Benchmark for Action-Conditioned Actor/Critic with optional IS-MCTS.
  +
  +CLI:
  +  python tools/benchmark_ac.py \
  +    --games 100 \
  +    --compact --k-history 12 \
  +    --ckpt checkpoints/ppo_ac.pth \
  +    --mcts --sims 128 --dets 16 \
  +    --seed 0 \
  +    --out-csv results.csv --out-json summary.json
  +"""
  +import argparse
  +import json
  +import random
  +import numpy as np
  +import pandas as pd
  +import torch
  +
  +from environment import ScoponeEnvMA
  +from algorithms.is_mcts import run_is_mcts
+++from belief import sample_determinization
  +from models.action_conditioned import ActionConditionedActor, CentralValueNet
  +
  +
  +def load_actor_critic(ckpt_path: str, obs_dim: int, map_dev: torch.device):
  +    actor = ActionConditionedActor(obs_dim=obs_dim)
  +    critic = CentralValueNet(obs_dim=obs_dim)
  +    try:
  +        ckpt = torch.load(ckpt_path, map_location=map_dev)
  +        if 'actor' in ckpt:
  +            actor.load_state_dict(ckpt['actor'])
  +        if 'critic' in ckpt:
  +            critic.load_state_dict(ckpt['critic'])
  +    except Exception as e:
  +        print(f"[WARN] Failed to load checkpoint {ckpt_path}: {e}")
  +    return actor, critic
  +
  +
  +def run_benchmark(games=50, use_mcts=False, sims=128, dets=16, compact=True, k_history=12, ckpt_path='', seed=0,
  +                  c_puct=1.0, root_temp=0.0, prior_smooth_eps=0.0, belief_particles=256, belief_ess_frac=0.5, robust_child=True,
  +                  root_dirichlet_alpha=0.0, root_dirichlet_eps=0.0):
  +    random.seed(seed)
  +    np.random.seed(seed)
  +    torch.manual_seed(seed)
  +
  +    # Build nets with correct obs_dim from env
  +    # Create a temp env to read obs_dim
  +    tmp_env = ScoponeEnvMA(k_history=k_history)
  +    obs_dim = tmp_env.observation_space.shape[0]
  +    del tmp_env
  +    actor = ActionConditionedActor(obs_dim=obs_dim)
  +    critic = CentralValueNet(obs_dim=obs_dim)
  +    if ckpt_path:
  +        try:
  +            import os
  +            map_dev = torch.device(os.environ.get(
  +                'SCOPONE_DEVICE',
  +                ('cuda' if torch.cuda.is_available() and os.environ.get('TESTS_FORCE_CPU') != '1' else 'cpu')
  +            ))
  +            actor, critic = load_actor_critic(ckpt_path, obs_dim, map_dev)
  +        except Exception as e:
  +            print(f"[WARN] Failed to load checkpoint {ckpt_path}: {e}")
  +    actor.eval(); critic.eval()
  +
  +    results = []
  +    import os
  +    device = torch.device(os.environ.get(
  +        'SCOPONE_DEVICE',
  +        ('cuda' if torch.cuda.is_available() and os.environ.get('TESTS_FORCE_CPU') != '1' else 'cpu')
  +    ))
  +    for g in range(games):
  +        env = ScoponeEnvMA(k_history=k_history)
  +        done = False
  +        info = {}
  +        while not done:
  +            obs = env._get_observation(env.current_player)
  +            legals = env.get_valid_actions()
  +            # legals è sempre un Tensor (A,80)
  +            if legals.numel() == 0:
  +                break
  +            if use_mcts:
  +                def policy_fn(o, leg):
  +                    o_t = o.clone().detach().to(device=device, dtype=torch.float32) if torch.is_tensor(o) else torch.tensor(o, dtype=torch.float32, device=device)
  +                    # leg è sempre Tensor (A,80)
  +                    leg_t = leg if leg.dim() == 2 else leg.unsqueeze(0)
  +                    leg_t = leg_t.to(device=device, dtype=torch.float32)
  +                    cp = getattr(env, 'current_player', 0)
  +                    seat = torch.zeros((1,6), dtype=torch.float32, device=device)
  +                    seat[0, cp] = 1.0
  +                    seat[0, 4] = 1.0 if cp in [0,2] else 0.0
  +                    seat[0, 5] = 1.0 if cp in [1,3] else 0.0
  +                    with torch.no_grad():
  +                        logits = actor(o_t.unsqueeze(0), leg_t, seat)
  +                    return torch.softmax(logits, dim=0)
  +                def value_fn(o, _env=None):
  +                    o_t = o.clone().detach().to(device=device, dtype=torch.float32) if torch.is_tensor(o) else torch.tensor(o, dtype=torch.float32, device=device)
  +                    cp = _env.current_player if _env is not None else 0
  +                    s = torch.zeros(6, dtype=torch.float32, device=device)
  +                    s[cp] = 1.0
  +                    s[4] = 1.0 if cp in [0, 2] else 0.0
  +                    s[5] = 1.0 if cp in [1, 3] else 0.0
  +                    with torch.no_grad():
  +                        return critic(o_t.unsqueeze(0), s.unsqueeze(0)).item()
  +                # belief sampler neurale per determinizzazione
  +                def belief_sampler_neural(_env):
--                     try:
--                         cp = _env.current_player
--                         obs_cur = _env._get_observation(cp)
--                         o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
--                         if torch.is_tensor(o_cpu):
--                             o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
--                         s_cpu = torch.zeros(6, dtype=torch.float32)
--                         s_cpu[cp] = 1.0
--                         s_cpu[4] = 1.0 if cp in [0, 2] else 0.0
--                         s_cpu[5] = 1.0 if cp in [1, 3] else 0.0
--                         o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                         s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                         with torch.no_grad():
--                             state_feat = actor.state_enc(o_t, s_t)
--                             logits = actor.belief_net(state_feat)
--                             hand_table = o_t[:, :83]
--                             hand_mask = hand_table[:, :40] > 0.5
--                             table_mask = hand_table[:, 43:83] > 0.5
--                             captured = o_t[:, 83:165]
--                             cap0_mask = captured[:, :40] > 0.5
--                             cap1_mask = captured[:, 40:80] > 0.5
--                             visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
--                             probs_flat = actor.belief_net.probs(logits, visible_mask)
--                         probs = probs_flat.view(3, 40).detach().cpu().numpy()
--                         vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
--                         unknown_ids = [cid for cid in range(40) if not vis[cid]]
--                         others = [(cp + 1) % 4, (cp + 2) % 4, (cp + 3) % 4]
--                         det = {pid: [] for pid in others}
--                         counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
--                         caps = [int(counts.get(pid, 0)) for pid in others]
--                         n = len(unknown_ids)
--                         if sum(caps) != n:
--                             caps[2] = max(0, n - caps[0] - caps[1])
--                         for cid in unknown_ids:
--                             pc = probs[:, cid]
--                             s = pc.sum()
--                             ps = pc / (s if s > 0 else 1e-9)
--                             j = int(torch.argmax(torch.tensor(ps)).item())
--                             if caps[j] > 0:
--                                 det[others[j]].append(cid)
--                                 caps[j] -= 1
--                         return det
--                     except Exception:
--                         return None
+++                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
+++                    noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
+++                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
  +                action = run_is_mcts(env, policy_fn, value_fn, num_simulations=sims, c_puct=c_puct,
  +                                     belief=None, num_determinization=dets, root_temperature=root_temp,
  +                                     prior_smooth_eps=prior_smooth_eps, robust_child=robust_child,
  +                                     root_dirichlet_alpha=root_dirichlet_alpha, root_dirichlet_eps=root_dirichlet_eps,
  +                                     belief_sampler=belief_sampler_neural)
  +            else:
  +                with torch.no_grad():
  +                    o_t = obs.clone().detach().to(device=device, dtype=torch.float32) if torch.is_tensor(obs) else torch.tensor(obs, dtype=torch.float32, device=device)
  +                    # legals è sempre Tensor (A,80)
  +                    leg_t = legals if legals.dim() == 2 else legals.unsqueeze(0)
  +                    leg_t = leg_t.to(device=device, dtype=torch.float32)
  +                    cp = getattr(env, 'current_player', 0)
  +                    seat = torch.zeros((1,6), dtype=torch.float32, device=device)
  +                    seat[0, cp] = 1.0
  +                    seat[0, 4] = 1.0 if cp in [0,2] else 0.0
  +                    seat[0, 5] = 1.0 if cp in [1,3] else 0.0
  +                    logits = actor(o_t.unsqueeze(0), leg_t, seat)
  +                    idx = torch.argmax(logits).item()
  +                action = legals[idx]
  +            _, _, done, info = env.step(action)
  +        # per-game record
  +        entry = {'game': g}
  +        if 'score_breakdown' in info:
  +            bd = info['score_breakdown']
  +            for t in [0, 1]:
  +                for k in ['carte', 'denari', 'settebello', 'primiera', 'scope', 'total']:
  +                    entry[f'team{t}_{k}'] = bd[t].get(k, 0)
  +            entry['win_team0'] = 1 if bd[0]['total'] > bd[1]['total'] else 0
  +        elif 'team_rewards' in info:
  +            tr = info['team_rewards']
  +            entry['team0_total'] = tr[0]
  +            entry['team1_total'] = tr[1]
  +            entry['win_team0'] = 1 if tr[0] > tr[1] else 0
  +        results.append(entry)
  +
  +    df = pd.DataFrame(results)
  +    win_rate = float(df['win_team0'].mean()) if 'win_team0' in df else 0.0
  +    means = {k: float(v) for k, v in df.mean(numeric_only=True).to_dict().items()}
  +    vars_ = {k: float(v) for k, v in df.var(numeric_only=True).to_dict().items()}
  +    summary = {'games': games, 'win_rate_team0': win_rate, 'means': means, 'vars': vars_}
  +    return df, summary
  +
  +
  +def main():
  +    ap = argparse.ArgumentParser(description='Benchmark AC Actor/Critic with optional IS-MCTS')
  +    ap.add_argument('--mcts', action='store_true', help='Use IS-MCTS booster')
  +    ap.add_argument('--sims', type=int, default=128, help='MCTS simulations per move')
  +    ap.add_argument('--dets', type=int, default=16, help='Belief determinisations per search')
  +    ap.add_argument('--compact', action='store_true', help='Use compact observation')
  +    ap.add_argument('--k-history', type=int, default=12, help='Recent moves for compact observation')
  +    ap.add_argument('--ckpt', type=str, default='', help='Checkpoint path for actor/critic (optional)')
  +    ap.add_argument('--games', type=int, default=50, help='Number of games to play')
  +    ap.add_argument('--seed', type=int, default=0, help='Random seed')
  +    ap.add_argument('--c-puct', type=float, default=1.0, help='PUCT exploration constant')
  +    ap.add_argument('--root-temp', type=float, default=0.0, help='Root temperature for visit-based sampling')
  +    ap.add_argument('--prior-smooth-eps', type=float, default=0.0, help='Prior smoothing epsilon')
  +    ap.add_argument('--belief-particles', type=int, default=256, help='Belief particles')
  +    ap.add_argument('--belief-ess-frac', type=float, default=0.5, help='Belief ESS fraction')
  +    ap.add_argument('--robust-child', action='store_true', help='Use robust child (max visits) else max-Q')
  +    ap.add_argument('--root-dirichlet-alpha', type=float, default=0.0, help='Root Dirichlet alpha')
  +    ap.add_argument('--root-dirichlet-eps', type=float, default=0.0, help='Root Dirichlet epsilon')
  +    ap.add_argument('--out-csv', type=str, default='', help='Path to save per-game CSV report')
  +    ap.add_argument('--out-json', type=str, default='', help='Path to save summary JSON report')
  +    args = ap.parse_args()
  +
  +    df, summary = run_benchmark(games=args.games, use_mcts=args.mcts, sims=args.sims, dets=args.dets,
  +                                compact=args.compact, k_history=args.k_history, ckpt_path=args.ckpt, seed=args.seed,
  +                                c_puct=args.c_puct, root_temp=args.root_temp, prior_smooth_eps=args.prior_smooth_eps,
  +                                belief_particles=args.belief_particles, belief_ess_frac=args.belief_ess_frac,
  +                                robust_child=args.robust_child,
  +                                root_dirichlet_alpha=args.root_dirichlet_alpha, root_dirichlet_eps=args.root_dirichlet_eps)
  +    print('Summary:', summary)
  +    if args.out_csv:
  +        df.to_csv(args.out_csv, index=False)
  +        print('Saved CSV to', args.out_csv)
  +    if args.out_json:
  +        with open(args.out_json, 'w') as f:
  +            json.dump(summary, f, indent=2)
  +        print('Saved JSON to', args.out_json)
  +
  +
  +if __name__ == '__main__':
  +    main()
  +
-- 
diff --cc tools/profile_ppo.py
index 8eb42d3,8eb42d3,0000000..3b56845
mode 100644,100644,000000..100644
--- a/tools/profile_ppo.py
+++ b/tools/profile_ppo.py
@@@@ -1,1310 -1,1310 -1,0 +1,1313 @@@@
  +import os
  +import sys
  +import argparse
  +import threading
  +import torch as _torch
  +
  +# Ensure project root on sys.path for module imports
  +ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))
  +if ROOT not in sys.path:
  +    sys.path.insert(0, ROOT)
  +
  +_train_dev = os.environ.get('SCOPONE_TRAIN_DEVICE', 'cpu')
  +if (_train_dev.startswith('cuda') and _torch.cuda.is_available()):
  +    _n_threads = int(os.environ.get('SCOPONE_TRAIN_THREADS', '2'))
  +    _n_interop = int(os.environ.get('SCOPONE_TRAIN_INTEROP_THREADS', '1'))
  +else:
  +    _cores = int(max(1, (os.cpu_count() or 1)))
  +    _target = max(1, int(_cores * 0.60))
  +    _n_threads = int(os.environ.get('SCOPONE_TRAIN_THREADS', str(_target)))
  +    _n_interop_default = max(1, _n_threads // 8)
  +    _n_interop = int(os.environ.get('SCOPONE_TRAIN_INTEROP_THREADS', str(_n_interop_default)))
  +
  +# Silence TensorFlow/absl noise before any heavy imports
  +os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '3')  # hide INFO/WARNING/ERROR from TF C++ logs
  +os.environ.setdefault('ABSL_LOGGING_MIN_LOG_LEVEL', '3')  # absl logs: only FATAL
  +os.environ.setdefault('TF_ENABLE_ONEDNN_OPTS', '0')  # disable oneDNN custom ops info spam
  +
  +# Abilita TensorBoard di default (override con SCOPONE_DISABLE_TB=1 per disattivarlo)
  +os.environ.setdefault('SCOPONE_DISABLE_TB', '0')
  +
  +## Abilita torch.compile di default per l'intero progetto (override via env)
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE', '0')
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE_MODE', 'reduce-overhead')
  +os.environ.setdefault('SCOPONE_TORCH_COMPILE_BACKEND', 'inductor')
  +os.environ.setdefault('SCOPONE_COMPILE_VERBOSE', '1')
  +
  +## Autotune controllabile: di default ON su CPU beneficia di fusioni; può essere disattivato via env
  +os.environ.setdefault('SCOPONE_INDUCTOR_AUTOTUNE', '1')
  +os.environ.setdefault('TORCHINDUCTOR_MAX_AUTOTUNE_GEMM', '0')
  +
  +## Evita graph break su .item() catturando scalari nei grafi
  +os.environ.setdefault('TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS', '1')
  +
  +## Abilita dynamic shapes per ridurre errori di symbolic shapes FX
  +os.environ.setdefault('TORCHDYNAMO_DYNAMIC_SHAPES', '0')
  +
  +## Alza il limite del cache di Dynamo per ridurre recompilazioni
  +os.environ.setdefault('TORCHDYNAMO_CACHE_SIZE_LIMIT', '32')
  +
  +## Non impostare TORCH_LOGS ad un valore invalido; lascia al default o definisci mapping esplicito se necessario
  +# Abilita e blocca i flag dell'osservazione all'avvio (usati da observation/environment al load)
  +# Se l'utente li ha già impostati nel proprio run, li rispettiamo (setdefault)
  +os.environ.setdefault('OBS_INCLUDE_DEALER', '1')
  +os.environ.setdefault('OBS_INCLUDE_INFERRED', '0')
+++os.environ.setdefault('OBS_INCLUDE_INFERRED_L2', '0')
+++os.environ.setdefault('OBS_INCLUDE_INFERRED_L3', '0')
  +os.environ.setdefault('OBS_INCLUDE_RANK_PROBS', '0')
  +os.environ.setdefault('OBS_INCLUDE_SCOPA_PROBS', '0')
  +
  +# Imposta ENV_DEVICE una sola volta coerente con SCOPONE_DEVICE o disponibilità CUDA
  +os.environ.setdefault('SCOPONE_DEVICE', 'cpu')
  +os.environ.setdefault('ENV_DEVICE', 'cpu')
  +
  +# Training compute device (models stay on CPU during env collection; moved only inside update)
  +os.environ.setdefault('SCOPONE_TRAIN_DEVICE', 'cpu')
  +
  +# Enable approximate GELU and gate all runtime checks via a single flag
  +os.environ.setdefault('SCOPONE_APPROX_GELU', '1')
  +os.environ.setdefault('SCOPONE_STRICT_CHECKS', '0')
  +os.environ.setdefault('SCOPONE_PROFILE', '0')
  +
  +# Additional trainer/eval tunables exposed via environment (defaults; override as needed)
  +os.environ.setdefault('SCOPONE_PAR_DEBUG', '0')
  +os.environ.setdefault('SCOPONE_WORKER_THREADS', '1')
  +os.environ.setdefault('SCOPONE_TORCH_PROF', '0')
  +os.environ.setdefault('SCOPONE_TORCH_TB_DIR', '')
  +os.environ.setdefault('SCOPONE_RPC_TIMEOUT_S', '30')
  +os.environ.setdefault('SCOPONE_RAISE_ON_INVALID_SIMS', '0')
  +os.environ.setdefault('SCOPONE_EP_PUT_TIMEOUT_S', '15')
  +os.environ.setdefault('SCOPONE_TORCH_PROF_DIR', 'profiles')
  +os.environ.setdefault('SCOPONE_RAISE_ON_CKPT_FAIL', '0')
  +os.environ.setdefault('ENABLE_BELIEF_SUMMARY', '0')
+++os.environ.setdefault('SCOPONE_BELIEF_BLEND_ALPHA', '0.65')
  +os.environ.setdefault('DET_NOISE', '0.0')
  +os.environ.setdefault('SCOPONE_COLLECT_MIN_BATCH', '0')
  +os.environ.setdefault('SCOPONE_COLLECT_MAX_LATENCY_MS', '3.0')
  +os.environ.setdefault('SCOPONE_COLLECTOR_STALL_S', '30')
  +
  +# Gameplay/training topology flags
  +os.environ.setdefault('SCOPONE_START_OPP', os.environ.get('SCOPONE_START_OPP', 'top1'))
  +
  +# Evaluation process knobs
  +os.environ.setdefault('SCOPONE_EVAL_DEBUG', '0')
  +os.environ.setdefault('SCOPONE_EVAL_MP_START', os.environ.get('SCOPONE_MP_START', 'forkserver'))
  +os.environ.setdefault('SCOPONE_EVAL_POOL_TIMEOUT_S', '0')
  +os.environ.setdefault('SCOPONE_ELO_DIFF_SCALE', '6.0')
  +
  +# TQDM_DISABLE: 1=disattiva tutte le barre/logging di tqdm; 0=abilitato
  +os.environ.setdefault('TQDM_DISABLE', '0')
  +## SCOPONE_PER_ENV_TQDM: 1=mostra barre per-env; 0=nascondi barre per-env (lascia barra globale)
  +os.environ.setdefault('SCOPONE_PER_ENV_TQDM', os.environ.get('SCOPONE_PER_ENV_TQDM', '0'))
  +
  +# SELFPLAY: 1=single net (self-play), 0=dual nets (Team A/B)
  +_selfplay_env = str(os.environ.get('SCOPONE_SELFPLAY', '1')).strip().lower()
  +_selfplay = (_selfplay_env in ['1', 'true', 'yes', 'on'])
  +
  +# SCOPONE_OPP_FROZEN: 1=freeze the opponent, 0=co-train with the opponent
  +_opp_frozen = os.environ.get('SCOPONE_OPP_FROZEN', '0') in ['1', 'true', 'yes', 'on']
  +
  +# SCOPONE_TRAIN_FROM_BOTH_TEAMS: effective ONLY when SELFPLAY=1 and OPP_FROZEN=0.
  +# Uses transitions from both teams for the single net; otherwise ignored (on-policy).
  +_tfb = os.environ.get('SCOPONE_TRAIN_FROM_BOTH_TEAMS', '0') in ['1', 'true', 'yes', 'on']
  +
  +# Warm-start policy controlled by SCOPONE_WARM_START: '0' start-from-scratch, '1' force top1 clone, '2' use top2 if available
  +os.environ.setdefault('SCOPONE_WARM_START', '2')
  +
  +# SCOPONE_ALTERNATE_ITERS: in dual-nets+frozen, train A for N iters then swap to B (and vice versa)
  +os.environ.setdefault('SCOPONE_ALTERNATE_ITERS', '1')
  +
  +# SCOPONE_FROZEN_UPDATE_EVERY: in selfplay+frozen, refresh the shadow (frozen) opponent every N iters
  +os.environ.setdefault('SCOPONE_FROZEN_UPDATE_EVERY', '1')
  +
  +# Refresh League from disk at startup (scan checkpoints/). 1=ON, 0=OFF
  +os.environ.setdefault('SCOPONE_LEAGUE_REFRESH', '0')
  +
  +# Parallel eval workers: 1=serial, >1 parallel via multiprocessing
  +os.environ.setdefault('SCOPONE_EVAL_WORKERS', str(max(1, (os.cpu_count() or 1)//2)))
  +
  +# Training flags (manual overrides available via env)
  +_save_every = int(os.environ.get('SCOPONE_SAVE_EVERY', '10'))
  +os.environ.setdefault('SCOPONE_MINIBATCH', '4096')
  +
  +# Checkpoint path control
  +os.environ.setdefault('SCOPONE_CKPT', 'checkpoints/ppo_ac.pth')
  +
  +# Default to random seed for training runs (set -1); stable only if user sets it
  +seed_env = int(os.environ.get('SCOPONE_SEED', '-1'))
  +
  +# Allow configuring iterations/horizon/num_envs via env; sensible defaults
  +iters = int(os.environ.get('SCOPONE_ITERS', '1'))
  +horizon = int(os.environ.get('SCOPONE_HORIZON', '16384'))
  +_DEFAULT_NUM_ENVS = int(os.environ.get('SCOPONE_NUM_ENVS', '1'))
  +
  +# Read checkpoint path from env for training
  +ckpt_path_env = os.environ.get('SCOPONE_CKPT', 'checkpoints/ppo_ac.pth')
  +
  +_mcts_warmup_iters = int(os.environ.get('SCOPONE_MCTS_WARMUP_ITERS', '500'))
  +
  +# MCTS eval flags
  +_eval_every = int(os.environ.get('SCOPONE_EVAL_EVERY', '10'))
  +_eval_c_puct = float(os.environ.get('SCOPONE_EVAL_MCTS_C_PUCT', '1.0'))
  +_eval_root_temp = float(os.environ.get('SCOPONE_EVAL_MCTS_ROOT_TEMP', '0.0'))
  +_eval_prior_eps = float(os.environ.get('SCOPONE_EVAL_MCTS_PRIOR_SMOOTH_EPS', '0.0'))
  +_eval_dir_alpha = float(os.environ.get('SCOPONE_EVAL_MCTS_DIRICHLET_ALPHA', '0.25'))
  +_eval_dir_eps = float(os.environ.get('SCOPONE_EVAL_MCTS_DIRICHLET_EPS', '0.25'))
  +_eval_belief_particles = int(os.environ.get('SCOPONE_EVAL_BELIEF_PARTICLES', '0'))
  +_eval_belief_ess = float(os.environ.get('SCOPONE_EVAL_BELIEF_ESS_FRAC', '0.5'))
  +_eval_use_mcts = os.environ.get('SCOPONE_EVAL_USE_MCTS', '0').lower() in ['1', 'true', 'yes', 'on']
  +_eval_mcts_sims = int(os.environ.get('SCOPONE_EVAL_MCTS_SIMS', '128'))
  +_eval_mcts_dets = int(os.environ.get('SCOPONE_EVAL_MCTS_DETS', '1'))
  +_eval_kh = int(os.environ.get('SCOPONE_EVAL_K_HISTORY', '39'))
  +_eval_games = int(os.environ.get('SCOPONE_EVAL_GAMES', '1000'))
  +
  +# Training config flags (from env)
  +_entropy_sched = os.environ.get('SCOPONE_ENTROPY_SCHED', 'linear')
  +_belief_particles = int(os.environ.get('SCOPONE_BELIEF_PARTICLES', '512'))
  +_belief_ess = float(os.environ.get('SCOPONE_BELIEF_ESS_FRAC', '0.5'))
  +_mcts_c_puct = float(os.environ.get('SCOPONE_MCTS_C_PUCT', '1.0'))
  +_mcts_root_temp = float(os.environ.get('SCOPONE_MCTS_ROOT_TEMP', '0.0'))
  +_mcts_prior_eps = float(os.environ.get('SCOPONE_MCTS_PRIOR_SMOOTH_EPS', '0.0'))
  +_mcts_dir_alpha = float(os.environ.get('SCOPONE_MCTS_DIRICHLET_ALPHA', '0.25'))
  +_mcts_dir_eps = float(os.environ.get('SCOPONE_MCTS_DIRICHLET_EPS', '0.25'))
  +_mcts_train = os.environ.get('SCOPONE_MCTS_TRAIN', '0') in ['1', 'true', 'yes', 'on']
  +_mcts_sims = int(os.environ.get('SCOPONE_MCTS_SIMS', '128'))
  +_mcts_dets = int(os.environ.get('SCOPONE_MCTS_DETS', '4'))
  +
  +# Targeted FD-level stderr filter to drop absl/TF CUDA registration warnings from C++
  +_SILENCE_ABSL = os.environ.get('SCOPONE_SILENCE_ABSL', '1') == '1'
  +if os.environ.get('SCALENE_RUNNING', '0') == '1':  # scalene needs raw stderr
  +    _SILENCE_ABSL = False
  +if _SILENCE_ABSL:
  +    _SUPPRESS_SUBSTRINGS = (
  +        "All log messages before absl::InitializeLog() is called are written to STDERR",
  +        "Unable to register cuDNN factory",
  +        "Unable to register cuBLAS factory",
  +        "cuda_dnn.cc",
  +        "cuda_blas.cc",
  +    )
  +
  +    _orig_fd2 = os.dup(2)
  +    _r_fd, _w_fd = os.pipe()
  +    os.dup2(_w_fd, 2)
  +
  +    def _stderr_reader(r_fd, orig_fd, suppressed):
  +        with os.fdopen(r_fd, 'rb', buffering=0) as r:
  +            buffer = b""
  +            while True:
  +                chunk = r.read(1024)
  +                if not chunk:
  +                    break
  +                buffer += chunk
  +                while b"\n" in buffer:
  +                    line, buffer = buffer.split(b"\n", 1)
  +                    txt = line.decode('utf-8', errors='ignore')
  +                    if not any(s in txt for s in suppressed):
  +                        os.write(orig_fd, line + b"\n")
  +            if buffer:
  +                txt = buffer.decode('utf-8', errors='ignore')
  +                if not any(s in txt for s in suppressed):
  +                    os.write(orig_fd, buffer)
  +
  +    _t = threading.Thread(target=_stderr_reader, args=(_r_fd, _orig_fd2, _SUPPRESS_SUBSTRINGS), daemon=True)
  +    _t.start()
  +
  +os.environ.setdefault('ENV_DEVICE', os.environ.get('SCOPONE_DEVICE', 'cpu'))
  +## Imposta metodo mp sicuro per CUDA: forkserver (override con SCOPONE_MP_START)
  +os.environ.setdefault('SCOPONE_MP_START', 'forkserver')
  +
  +import io
  +import torch
  +import subprocess
  +import webbrowser
  +from datetime import datetime
  +import platform
  +
  +# Default CPU for profiling unless user sets GPU via env
  +
  +from trainers.train_ppo import train_ppo
  +from utils.seed import resolve_seed
  +
  +
  +def _resolve_num_envs(args, clamp: int | None = None) -> int:
  +    raw = getattr(args, 'num_envs', None)
  +    if raw is None:
  +        val = _DEFAULT_NUM_ENVS
  +    else:
  +        try:
  +            val = int(raw)
  +        except (TypeError, ValueError):
  +            val = _DEFAULT_NUM_ENVS
  +    if clamp is not None:
  +        val = min(val, clamp)
  +    return max(1, val)
  +
  +
  +def main():
  +    parser = argparse.ArgumentParser(description='Profile short PPO run (torch or line-level).')
  +    parser.add_argument('--iters', type=int, default=iters, help='Iterations to run (default from SCOPONE_ITERS)')
  +    parser.add_argument('--horizon', type=int, default=horizon, help='Rollout horizon per iteration (default from SCOPONE_HORIZON)')
  +    parser.add_argument('--line', dest='line', action='store_true', default=False, help='Enable line-by-line profiler with per-line timings (default: on)')
  +    parser.add_argument('--no-line', dest='line', action='store_false', help='Disable line-by-line profiler')
  +    parser.add_argument('--wrap-update', dest='wrap_update', action='store_true', default=True, help='Also profile ActionConditionedPPO.update (default: on; slower)')
  +    parser.add_argument('--no-wrap-update', dest='wrap_update', action='store_false', help='Disable profiling of ActionConditionedPPO.update')
  +    parser.add_argument('--report', action='store_true', help='Print extended line-profiler report')
  +    parser.add_argument('--num-envs', type=int, default=_DEFAULT_NUM_ENVS, help='Number of parallel environments (default from SCOPONE_NUM_ENVS)')
  +    # Line-profiler scope controls
  +    parser.add_argument('--add-func', action='append', default=[], help='Qualified function or method to include, e.g. pkg.mod:Class.method or algorithms.ppo_ac:ActionConditionedPPO.update')
  +    parser.add_argument('--add-module', action='append', default=[], help='Module to include all functions from, e.g. algorithms.ppo_ac or environment')
  +    parser.add_argument('--profile-all', dest='profile_all', action='store_true', default=False, help='Profile all modules under project root (heavy)')
  +    parser.add_argument('--no-profile-all', dest='profile_all', action='store_false', help=argparse.SUPPRESS)
  +    parser.add_argument('--line-full', dest='line_full', action='store_true', default=False, help='Show all per-line rows (not just top 30)')
  +    parser.add_argument('--line-csv', dest='line_csv', type=str, default=None, help='Path to write full per-line CSV (file, line, hits, cpu_s, gpu_s, transfer_s)')
  +    parser.add_argument('--line-self-only', dest='line_self_only', action='store_true', default=False, help='Use only self CPU time (drop lines without self time)')
  +    parser.add_argument('--cprofile', action='store_true', default=False, help='Use Python cProfile instead of torch or line-profiler')
  +    parser.add_argument('--cprofile-out', type=str, default=None, help='Output path for cProfile stats file (.prof). Default: timestamped file')
  +    parser.add_argument('--snakeviz', dest='snakeviz', action='store_true', default=True, help='Open SnakeViz on the generated .prof (default: on)')
  +    parser.add_argument('--no-snakeviz', dest='snakeviz', action='store_false', help='Do not open SnakeViz after profiling')
  +    parser.add_argument('--scalene', action='store_true', default=False, help='Run training under Scalene (CLI by default)')
  +    parser.add_argument('--scalene-out', type=str, default=None, help='Output path base for Scalene report. If ends with .html, generate HTML; else CLI only. Default: timestamped base')
  +    parser.add_argument('--scalene-open', dest='scalene_open', action='store_true', default=True, help='Open Scalene HTML report in a browser (default: on)')
  +    parser.add_argument('--no-scalene-open', dest='scalene_open', action='store_false', help='Do not open Scalene report after profiling')
  +    parser.add_argument('--scalene-cli', dest='scalene_cli', action='store_true', default=True, help='Print Scalene text report to terminal (default: on)')
  +    parser.add_argument('--no-scalene-cli', dest='scalene_cli', action='store_false', help='Do not print Scalene text report to terminal')
  +    parser.add_argument('--scalene-cpu-only', dest='scalene_cpu_only', action='store_true', default=True, help='Limit Scalene to CPU profiling only (default: on)')
  +    parser.add_argument('--no-scalene-cpu-only', dest='scalene_cpu_only', action='store_false', help='Include memory/GPU metrics (disables CPU-only)')
  +    parser.add_argument('--scalene-gpu-modes', dest='scalene_gpu_modes', action='store_true', default=False, help='Attempt to enable per-process GPU accounting via scalene.set_nvidia_gpu_modes (default: off)')
  +    parser.add_argument('--no-scalene-gpu-modes', dest='scalene_gpu_modes', action='store_false', help='Do not attempt to set NVIDIA GPU modes for Scalene')
  +    parser.add_argument('--scalene-run', action='store_true', default=False, help=argparse.SUPPRESS)
  +    parser.add_argument('--torch-profiler', dest='torch_profiler', action='store_true', default=False, help='Use PyTorch profiler (no default)')
  +    args = parser.parse_args()
  +    # Default to random seed for profiling runs; allow override via env/CLI passthrough
  +    seed_env = int(os.environ.get('SCOPONE_SEED', '-1'))
  +    seed = resolve_seed(seed_env)
  +
  +    # Ensure default profiles directory exists for profiler outputs
  +    DEFAULT_PROFILES_DIR = os.path.abspath(os.path.join(ROOT, 'profiles'))
  +    try:
  +        os.makedirs(DEFAULT_PROFILES_DIR, exist_ok=True)
  +    except Exception:
  +        pass
  +
  +    # If requested, re-exec this script under Scalene to produce an HTML report.
  +    if getattr(args, 'scalene', False) and not getattr(args, 'scalene_run', False):
  +        script_path = os.path.abspath(__file__)
  +        # Determine output mode and HTML path based on --scalene-out
  +        want_html = False
  +        html_path = None
  +        if getattr(args, 'scalene_out', None):
  +            val = str(args.scalene_out).strip().lower()
  +            if val == 'html':
  +                want_html = True
  +                ts = datetime.now().strftime('%Y%m%d_%H%M%S')
  +                # Default directory: profiles/; default name: ppo_scalene_<timestamp>.html
  +                html_path = os.path.abspath(os.path.join(DEFAULT_PROFILES_DIR, f'ppo_scalene_{ts}.html'))
  +            elif val == 'cli':
  +                want_html = False
  +            else:
  +                _raw = os.path.abspath(args.scalene_out)
  +                _, ext = os.path.splitext(_raw)
  +                if ext.lower() == '.html':
  +                    want_html = True
  +                    html_path = _raw
  +        else:
  +            # No explicit output provided: default to HTML in profiles/
  +            want_html = True
  +            ts = datetime.now().strftime('%Y%m%d_%H%M%S')
  +            html_path = os.path.abspath(os.path.join(DEFAULT_PROFILES_DIR, f'ppo_scalene_{ts}.html'))
  +        # Ensure directory exists for HTML output
  +        if want_html and html_path:
  +            out_dir = os.path.dirname(html_path) or DEFAULT_PROFILES_DIR
  +            os.makedirs(out_dir, exist_ok=True)
  +        ne = _resolve_num_envs(args)
  +        scalene_cmd = [sys.executable, '-m', 'scalene']
  +        if getattr(args, 'scalene_cpu_only', True):
  +            scalene_cmd.append('--cpu-only')
  +        if getattr(args, 'scalene_gpu_modes', False):
  +            try:
  +                proc = subprocess.run([sys.executable, '-m', 'scalene.set_nvidia_gpu_modes'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
  +                if proc.returncode != 0:
  +                    is_wsl = ('WSL_INTEROP' in os.environ) or ('microsoft' in platform.release().lower())
  +                    if is_wsl:
  +                        print('Scalene GPU modes not supported on WSL2; skipping.')
  +                    else:
  +                        print('Failed to set NVIDIA GPU modes for Scalene (non-zero exit). Try: sudo python -m scalene.set_nvidia_gpu_modes')
  +            except FileNotFoundError:
  +                print('Scalene GPU modes helper not found. Update scalene to a recent version.')
  +            except Exception as e:
  +                print(f'Failed to set NVIDIA GPU modes for Scalene: {e}')
  +        # Decide HTML vs CLI
  +        if want_html and html_path:
  +            scalene_cmd += ['--html', '--reduced-profile', '--outfile', html_path]
  +        else:
  +            # CLI only; request CLI output explicitly
  +            scalene_cmd += ['--cli', '--cpu-percent-threshold', '0', '--malloc-threshold', '1000000000']
  +        # Include all modules (not only the executed file's dir) and restrict to project root
  +        scalene_cmd += ['--profile-all', '--profile-only', ROOT]
  +        scalene_cmd += [
  +            script_path,
  +            '--scalene-run',
  +            '--no-line',
  +            '--no-wrap-update',
  +            '--iters', str(max(0, args.iters)),
  +            '--horizon', str(max(40, args.horizon)),
  +            '--num-envs', str(ne),
  +        ]
  +        # propagate selfplay via env
  +        env = os.environ.copy()
  +        env['SCOPONE_SEED'] = str(seed)
  +        env['SCALENE_RUNNING'] = '1'
  +        env['SCOPONE_SILENCE_ABSL'] = '0'
  +        env['TQDM_DISABLE'] = env.get('TQDM_DISABLE','0')
  +        # propagate per-env policy
  +        if 'SCOPONE_PER_ENV_TQDM' not in env:
  +            env['SCOPONE_PER_ENV_TQDM'] = ('0' if int(ne) > 1 else '1')
  +        # read desired selfplay from env or default to ON; do not override here
  +        # keep SCOPONE_SELFPLAY as-is so outer env controls behavior
  +        print("Running under Scalene... this may add overhead.")
  +        try:
  +            # Stream output live while capturing for summary at the end
  +            proc = subprocess.Popen(scalene_cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)
  +            captured_lines = []
  +            assert proc.stdout is not None
  +            # Suppress frequent Scalene JSON validation/pydantic warnings and absl CUDA factory spam
  +            def _suppress_scalene_line(txt: str) -> bool:
  +                txt_low = txt.lower()
  +                if ('json failed validation' in txt_low) or ('validation error for' in txt_low):
  +                    return True
  +                if ('n_core_utilization' in txt_low) or ('scalenejsonschema' in txt_low):
  +                    return True
  +                if ('errors.pydantic.dev' in txt_low):
  +                    return True
  +                # Newer Scalene prints just the validation rows without headers
  +                if ('input should be less than or equal to 1' in txt_low) or ('less_than_equal' in txt_low):
  +                    return True
  +                # absl/CUDA registration noise
  +                if ('unable to register cudnn factory' in txt_low) or ('unable to register cublas factory' in txt_low):
  +                    return True
  +                if ('all log messages before absl::initializelog() is called are written to stderr'.lower() in txt_low):
  +                    return True
  +                return False
  +            for _ln in proc.stdout:
  +                if _suppress_scalene_line(_ln):
  +                    continue
  +                print(_ln, end='')
  +                captured_lines.append(_ln)
  +            proc.wait()
  +            full_cli_output = ''.join(captured_lines)
  +        except FileNotFoundError:
  +            print("Scalene is not installed. Install with: pip install scalene")
  +        except Exception as e:
  +            print(f"Failed to run Scalene: {e}")
  +        else:
  +            if 'full_cli_output' in locals() and full_cli_output:
  +                # Save full CLI output to file to avoid flooding terminal
  +                ts_cli = datetime.now().strftime('%Y%m%d_%H%M%S')
  +                cli_out_path = os.path.abspath(os.path.join(DEFAULT_PROFILES_DIR, f'ppo_scalene_{ts_cli}_cli.txt'))
  +                with open(cli_out_path, 'w', encoding='utf-8') as f:
  +                    f.write(full_cli_output)
  +                print("\nScalene CLI saved to:", cli_out_path)
  +
  +                # Print compact, line-profiler-like summary at the bottom
  +                print("\n===== Scalene — Compact summary (by file) =====")
  +                clean_output = full_cli_output  # ensure defined for fallback paths
  +                try:
  +                    import re as _re
  +
  +                    def _strip_ansi(txt: str) -> str:
  +                        return _re.sub(r"\x1B\[[0-?]*[ -/]*[@-~]", "", txt)
  +
  +                    def _to_rel(pth: str) -> str:
  +                        ap = os.path.abspath(pth)
  +                        if ROOT in ap:
  +                            return os.path.relpath(ap, ROOT)
  +                        return ap
  +
  +                    def _parse_secs(s: str) -> float:
  +                        s = s.strip()
  +                        total = 0.0
  +                        for m in _re.findall(r'(\d+(?:\.\d+)?)\s*([hms])', s):
  +                            val = float(m[0])
  +                            unit = m[1]
  +                            if unit == 'h':
  +                                total += val * 3600.0
  +                            elif unit == 'm':
  +                                total += val * 60.0
  +                            else:
  +                                total += val
  +                        if total == 0.0:
  +                            mmss = _re.match(r'(?:(\d+)m:)?(\d+(?:\.\d+)?)s', s)
  +                            if mmss:
  +                                mins = float(mmss.group(1) or '0')
  +                                secs = float(mmss.group(2))
  +                                total = mins * 60.0 + secs
  +                        return total
  +
  +                    clean_output = _strip_ansi(full_cli_output)
  +                    per_file = {}
  +                    for line in clean_output.splitlines():
  +                        if '.py: % of time' in line:
  +                            before, after = line.split(': % of time', 1)
  +                            file_abs = before.strip()
  +                            # seconds may be absent in some builds
  +                            m_pct = _re.search(r'=\s*([0-9]+(?:\.[0-9]+)?)%\s*(?:\(([^\)]*)\))?', after)
  +                            if not m_pct:
  +                                continue
  +                            pct = float(m_pct.group(1))
  +                            secs = _parse_secs(m_pct.group(2)) if (m_pct.lastindex and m_pct.lastindex >= 2) else 0.0
  +                            per_file[_to_rel(file_abs)] = {'seconds': secs, 'percent': pct}
  +
  +                    if per_file:
  +                        ranked = sorted(per_file.items(), key=lambda kv: kv[1]['seconds'], reverse=True)
  +                        for i, (fn, s) in enumerate(ranked[:30], 1):
  +                            print(f"{i:>2}. {fn}\n    Python: {s['seconds']:8.2f}s  ({s['percent']:5.1f}%)")
  +                        # Top lines across files (approximate absolute time via per-file totals * per-line percentage)
  +                        print("\n===== Scalene — Top lines (by total time) =====")
  +                        per_site = {}
  +                        current_file = None
  +                        current_file_secs = 0.0
  +                        for _line in clean_output.splitlines():
  +                            m_file2 = _re.match(r"^\s*(/.*?\.py):\s*% of time\s*=\s*([0-9.]+)%\s*(?:\(([^)]*)\))?", _line)
  +                            if m_file2:
  +                                current_file = _to_rel(m_file2.group(1).strip())
  +                                current_file_secs = _parse_secs(m_file2.group(3)) if (m_file2.lastindex and m_file2.lastindex >= 3) else 0.0
  +                                continue
  +                            if not current_file or current_file_secs <= 0.0:
  +                                continue
  +                            m_row = _re.match(r"^\s*(\d+)\s*[│|]\s*([^│|]*)[│|]([^│|]*)[│|]([^│|]*)[│|]", _line)
  +                            if not m_row:
  +                                continue
  +                            ln_no = int(m_row.group(1))
  +                            def _pct_to_float(txt: str) -> float:
  +                                t = txt.strip()
  +                                m = _re.search(r"([0-9]+(?:\.[0-9]+)?)%", t)
  +                                return float(m.group(1)) if m else 0.0
  +                            py_pct = _pct_to_float(m_row.group(2))
  +                            na_pct = _pct_to_float(m_row.group(3))
  +                            sy_pct = _pct_to_float(m_row.group(4))
  +                            total_pct = py_pct + na_pct + sy_pct
  +                            if total_pct <= 0.0:
  +                                continue
  +                            secs = current_file_secs * (total_pct / 100.0)
  +                            key = (current_file, ln_no)
  +                            agg = per_site.get(key)
  +                            if not agg:
  +                                per_site[key] = {'seconds': 0.0, 'py_pct': 0.0, 'na_pct': 0.0, 'sy_pct': 0.0}
  +                                agg = per_site[key]
  +                            agg['seconds'] += secs
  +                            agg['py_pct'] += py_pct
  +                            agg['na_pct'] += na_pct
  +                            agg['sy_pct'] += sy_pct
  +                        if per_site:
  +                            ranked_sites = sorted(per_site.items(), key=lambda kv: kv[1]['seconds'], reverse=True)
  +                            for i, ((fn, ln_no), svals) in enumerate(ranked_sites[:30], 1):
  +                                print(f"{i:>2}. {fn}:{ln_no}  total: {svals['seconds']:8.2f}s  (py {svals['py_pct']:5.1f}%, nat {svals['na_pct']:5.1f}%, sys {svals['sy_pct']:5.1f}%)")
  +                        else:
  +                            # Fallback: parse 'function summary' blocks and attribute function % to definition line
  +                            per_site2 = {}
  +                            last_file = None
  +                            last_file_secs = 0.0
  +                            in_func_summary = False
  +                            for _line in clean_output.splitlines():
  +                                mf = _re.match(r"^\s*(/.*?\.py):\s*% of time\s*=\s*([0-9.]+)%\s*(?:\(([^)]*)\))?", _line)
  +                                if mf:
  +                                    last_file = _to_rel(mf.group(1).strip())
  +                                    last_file_secs = _parse_secs(mf.group(3)) if (mf.lastindex and mf.lastindex >= 3) else 0.0
  +                                    in_func_summary = False
  +                                    continue
  +                                if 'function summary for' in _line:
  +                                    in_func_summary = True
  +                                    continue
  +                                if in_func_summary:
  +                                    mfr = _re.match(r"^\s*(\d+)\s*[│|]\s*([^│|]*)[│|]([^│|]*)[│|]([^│|]*)[│|]", _line)
  +                                    if mfr and last_file and last_file_secs > 0.0:
  +                                        ln_no = int(mfr.group(1))
  +                                        def _pctf(txt: str) -> float:
  +                                            mm = _re.search(r"([0-9]+(?:\.[0-9]+)?)%", txt.strip())
  +                                            return float(mm.group(1)) if mm else 0.0
  +                                        py = _pctf(mfr.group(2)); na = _pctf(mfr.group(3)); sy = _pctf(mfr.group(4))
  +                                        tot = py + na + sy
  +                                        if tot <= 0.0:
  +                                            continue
  +                                        secs = last_file_secs * (tot / 100.0)
  +                                        key = (last_file, ln_no)
  +                                        agg = per_site2.get(key)
  +                                        if not agg:
  +                                            per_site2[key] = {'seconds': 0.0, 'py_pct': 0.0, 'na_pct': 0.0, 'sy_pct': 0.0}
  +                                            agg = per_site2[key]
  +                                        agg['seconds'] += secs
  +                                        agg['py_pct'] += py
  +                                        agg['na_pct'] += na
  +                                        agg['sy_pct'] += sy
  +                                    else:
  +                                        # Heuristic end of block
  +                                        if _line.strip().startswith('=====') or '.py:' in _line:
  +                                            in_func_summary = False
  +                            if per_site2:
  +                                ranked_sites = sorted(per_site2.items(), key=lambda kv: kv[1]['seconds'], reverse=True)
  +                                for i, ((fn, ln_no), svals) in enumerate(ranked_sites[:30], 1):
  +                                    print(f"{i:>2}. {fn}:{ln_no}  total: {svals['seconds']:8.2f}s  (py {svals['py_pct']:5.1f}%, nat {svals['na_pct']:5.1f}%, sys {svals['sy_pct']:5.1f}%)")
  +                            else:
  +                                print("(no per-line/function rows parsed; try increasing iters/horizon)")
  +                    else:
  +                        # Fallback: print last 80 lines of raw output
  +                        tail = '\n'.join(clean_output.splitlines()[-80:])
  +                        print(tail)
  +                except Exception:
  +                    # On parser error, still show the tail of cleaned output or raw output
  +                    tail = '\n'.join((clean_output or full_cli_output).splitlines()[-80:])
  +                    print(tail)
  +
  +            if want_html and html_path:
  +                print(f"\nScalene HTML report: {html_path}")
  +                if getattr(args, 'scalene_open', True):
  +                    url = 'file://' + html_path if not html_path.startswith('file://') else html_path
  +                    opened = webbrowser.open(url)
  +                    if not opened:
  +                        subprocess.Popen(['xdg-open', html_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
  +                print("If it didn't open, open the HTML file manually in your browser.")
  +        return
  +
  +    # Inner run invoked by Scalene: execute training without additional profilers.
  +    if getattr(args, 'scalene_run', False):
  +        num_envs_eff = _resolve_num_envs(args)
  +        _selfplay_env = str(os.environ.get('SCOPONE_SELFPLAY', '1')).strip().lower()
  +        _selfplay = (_selfplay_env in ['1', 'true', 'yes', 'on'])
  +        # Training flags for profiling as well
  +        _save_every = int(os.environ.get('SCOPONE_SAVE_EVERY','10'))
  +        _entropy_sched = os.environ.get('SCOPONE_ENTROPY_SCHED','linear')
  +        _belief_particles = int(os.environ.get('SCOPONE_BELIEF_PARTICLES','512'))
  +        _belief_ess = float(os.environ.get('SCOPONE_BELIEF_ESS_FRAC','0.5'))
  +        _mcts_train = os.environ.get('SCOPONE_MCTS_TRAIN','0') in ['1','true','yes','on']
  +        _mcts_sims = int(os.environ.get('SCOPONE_MCTS_SIMS','128'))
  +        _mcts_dets = int(os.environ.get('SCOPONE_MCTS_DETS','4'))
  +        _mcts_c_puct = float(os.environ.get('SCOPONE_MCTS_C_PUCT','1.0'))
  +        _mcts_root_temp = float(os.environ.get('SCOPONE_MCTS_ROOT_TEMP','0.0'))
  +        _mcts_prior_eps = float(os.environ.get('SCOPONE_MCTS_PRIOR_SMOOTH_EPS','0.0'))
  +        _mcts_dir_alpha = float(os.environ.get('SCOPONE_MCTS_DIRICHLET_ALPHA','0.25'))
  +        _mcts_dir_eps = float(os.environ.get('SCOPONE_MCTS_DIRICHLET_EPS','0.25'))
  +        _eval_games = int(os.environ.get('SCOPONE_EVAL_GAMES','1000'))
  +        train_ppo(num_iterations=max(0, args.iters), horizon=max(40, args.horizon), k_history=39, num_envs=num_envs_eff,
  +                  mcts_sims=_mcts_sims, mcts_sims_eval=0, save_every=_save_every,
  +                  entropy_schedule_type=_entropy_sched,
  +                  eval_every=0, eval_games=_eval_games, mcts_in_eval=False, seed=seed, use_selfplay=_selfplay,
  +                  mcts_warmup_iters=_mcts_warmup_iters)
  +        return
  +
  +    # cProfile mode takes precedence over line/torch profiler
  +    if getattr(args, 'cprofile', False):
  +        import cProfile
  +        import pstats
  +
  +        prof = cProfile.Profile()
  +        # Honor training/profile env config (consistent with other modes)
  +        _save_every = int(os.environ.get('SCOPONE_SAVE_EVERY','10'))
  +        _entropy_sched = os.environ.get('SCOPONE_ENTROPY_SCHED','linear')
  +        _belief_particles = int(os.environ.get('SCOPONE_BELIEF_PARTICLES','512'))
  +        _belief_ess = float(os.environ.get('SCOPONE_BELIEF_ESS_FRAC','0.5'))
  +        _mcts_train = os.environ.get('SCOPONE_MCTS_TRAIN','0') in ['1','true','yes','on']
  +        _mcts_sims = int(os.environ.get('SCOPONE_MCTS_SIMS','128'))
  +        _mcts_dets = int(os.environ.get('SCOPONE_MCTS_DETS','4'))
  +        _mcts_c_puct = float(os.environ.get('SCOPONE_MCTS_C_PUCT','1.0'))
  +        _mcts_root_temp = float(os.environ.get('SCOPONE_MCTS_ROOT_TEMP','0.0'))
  +        _mcts_prior_eps = float(os.environ.get('SCOPONE_MCTS_PRIOR_SMOOTH_EPS','0.0'))
  +        _mcts_dir_alpha = float(os.environ.get('SCOPONE_MCTS_DIRICHLET_ALPHA','0.25'))
  +        _mcts_dir_eps = float(os.environ.get('SCOPONE_MCTS_DIRICHLET_EPS','0.25'))
  +        _eval_games = int(os.environ.get('SCOPONE_EVAL_GAMES','1000'))
  +
  +        num_envs_eff = _resolve_num_envs(args)
  +
  +        def _run():
  +            _selfplay_env = str(os.environ.get('SCOPONE_SELFPLAY', '1')).strip().lower()
  +            _selfplay = (_selfplay_env in ['1', 'true', 'yes', 'on'])
  +            train_ppo(num_iterations=max(0, args.iters), horizon=max(40, args.horizon), k_history=39, num_envs=num_envs_eff,
  +                      save_every=_save_every,
  +                      entropy_schedule_type=_entropy_sched,
  +                      belief_particles=_belief_particles, belief_ess_frac=_belief_ess,
  +                      mcts_in_eval=False, mcts_train=_mcts_train,
  +                      mcts_sims=_mcts_sims, mcts_sims_eval=0, mcts_dets=_mcts_dets, mcts_c_puct=_mcts_c_puct,
  +                      mcts_root_temp=_mcts_root_temp, mcts_prior_smooth_eps=_mcts_prior_eps,
  +                      mcts_dirichlet_alpha=_mcts_dir_alpha, mcts_dirichlet_eps=_mcts_dir_eps,
  +                      eval_every=0, eval_games=_eval_games,
  +                      seed=seed, use_selfplay=_selfplay,
  +                      mcts_warmup_iters=_mcts_warmup_iters)
  +
  +        prof.enable()
  +        try:
  +            _run()
  +        finally:
  +            prof.disable()
  +
  +        if getattr(args, 'cprofile_out', None):
  +            out_path = args.cprofile_out
  +        else:
  +            ts = datetime.now().strftime('%Y%m%d_%H%M%S')
  +            out_path = os.path.abspath(os.path.join(DEFAULT_PROFILES_DIR, f'ppo_profile_{ts}.prof'))
  +        try:
  +            prof.dump_stats(out_path)
  +            print(f"\ncProfile stats written to: {out_path}")
  +        except Exception as e:
  +            print(f"Failed to write cProfile stats: {e}")
  +
  +        # Print concise summaries (top by cumulative and self time)
  +        try:
  +            import io as _io
  +            s1 = _io.StringIO()
  +            ps = pstats.Stats(prof, stream=s1).sort_stats('cumtime')
  +            ps.print_stats(30)
  +            print("\nTop functions by cumulative time (cumtime):\n" + s1.getvalue())
  +
  +            s2 = _io.StringIO()
  +            pstats.Stats(prof, stream=s2).sort_stats('tottime').print_stats(30)
  +            print("\nTop functions by self time (tottime):\n" + s2.getvalue())
  +        except Exception as e:
  +            print(f"Failed to print cProfile summary: {e}")
  +
  +        # Optionally launch SnakeViz
  +        if getattr(args, 'snakeviz', False):
  +            try:
  +                # Prefer python -m snakeviz to avoid PATH issues
  +                subprocess.Popen([sys.executable, '-m', 'snakeviz', out_path], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
  +                print("Launched SnakeViz in background. If not opening, run: snakeviz " + out_path)
  +            except Exception as e:
  +                print(f"Could not launch SnakeViz automatically: {e}\nInstall with: pip install snakeviz\nThen run: snakeviz {out_path}")
  +        return
  +
  +    if args.line:
  +        # Lightweight line-by-line profiling for Python code with file:line output
  +        from profilers.line_profiler import profile as line_profile, global_profiler
  +        # Helper to preserve original callable signatures for the type checker
  +        from typing import Any, Callable, TypeVar, cast
  +        _F = TypeVar('_F', bound=Callable[..., object])
  +        def _lp(fn: _F) -> _F:
  +            return cast(_F, line_profile(fn))
  +
  +        # Optional: register additional targets for profiling before running training
  +        def _resolve_attr(path: str):
  +            import importlib as _importlib
  +            if ':' in path:
  +                mod_path, attr_path = path.split(':', 1)
  +            else:
  +                parts = path.split('.')
  +                for i in range(len(parts), 0, -1):
  +                    mod_path = '.'.join(parts[:i])
  +                    _importlib.import_module(mod_path)
  +                    attr_path = '.'.join(parts[i:])
  +                    break
  +                else:
  +                    mod_path, attr_path = path, ''
  +            mod = _importlib.import_module(mod_path)
  +            if not attr_path:
  +                return mod
  +            obj = mod
  +            for name in attr_path.split('.'):
  +                obj = getattr(obj, name)
  +            return obj
  +
  +        def _iter_module_functions(module):
  +            import inspect as _inspect
  +            for name, obj in _inspect.getmembers(module):
  +                if _inspect.isfunction(obj) and getattr(obj, '__module__', None) == module.__name__:
  +                    yield obj
  +                if _inspect.isclass(obj) and getattr(obj, '__module__', None) == module.__name__:
  +                    for _, member in _inspect.getmembers(obj):
  +                        if _inspect.isfunction(member) and getattr(member, '__qualname__', '').startswith(obj.__name__ + '.'):
  +                            yield member
  +
  +        def _iter_package_functions(pkg_module):
  +            import pkgutil as _pkgutil
  +            import importlib as _importlib
  +            for _finder, _name, _is_pkg in _pkgutil.walk_packages(pkg_module.__path__, pkg_module.__name__ + "."):
  +                submod = _importlib.import_module(_name)
  +                for fn in _iter_module_functions(submod):
  +                    yield fn
  +
  +        # Register user-specified functions and modules
  +        any_registered = False
  +        if getattr(args, 'add_func', None):
  +            for spec in (args.add_func or []):
  +                obj = _resolve_attr(spec)
  +                if obj is None:
  +                    continue
  +                line_profile(getattr(obj, '__func__', obj))
  +                any_registered = True
  +        if getattr(args, 'add_module', None):
  +            for mod_spec in (args.add_module or []):
  +                mod = _resolve_attr(mod_spec)
  +                if mod is None:
  +                    continue
  +                for fn in _iter_module_functions(mod):
  +                    line_profile(fn)
  +                    any_registered = True
  +
  +        # If requested, register most project modules/functions automatically (heavy)
  +        if getattr(args, 'profile_all', False):
  +            import importlib as _importlib
  +            default_targets = [
  +                'environment',
  +                'observation',
  +                'benchmark',
  +                'main',
  +                'algorithms',
  +                'models',
  +                'trainers',
  +                'evaluation',
  +            ]
  +            for tgt in default_targets:
  +                mod = _importlib.import_module(tgt)
  +                # Package: recurse; Module: shallow
  +                if hasattr(mod, '__path__'):
  +                    for fn in _iter_package_functions(mod):
  +                        line_profile(fn)
  +                        any_registered = True
  +        # Wrap hotspots and also enable global tracing fallback
  +        import trainers.train_ppo as train_mod
  +        train_mod.collect_trajectory = _lp(train_mod.collect_trajectory)
  +        train_fn = _lp(train_mod.train_ppo)
  +        # Trainer data path internals
  +        train_mod.collect_trajectory_parallel = _lp(train_mod.collect_trajectory_parallel)
  +        train_mod._batched_select_indices = _lp(train_mod._batched_select_indices)
  +        train_mod._batched_select_indices_with_actor = _lp(train_mod._batched_select_indices_with_actor)
  +        train_mod._batched_service = _lp(train_mod._batched_service)
  +        # Worker loop
  +        if hasattr(train_mod, '_env_worker'):
  +            train_mod._env_worker = _lp(train_mod._env_worker)
  +        # Trainer parallel/batching internals
  +        if hasattr(train_mod, 'collect_trajectory_parallel'):
  +            train_mod.collect_trajectory_parallel = _lp(train_mod.collect_trajectory_parallel)
  +        for _name in ['_batched_select_indices', '_batched_select_indices_with_actor', '_batched_service']:
  +            if hasattr(train_mod, _name):
  +                setattr(train_mod, _name, _lp(getattr(train_mod, _name)))
  +
  +        # Always profile eval, environment and MCTS stack by default under --line
  +        import evaluation.eval as eval_mod
  +        eval_mod.evaluate_pair_actors = _lp(eval_mod.evaluate_pair_actors)
  +        eval_mod.play_match = _lp(eval_mod.play_match)
  +        # Environment hotspots
  +        import environment as env_mod
  +        if hasattr(env_mod, 'ScoponeEnvMA'):
  +            _Env = env_mod.ScoponeEnvMA
  +            if hasattr(_Env, 'step'):
  +                setattr(cast(Any, _Env), 'step', _lp(_Env.step))
  +            if hasattr(_Env, 'get_valid_actions'):
  +                setattr(cast(Any, _Env), 'get_valid_actions', _lp(_Env.get_valid_actions))
  +            if hasattr(_Env, '_get_observation'):
  +                setattr(cast(Any, _Env), '_get_observation', _lp(_Env._get_observation))
  +            if hasattr(_Env, 'reset'):
  +                setattr(cast(Any, _Env), 'reset', _lp(_Env.reset))
  +        import algorithms.is_mcts as mcts_mod
  +        if hasattr(mcts_mod, 'run_is_mcts'):
  +            mcts_mod.run_is_mcts = _lp(mcts_mod.run_is_mcts)
  +        # If IS-MCTS exposes subroutines, profile them too (best-effort)
  +        for sub_name in ['tree_policy', 'expand', 'simulate', 'backpropagate', 'select_child']:
  +            if hasattr(mcts_mod, sub_name):
  +                setattr(mcts_mod, sub_name, _lp(getattr(mcts_mod, sub_name)))
  +        # Model forward paths (actor/belief)
  +        import models.action_conditioned as ac_mod
  +        if hasattr(ac_mod, 'ActionConditionedActor'):
  +            _Act = ac_mod.ActionConditionedActor
  +            if hasattr(_Act, 'forward'):
  +                setattr(cast(Any, _Act), 'forward', _lp(_Act.forward))
  +        if hasattr(ac_mod, 'CentralValueNet'):
  +            _Crit = ac_mod.CentralValueNet
  +            if hasattr(_Crit, 'forward'):
  +                setattr(cast(Any, _Crit), 'forward', _lp(_Crit.forward))
  +        # PPO core
  +        import algorithms.ppo_ac as ppo_mod
  +        if hasattr(ppo_mod, 'ActionConditionedPPO'):
  +            _PPO = ppo_mod.ActionConditionedPPO
  +            if hasattr(_PPO, 'update'):
  +                setattr(cast(Any, _PPO), 'update', _lp(_PPO.update))
  +            if hasattr(_PPO, 'compute_loss'):
  +                setattr(cast(Any, _PPO), 'compute_loss', _lp(_PPO.compute_loss))
  +            if hasattr(_PPO, 'select_action'):
  +                setattr(cast(Any, _PPO), 'select_action', _lp(_PPO.select_action))
  +            if hasattr(_PPO, '_select_action_core'):
  +                setattr(cast(Any, _PPO), '_select_action_core', _lp(_PPO._select_action_core))
  +            # Try common names for submodules
  +            for attr in ['state_enc', 'belief_net']:
  +                if hasattr(_Act, attr):
  +                    sub = getattr(_Act, attr)
  +                    # If it's a Module class attribute, decorate .forward
  +                    if hasattr(sub, 'forward'):
  +                        setattr(cast(Any, sub), 'forward', _lp(sub.forward))
  +                if hasattr(_Act, attr):
  +                    sub = getattr(_Act, attr)
  +                    # If it's a Module class attribute, decorate .forward
  +                    if hasattr(sub, 'forward'):
  +                        sub.forward = line_profile(sub.forward)
  +
  +        # Allow nested local functions in trainer to be decorated if they opt-in
  +        setattr(train_mod, 'LINE_PROFILE_DECORATOR', line_profile)
  +
  +        # Observation encoding hot path
  +        import observation as obs_mod
  +        # The environment binds maybe_compile_function(_encode_state_compact_for_player_fast)
  +        # We decorate both the symbol and the compiled wrapper if present
  +        if hasattr(obs_mod, 'encode_state_compact_for_player_fast'):
  +            obs_mod.encode_state_compact_for_player_fast = line_profile(obs_mod.encode_state_compact_for_player_fast)
  +        for _obs_fn in ['bitset_rank_counts', 'bitset_table_sum']:
  +            if hasattr(obs_mod, _obs_fn):
  +                setattr(obs_mod, _obs_fn, line_profile(getattr(obs_mod, _obs_fn)))
  +
  +        # Algorithms (PPO) heavy paths
  +        import algorithms.ppo_ac as ppo_ac_mod
  +        if hasattr(ppo_ac_mod, 'ActionConditionedPPO'):
  +            _PPO = ppo_ac_mod.ActionConditionedPPO
  +            if hasattr(_PPO, 'update'):
  +                _PPO.update = line_profile(_PPO.update)
  +            if hasattr(_PPO, 'compute_loss'):
  +                _PPO.compute_loss = line_profile(_PPO.compute_loss)
  +
  +        if args.wrap_update:
  +            import algorithms.ppo_ac as ppo_mod
  +            setattr(cast(Any, ppo_mod.ActionConditionedPPO), 'update', _lp(ppo_mod.ActionConditionedPPO.update))
  +            actions_mod.decode_action_ids = line_profile(actions_mod.decode_action_ids)
  +
  +        if args.wrap_update:
  +            import algorithms.ppo_ac as ppo_mod
  +            ppo_mod.ActionConditionedPPO.update = line_profile(ppo_mod.ActionConditionedPPO.update)
  +
  +        # Shorter run for line profiler to keep overhead manageable
  +        # If profiler supports global tracing, register key functions
  +        if hasattr(global_profiler, 'allowed_codes'):
  +            global_profiler.allowed_codes.add(train_mod.collect_trajectory.__code__)
  +            global_profiler.allowed_codes.add(train_mod.train_ppo.__code__)
  +        # Use the same default as other modes for apples-to-apples comparisons
  +        num_envs_eff = _resolve_num_envs(args)
  +        _selfplay_env = str(os.environ.get('SCOPONE_SELFPLAY', '1')).strip().lower()
  +        _selfplay = (_selfplay_env in ['1', 'true', 'yes', 'on'])
  +        # Eval parity with main: read eval flags from env
  +        _eval_games = int(os.environ.get('SCOPONE_EVAL_GAMES','1000'))
  +        _eval_use_mcts = os.environ.get('SCOPONE_EVAL_USE_MCTS','0').lower() in ['1','true','yes','on']
  +        _eval_mcts_sims = int(os.environ.get('SCOPONE_EVAL_MCTS_SIMS','128'))
  +        _eval_mcts_dets = int(os.environ.get('SCOPONE_EVAL_MCTS_DETS','1'))
  +        _eval_c_puct = float(os.environ.get('SCOPONE_EVAL_MCTS_C_PUCT','1.0'))
  +        _eval_root_temp = float(os.environ.get('SCOPONE_EVAL_MCTS_ROOT_TEMP','0.0'))
  +        _eval_prior_eps = float(os.environ.get('SCOPONE_EVAL_MCTS_PRIOR_SMOOTH_EPS','0.0'))
  +        _eval_dir_alpha = float(os.environ.get('SCOPONE_EVAL_MCTS_DIRICHLET_ALPHA','0.25'))
  +        _eval_dir_eps = float(os.environ.get('SCOPONE_EVAL_MCTS_DIRICHLET_EPS','0.25'))
  +        train_fn(num_iterations=max(0, args.iters), horizon=max(40, args.horizon), k_history=39, num_envs=num_envs_eff,
  +                 mcts_sims=0, mcts_sims_eval=_eval_mcts_sims, eval_every=0, eval_games=_eval_games,
  +                 mcts_in_eval=_eval_use_mcts, mcts_dets=_eval_mcts_dets, mcts_c_puct=_eval_c_puct,
  +                 mcts_root_temp=_eval_root_temp, mcts_prior_smooth_eps=_eval_prior_eps,
  +                 mcts_dirichlet_alpha=_eval_dir_alpha, mcts_dirichlet_eps=_eval_dir_eps,
  +                 seed=seed, use_selfplay=_selfplay)
  +
  +        # Print per-function and per-line stats sorted by self time (TOT time per line)
  +        # Options: sort_by in {'cpu','gpu','transfer','self_cpu','self_gpu'} depending on implementation.
  +        # Prefer self CPU time to focus on exclusive line cost.
  +        global_profiler.print_stats(sort_by='self_cpu')
  +        if args.report:
  +            print(global_profiler.generate_report(include_line_details=True))
  +
  +        # Aggregate by file and by file:line from line-profiler results (use self time where available)
  +        import inspect
  +        from collections import defaultdict
  +
  +        project_root = ROOT
  +        def relpath(p):
  +            ap = os.path.abspath(p)
  +            if project_root in ap:
  +                return os.path.relpath(ap, project_root)
  +            return ap
  +
  +        per_file = defaultdict(lambda: {'cpu_s': 0.0, 'gpu_s': 0.0, 'transfer_s': 0.0, 'hits': 0})
  +        per_site = defaultdict(lambda: {'hits': 0, 'cpu_s': 0.0, 'gpu_s': 0.0, 'transfer_s': 0.0})
  +
  +        # Exclude profiler internals and this script from aggregation
  +        def _is_excluded(rel_path: str) -> bool:
  +            rp = (rel_path or '').replace('\\', '/')
  +            return (
  +                rp.startswith('profilers/') or
  +                rp.endswith('/profilers/line_profiler.py') or
  +                rp.endswith('profilers/line_profiler.py') or
  +                rp.endswith('tools/profile_ppo.py')
  +            )
  +
  +        use_self_only = bool(getattr(args, 'line_self_only', False))
  +
  +        for func_name, lines in global_profiler.results.items():
  +            func_obj = global_profiler.functions.get(func_name)
  +            if func_obj is None:
  +                continue
  +            filename = inspect.getsourcefile(func_obj) or func_obj.__code__.co_filename
  +            rfile = relpath(filename)
  +            if _is_excluded(rfile):
  +                continue
  +            for line_no, payload in lines.items():
  +                # Support both 4-tuple and 5-tuple (with self_cpu)
  +                if isinstance(payload, (list, tuple)) and len(payload) >= 4:
  +                    hits, cpu_time, gpu_time, transfer_time = payload[:4]
  +                    self_cpu = payload[4] if len(payload) >= 5 else None
  +                else:
  +                    continue
  +                # Decide which metric to use
  +                metric_cpu = (self_cpu if (self_cpu is not None) else (None if use_self_only else cpu_time))
  +                if metric_cpu is None:
  +                    continue
  +                # Some implementations provide self_cpu in rest[0]; prefer it when present
  +                pf = per_file[rfile]
  +                pf['cpu_s'] += metric_cpu
  +                pf['gpu_s'] += gpu_time
  +                pf['transfer_s'] += transfer_time
  +                pf['hits'] += hits
  +                key = (rfile, int(line_no))
  +                site = per_site[key]
  +                site['hits'] += hits
  +                site['cpu_s'] += metric_cpu
  +                site['gpu_s'] += gpu_time
  +                site['transfer_s'] += transfer_time
  +
  +        # Print file summary
  +        print("\n===== Line-profiler — Time by file (self CPU) =====" if use_self_only else "\n===== Line-profiler — Time by file (preferring self CPU) =====")
  +        ranked_files = sorted(per_file.items(), key=lambda kv: kv[1]['cpu_s'], reverse=True)
  +        for i, (f, s) in enumerate(ranked_files[:20], 1):
  +            total = s['cpu_s']
  +            print(f"{i:>2}. {f}\n    CPU: {total:8.4f}s  GPU: {s['gpu_s']:8.4f}s  Transfer: {s['transfer_s']:8.4f}s  Hits: {s['hits']}")
  +
  +        # Print top lines across files (optionally full)
  +        print("\n===== Line-profiler — Top lines (by Self CPU time) =====" if use_self_only else "\n===== Line-profiler — Top lines (preferring Self CPU) =====")
  +        ranked_sites = sorted(per_site.items(), key=lambda kv: kv[1]['cpu_s'], reverse=True)
  +        limit = None
  +        limit = None if getattr(args, 'line_full', False) else 30
  +        for i, ((f, ln), agg) in enumerate(ranked_sites[: (None if limit is None else limit) ], 1):
  +            print(f"{i:>2}. {f}:{ln}  Self CPU: {agg['cpu_s']:.6f}s  GPU: {agg['gpu_s']:.6f}s  Transfer: {agg['transfer_s']:.6f}s  Hits: {agg['hits']}")
  +
  +        # Optional CSV dump for full per-line times across all files
  +        line_csv = getattr(args, 'line_csv', None)
  +        if line_csv:
  +            import csv as _csv
  +            abs_csv = os.path.abspath(line_csv)
  +            out_dir = os.path.dirname(abs_csv)
  +            if out_dir:
  +                os.makedirs(out_dir, exist_ok=True)
  +            with open(abs_csv, 'w', newline='', encoding='utf-8') as f:
  +                w = _csv.writer(f)
  +                w.writerow(['file', 'line', 'hits', 'cpu_s', 'gpu_s', 'transfer_s'])
  +                for (frel, lno), agg in ranked_sites:
  +                    w.writerow([frel, int(lno), int(agg['hits']), f"{agg['cpu_s']:.9f}", f"{agg['gpu_s']:.9f}", f"{agg['transfer_s']:.9f}"])
  +            print("Saved per-line CSV to:", abs_csv)
  +        return
  +
  +    #trace_path = os.path.abspath('profile_trace.json')
  +    #print(f"Profiling short PPO run... trace -> {trace_path}")
  +
  +    # Require explicit selection of torch profiler; otherwise error out
  +    if not getattr(args, 'torch_profiler', False):
  +        print("Error: no profiler selected. Use one of: --torch-profiler, --line, --cprofile, --scalene")
  +        return
  +
  +    # Keep run short to avoid OOM without scheduler
  +    # Wrap selected hotspots with record_function via monkeypatch, without editing sources
  +    from torch.profiler import record_function as _record_function
  +    # observation
  +    import observation as _obs_mod
  +    if hasattr(_obs_mod, 'encode_state_compact_for_player_fast'):
  +        _orig_encode = _obs_mod.encode_state_compact_for_player_fast
  +        def _wrap_encode(*a, **kw):
  +            with _record_function('obs.encode_state_compact_for_player_fast'):
  +                return _orig_encode(*a, **kw)
  +        _obs_mod.encode_state_compact_for_player_fast = _wrap_encode  # type: ignore
  +    # environment
  +    import environment as _env_mod
  +    if hasattr(_env_mod.ScoponeEnvMA, 'get_valid_actions'):
  +        _orig_gva = _env_mod.ScoponeEnvMA.get_valid_actions
  +        def _wrap_gva(self, *a, **kw):
  +            with _record_function('env.get_valid_actions'):
  +                return _orig_gva(self, *a, **kw)
  +        _env_mod.ScoponeEnvMA.get_valid_actions = _wrap_gva  # type: ignore
  +    if hasattr(_env_mod.ScoponeEnvMA, '_get_observation'):
  +        _orig_go = _env_mod.ScoponeEnvMA._get_observation
  +        def _wrap_go(self, *a, **kw):
  +            with _record_function('env._get_observation'):
  +                return _orig_go(self, *a, **kw)
  +        _env_mod.ScoponeEnvMA._get_observation = _wrap_go  # type: ignore
  +    if hasattr(_env_mod.ScoponeEnvMA, 'step'):
  +        _orig_step = _env_mod.ScoponeEnvMA.step
  +        def _wrap_step(self, *a, **kw):
  +            with _record_function('env.step'):
  +                return _orig_step(self, *a, **kw)
  +        _env_mod.ScoponeEnvMA.step = _wrap_step  # type: ignore
  +    # algorithms / actor
  +    import algorithms.ppo_ac as _ppo_mod
  +    if hasattr(_ppo_mod.ActionConditionedPPO, '_select_action_core'):
  +        _orig_core = _ppo_mod.ActionConditionedPPO._select_action_core
  +        def _wrap_core(self, *a, **kw):
  +            with _record_function('algo._select_action_core'):
  +                return _orig_core(self, *a, **kw)
  +        _ppo_mod.ActionConditionedPPO._select_action_core = _wrap_core  # type: ignore
  +    if hasattr(_ppo_mod.ActionConditionedPPO, 'select_action'):
  +        _orig_sel = _ppo_mod.ActionConditionedPPO.select_action
  +        def _wrap_sel(self, *a, **kw):
  +            with _record_function('algo.select_action'):
  +                return _orig_sel(self, *a, **kw)
  +        _ppo_mod.ActionConditionedPPO.select_action = _wrap_sel  # type: ignore
  +    # model internals
  +    import models.action_conditioned as _ac_mod
  +    if hasattr(_ac_mod.ActionConditionedActor, 'compute_state_proj'):
  +        _orig_csp = _ac_mod.ActionConditionedActor.compute_state_proj
  +        def _wrap_csp(self, *a, **kw):
  +            with _record_function('model.compute_state_proj'):
  +                return _orig_csp(self, *a, **kw)
  +        _ac_mod.ActionConditionedActor.compute_state_proj = _wrap_csp  # type: ignore
  +    if hasattr(_ac_mod.ActionConditionedActor, '_mha_masked_mean'):
  +        _orig_mmm = _ac_mod.ActionConditionedActor._mha_masked_mean
  +        def _wrap_mmm(self, *a, **kw):
  +            with _record_function('model._mha_masked_mean'):
  +                return _orig_mmm(self, *a, **kw)
  +        _ac_mod.ActionConditionedActor._mha_masked_mean = _wrap_mmm  # type: ignore
  +
  +    # Configure child workers to run a torch profiler by default when torch-profiler is selected
  +    _profiles_dir = os.path.abspath(os.path.join(ROOT, 'profiles'))
  +    os.makedirs(_profiles_dir, exist_ok=True)
  +    os.environ['SCOPONE_TORCH_PROF'] = '1'
  +    os.environ['SCOPONE_TORCH_PROF_DIR'] = _profiles_dir
  +    from datetime import datetime as _dt
  +    _run_tag = _dt.now().strftime('%Y%m%d_%H%M%S')
  +    os.environ['SCOPONE_TORCH_PROF_RUN'] = _run_tag
  +    # TensorBoard combined timeline directory (shared across main + workers)
  +    _tb_dir = os.path.abspath(os.path.join(ROOT, 'runs', 'profiler', _run_tag))
  +    os.makedirs(_tb_dir, exist_ok=True)
  +    os.environ['SCOPONE_TORCH_TB_DIR'] = _tb_dir
  +
  +    from torch.profiler import schedule as _tp_schedule, tensorboard_trace_handler as _tb_handler
  +    _tb_handler_main = _tb_handler(_tb_dir, worker_name=f"main-pid{os.getpid()}")
  +
  +    # Relax timeouts and workload for profiling stability
  +    os.environ['SCOPONE_RPC_TIMEOUT_S'] = '300'
  +    os.environ['SCOPONE_COLLECTOR_STALL_S'] = '300'
  +    os.environ['SCOPONE_EP_PUT_TIMEOUT_S'] = '60'
  +
  +    with torch.profiler.profile(
  +        activities=[
  +            torch.profiler.ProfilerActivity.CPU,
  +            torch.profiler.ProfilerActivity.CUDA,
  +        ],
  +        record_shapes=False,
  +        profile_memory=False,
  +        with_stack=True,
  +        with_modules=True,
  +        schedule=_tp_schedule(wait=0, warmup=0, active=1, repeat=1000000),
  +        on_trace_ready=_tb_handler_main,
  +    ) as prof:
  +        # Constrain workload under profiler
  +        _prof_max_envs = int(os.environ.get('SCOPONE_PROF_NUM_ENVS', '8'))
  +        _prof_max_horizon = int(os.environ.get('SCOPONE_PROF_HORIZON', '2048'))
  +        num_envs_eff = _resolve_num_envs(args, clamp=_prof_max_envs)
  +        _selfplay_env = str(os.environ.get('SCOPONE_SELFPLAY', '1')).strip().lower()
  +        _selfplay = (_selfplay_env in ['1', 'true', 'yes', 'on'])
  +        def _on_iter_end_cb(it_idx: int):
  +            prof.step()
  +        _h_eff = int(min(max(40, args.horizon), _prof_max_horizon))
  +        print(f"[torch-profiler] Effective profiling config: num_envs={num_envs_eff}, horizon={_h_eff}, RPC_TIMEOUT_S={os.environ.get('SCOPONE_RPC_TIMEOUT_S')}")
  +        _eval_games = int(os.environ.get('SCOPONE_EVAL_GAMES','1000'))
  +        train_ppo(num_iterations=max(0, args.iters), horizon=_h_eff, k_history=39, num_envs=num_envs_eff,
  +                  mcts_sims=0, mcts_sims_eval=0, eval_every=0, eval_games=_eval_games, mcts_in_eval=False, seed=seed, use_selfplay=_selfplay,
  +                  on_iter_end=_on_iter_end_cb,
  +                  mcts_warmup_iters=_mcts_warmup_iters)
  +
  +    # Export chrome trace for main process
  +    out_main = os.path.abspath(os.path.join(_profiles_dir, f"tp_main_{os.getpid()}_{os.environ.get('SCOPONE_TORCH_PROF_RUN','run')}.json"))
  +    prof.export_chrome_trace(out_main)
  +    print(f"\nSaved main torch profiler trace: {out_main}")
  +    print(f"TensorBoard combined timeline saved under: {_tb_dir}")
  +    print("Per-worker traces are saved in profiles/ as tp_worker_<wid>_<pid>_<run>.json")
  +
  +    # Print top operators by CUDA time and CPU time
  +    print("\nTop ops by CUDA time:")
  +    print(prof.key_averages(group_by_input_shape=True).table(sort_by="self_cuda_time_total", row_limit=25))
  +    print("\nTop ops by CPU time:")
  +    print(prof.key_averages(group_by_input_shape=True).table(sort_by="self_cpu_time_total", row_limit=25))
  +
  +    # Stack-grouped tables (gives file:line attribution)
  +    print("\nTop by CUDA time (grouped by stack):")
  +    print(prof.key_averages(group_by_stack_n=10).table(sort_by="self_cuda_time_total", row_limit=30))
  +    print("\nTop by CPU time (grouped by stack):")
  +    print(prof.key_averages(group_by_stack_n=10).table(sort_by="self_cpu_time_total", row_limit=30))
  +
  +    # Summarize record_function tags to make hotspots immediately visible
  +    from collections import defaultdict as _dd
  +    tag_totals = _dd(lambda: { 'cpu_us': 0.0, 'cuda_us': 0.0, 'count': 0 })
  +    for avg in prof.key_averages():
  +        name = getattr(avg, 'key', '') or getattr(avg, 'name', '') or ''
  +        if not isinstance(name, str):
  +            continue
  +        if not (name.startswith('env.') or name.startswith('algo.') or name.startswith('model.') or name.startswith('obs.')):
  +            continue
  +        cpu_us = getattr(avg, 'self_cpu_time_total', 0.0) or 0.0
  +        cuda_us = getattr(avg, 'self_device_time_total', None)
  +        if cuda_us is None:
  +            cuda_us = getattr(avg, 'self_cuda_time_total', 0.0) or 0.0
  +        tag_totals[name]['cpu_us'] += float(cpu_us)
  +        tag_totals[name]['cuda_us'] += float(cuda_us)
  +        tag_totals[name]['count'] += getattr(avg, 'count', 1) or 1
  +
  +    if tag_totals:
  +        def _to_ms(us: float) -> float:
  +            return float(us) / 1000.0
  +        print("\nTop by tag (record_function):")
  +        ranked_tags = sorted(tag_totals.items(), key=lambda kv: (kv[1]['cuda_us'] + kv[1]['cpu_us']), reverse=True)
  +        for i, (tag, s) in enumerate(ranked_tags[:20], 1):
  +            total_ms = _to_ms(s['cpu_us'] + s['cuda_us'])
  +            print(f"{i:>2}. {tag}\n    Total: {total_ms:8.2f} ms | CPU: {_to_ms(s['cpu_us']):8.2f} ms | CUDA: {_to_ms(s['cuda_us']):8.2f} ms | count: {int(s['count'])}")
  +
  +    # Aggregate by user source file (from Python stacks) and highlight H2D/D2H memcpys
  +    try:
  +        from collections import defaultdict
  +        import re
  +
  +        project_root = ROOT
  +        events = prof.events()
  +
  +        def to_ms(us):
  +            return float(us) / 1000.0
  +
  +        def frame_filename_and_line(frame_like):
  +            """Return (filename, line) from a frame-like object or string."""
  +            fn = getattr(frame_like, 'filename', None)
  +            ln = getattr(frame_like, 'line', None)
  +            if fn:
  +                return fn, ln if isinstance(ln, int) else None
  +            s = str(frame_like)
  +            m = re.search(r"(.*?\.py):(\d+)", s) or re.search(r"(.*?\.py)\((\d+)\)", s)
  +            if m:
  +                fn = m.group(1)
  +                ln = int(m.group(2))
  +                return fn, ln
  +            if '.py' in s:
  +                pre = s.split('.py', 1)[0] + '.py'
  +                ln_m = re.search(r":(\d+)", s)
  +                ln = int(ln_m.group(1)) if ln_m else None
  +                return pre, ln
  +            return None, None
  +
  +        def iter_stack_frames(stack_obj):
  +            if not stack_obj:
  +                return
  +            frames_attr = getattr(stack_obj, 'frames', None)
  +            if isinstance(frames_attr, (list, tuple)):
  +                for fr in frames_attr:
  +                    yield fr
  +                return
  +            if isinstance(stack_obj, (list, tuple)):
  +                for fr in stack_obj:
  +                    yield fr
  +                return
  +            if isinstance(stack_obj, str):
  +                for line in stack_obj.splitlines():
  +                    yield line
  +                return
  +            yield stack_obj
  +
  +        def is_in_project(abs_path):
  +            if project_root in abs_path:
  +                return True
  +            return os.sep + os.path.basename(project_root) + os.sep in abs_path
  +
  +        def find_user_file(stack_obj):
  +            for fr in iter_stack_frames(stack_obj):
  +                fn, ln = frame_filename_and_line(fr)
  +                if not fn:
  +                    continue
  +                abs_fn = os.path.abspath(fn)
  +                if is_in_project(abs_fn):
  +                    rel = os.path.relpath(abs_fn, project_root)
  +                    return rel, (ln if isinstance(ln, int) else None)
  +            return None, None
  +
  +        per_file = defaultdict(lambda: {
  +            'cpu_ms': 0.0,
  +            'cuda_ms': 0.0,
  +            'count': 0,
  +            'memcpy_h2d_count': 0,
  +            'memcpy_d2h_count': 0,
  +            'memcpy_ms': 0.0,
  +        })
  +        memcpy_sites = defaultdict(lambda: {'count': 0, 'ms': 0.0})
  +
  +        for evt in events:
  +            name = str(getattr(evt, 'name', ''))
  +            cpu_us = getattr(evt, 'self_cpu_time_total', 0.0) or 0.0
  +            cuda_us = getattr(evt, 'self_device_time_total', None)
  +            if cuda_us is None:
  +                cuda_us = getattr(evt, 'self_cuda_time_total', 0.0)
  +            cuda_us = cuda_us or 0.0
  +            stack = getattr(evt, 'stack', None)
  +            file_rel, line_no = find_user_file(stack)
  +            if not file_rel:
  +                file_rel = '<external/CUDA or Library>'
  +                line_no = -1
  +            stats = per_file[file_rel]
  +            stats['cpu_ms'] += to_ms(cpu_us)
  +            stats['cuda_ms'] += to_ms(cuda_us)
  +            stats['count'] += 1
  +
  +            lname = name.lower().replace(' ', '')
  +            is_memcpy = ('memcpy' in lname) or ('memcpyasync' in lname) or ('dtoh' in lname) or ('htod' in lname)
  +            if is_memcpy:
  +                memcpy_ms = to_ms(cuda_us if cuda_us else cpu_us)
  +                stats['memcpy_ms'] += memcpy_ms
  +                kind = 'H2D' if ('h2d' in lname or 'htod' in lname) else ('D2H' if ('d2h' in lname or 'dtoh' in lname) else 'UNK')
  +                if kind == 'H2D':
  +                    stats['memcpy_h2d_count'] += 1
  +                elif kind == 'D2H':
  +                    stats['memcpy_d2h_count'] += 1
  +                site_key = (file_rel, int(line_no) if isinstance(line_no, int) else -1, kind)
  +                memcpy_sites[site_key]['count'] += 1
  +                memcpy_sites[site_key]['ms'] += memcpy_ms
  +
  +        def fmt_row(idx, file_rel, s):
  +            total_ms = s['cpu_ms'] + s['cuda_ms']
  +            memcpy_pct = (100.0 * s['memcpy_ms'] / total_ms) if total_ms > 0 else 0.0
  +            return (f"{idx:>2}. {file_rel}\n"
  +                    f"    CUDA: {s['cuda_ms']:8.2f} ms | CPU: {s['cpu_ms']:8.2f} ms | Total: {total_ms:8.2f} ms\n"
  +                    f"    memcpy H2D: {s['memcpy_h2d_count']:4d}  D2H: {s['memcpy_d2h_count']:4d} | memcpy time: {s['memcpy_ms']:7.2f} ms ({memcpy_pct:4.1f}%)")
  +
  +        print("\n===== Time by source file (self times) =====")
  +        ranked = sorted(per_file.items(), key=lambda kv: (kv[1]['cuda_ms'] + kv[1]['cpu_ms']), reverse=True)
  +        for i, (file_rel, s) in enumerate(ranked[:30], 1):
  +            print(fmt_row(i, file_rel, s))
  +
  +        only_external = all(k == '<external/CUDA or Library>' for k, _ in per_file.items()) or len(per_file) == 0
  +        if only_external:
  +            per_file_agg = defaultdict(lambda: {
  +                'cpu_ms': 0.0,
  +                'cuda_ms': 0.0,
  +                'count': 0,
  +                'memcpy_h2d_count': 0,
  +                'memcpy_d2h_count': 0,
  +                'memcpy_ms': 0.0,
  +            })
  +            for avg in prof.key_averages(group_by_stack_n=25):
  +                stack_obj = getattr(avg, 'stack', None)
  +                file_rel, _ = find_user_file(stack_obj)
  +                if not file_rel:
  +                    file_rel = '<external/CUDA or Library>'
  +                cuda_us = getattr(avg, 'self_device_time_total', None)
  +                if cuda_us is None:
  +                    cuda_us = getattr(avg, 'self_cuda_time_total', 0.0)
  +                cpu_us = getattr(avg, 'self_cpu_time_total', 0.0) or 0.0
  +                s = per_file_agg[file_rel]
  +                s['cpu_ms'] += to_ms(cpu_us)
  +                s['cuda_ms'] += to_ms(cuda_us or 0.0)
  +                s['count'] += getattr(avg, 'count', 1) or 1
  +            print("\n===== Fallback (aggregated stacks) — Time by source file =====")
  +            ranked2 = sorted(per_file_agg.items(), key=lambda kv: (kv[1]['cuda_ms'] + kv[1]['cpu_ms']), reverse=True)
  +            for i, (file_rel, s) in enumerate(ranked2[:30], 1):
  +                print(fmt_row(i, file_rel, s))
  +
  +        if memcpy_sites:
  +            print("\n===== Top memcpy sites (by time) =====")
  +            ranked_sites = sorted(memcpy_sites.items(), key=lambda kv: kv[1]['ms'], reverse=True)
  +            for i, ((file_rel, ln, kind), agg) in enumerate(ranked_sites[:30], 1):
  +                loc = f"{file_rel}:{ln if ln and ln>0 else '?'}"
  +                print(f"{i:>2}. {kind:>3}  {agg['ms']:8.2f} ms  | count: {agg['count']:4d}  | {loc}")
  +        else:
  +            print("\n(no memcpy events detected; if you expect transfers, ensure CUDA profiling is enabled)")
  +    except Exception as e:
  +        print(f"Per-file aggregation failed: {e}")
  +
  +
  +if __name__ == "__main__":
  +    main()
diff --cc trainers/train_ppo.py
index 7744425,7744425,0000000..350cbef
mode 100644,100644,000000..100644
--- a/trainers/train_ppo.py
+++ b/trainers/train_ppo.py
@@@@ -1,4277 -1,4277 -1,0 +1,4089 @@@@
  +import torch
  +from tqdm import tqdm
  +from typing import Any, Dict, List, Callable, Optional, Tuple
  +import os
  +import time
  +import sys
  +import multiprocessing as mp
  +import platform
  +import queue
  +import random
  +import numpy as np
  +
  +# Ensure project root is on sys.path when running as script
  +_THIS_DIR = os.path.dirname(os.path.abspath(__file__))
  +_PROJECT_ROOT = os.path.abspath(os.path.join(_THIS_DIR, os.pardir))
  +if _PROJECT_ROOT not in sys.path:
  +    sys.path.insert(0, _PROJECT_ROOT)
  +
  +from environment import ScoponeEnvMA
  +from algorithms.ppo_ac import ActionConditionedPPO
  +from utils.device import get_compute_device
  + 
  +from selfplay.league import League
+++from belief import sample_determinization
  +from models.action_conditioned import ActionConditionedActor
  +from utils.compile import maybe_compile_module
  +from utils.seed import set_global_seeds, resolve_seed, temporary_seed
  +from evaluation.eval import evaluate_pair_actors
  +
  +import torch.optim as optim
  +
  +device = get_compute_device()
  +# Global perf flags
  +torch.backends.cudnn.benchmark = True
  +torch.backends.cudnn.enabled = True
  +torch.backends.cuda.matmul.allow_tf32 = True
  +torch.set_float32_matmul_precision('high')
  +
  +# Profiling controls (single boolean -> full detail when enabled)
  +_PAR_TIMING = (os.environ.get('SCOPONE_PROFILE', '0') != '0')
  +_PAR_DEBUG = (os.environ.get('SCOPONE_PAR_DEBUG', '0') in ['1','true','yes','on'])
  +_PPO_DEBUG = (os.environ.get('SCOPONE_PPO_DEBUG', '0').strip().lower() in ['1', 'true', 'yes', 'on'])
  +
  +# Reuse cached tensors to limit per-step allocations inside workers
  +_SEAT_VEC_CACHE: Dict[int, torch.Tensor] = {}
  +_BELIEF_ZERO = torch.zeros(120, dtype=torch.float32)
  +_OTHERS_HANDS_ZERO = torch.zeros((3, 40), dtype=torch.float32)
  +_EMPTY_LEGAL = torch.zeros((0, 80), dtype=torch.float32)
  +
  +_SERIAL_RNG_STATE: Dict[int, Dict[str, Any]] = {}
  +
  +
  +def _serial_seed_enter(seed: Optional[int]) -> Optional[Dict[str, Any]]:
  +    if seed is None:
  +        return None
  +    try:
  +        seed_key = int(seed)
  +    except Exception:
  +        return None
  +    if seed_key < 0:
  +        return None
  +    py_outer = random.getstate()
  +    try:
  +        np_outer = np.random.get_state()
  +    except AttributeError:
  +        np_outer = None
  +    torch_outer = torch.get_rng_state()
  +    cuda_outer = torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None
  +
  +    stored = _SERIAL_RNG_STATE.get(seed_key)
  +    if stored is None:
  +        set_global_seeds(seed_key)
  +    else:
  +        py_state = stored.get('py')
  +        if py_state is not None:
  +            random.setstate(py_state)
  +        np_state = stored.get('np')
  +        if np_state is not None:
  +            np.random.set_state(np_state)  # type: ignore[arg-type]
  +        torch_state = stored.get('torch')
  +        if torch_state is not None:
  +            torch.set_rng_state(torch_state)
  +        cuda_state = stored.get('cuda')
  +        if cuda_state is not None and torch.cuda.is_available():
  +            torch.cuda.set_rng_state_all(cuda_state)  # type: ignore[arg-type]
  +
  +    return {
  +        'seed': seed_key,
  +        'py_outer': py_outer,
  +        'np_outer': np_outer,
  +        'torch_outer': torch_outer,
  +        'cuda_outer': cuda_outer,
  +    }
  +
  +
  +def _serial_seed_exit(token: Optional[Dict[str, Any]]) -> None:
  +    if token is None:
  +        return
  +    seed_key = int(token['seed'])
  +    try:
  +        np_new = np.random.get_state()
  +    except AttributeError:
  +        np_new = None
  +    _SERIAL_RNG_STATE[seed_key] = {
  +        'py': random.getstate(),
  +        'np': np_new,
  +        'torch': torch.get_rng_state(),
  +        'cuda': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,
  +    }
  +    py_outer = token.get('py_outer')
  +    if py_outer is not None:
  +        random.setstate(py_outer)
  +    np_outer = token.get('np_outer')
  +    if np_outer is not None:
  +        np.random.set_state(np_outer)  # type: ignore[arg-type]
  +    torch_outer = token.get('torch_outer')
  +    if torch_outer is not None:
  +        torch.set_rng_state(torch_outer)
  +    cuda_outer = token.get('cuda_outer')
  +    if cuda_outer is not None and torch.cuda.is_available():
  +        torch.cuda.set_rng_state_all(cuda_outer)  # type: ignore[arg-type]
  +
+++
+++
  +def _dbg(msg: str) -> None:
  +    if _PAR_DEBUG:
  +        try:
  +            tqdm.write(str(msg))
  +        except Exception:
  +            print(str(msg), flush=True)
  +
  +
  +def _flatten_cpu(tensor: Optional[torch.Tensor], dtype: Optional[torch.dtype] = torch.float32) -> Optional[torch.Tensor]:
  +    if tensor is None:
  +        return None
  +    if not torch.is_tensor(tensor):
  +        tensor = torch.as_tensor(tensor)
  +    if tensor.numel() == 0:
  +        return tensor.detach().to('cpu', dtype=dtype) if dtype is not None else tensor.detach().to('cpu')
  +    out = tensor.detach().to('cpu')
  +    if dtype is not None and out.dtype != dtype:
  +        out = out.to(dtype=dtype)
  +    return out.reshape(-1)
  +
  +
  +def _tensor_basic_stats(tensor: Optional[torch.Tensor]) -> Dict[str, float]:
  +    if tensor is None:
  +        return {'count': 0}
  +    numel = int(tensor.numel())
  +    if numel == 0:
  +        return {'count': 0}
  +    mean = float(tensor.mean().item())
  +    std = float(tensor.std(unbiased=False).item()) if numel > 1 else 0.0
  +    min_v = float(tensor.min().item())
  +    max_v = float(tensor.max().item())
  +    abs_mean = float(tensor.abs().mean().item())
  +    return {
  +        'count': numel,
  +        'mean': mean,
  +        'std': std,
  +        'min': min_v,
  +        'max': max_v,
  +        'abs_mean': abs_mean,
  +    }
  +
  +
  +def _format_stats(name: str, stats: Dict[str, float]) -> str:
  +    if stats.get('count', 0) == 0:
  +        return f"{name}=c0"
  +    return (
  +        f"{name}=c{stats['count']} μ{stats['mean']:.4f} σ{stats['std']:.4f} "
  +        f"|μ|{stats['abs_mean']:.4f} min{stats['min']:.4f} max{stats['max']:.4f}"
  +    )
  +
  +
  +def _maybe_log_ppo_batch(
  +    label: str,
  +    rew: Optional[torch.Tensor],
  +    ret: Optional[torch.Tensor],
  +    adv: Optional[torch.Tensor],
  +    val: Optional[torch.Tensor],
  +    next_val: Optional[torch.Tensor],
  +    done_mask: Optional[torch.Tensor],
  +    *,
  +    old_logp: Optional[torch.Tensor] = None,
  +    seat_tensor: Optional[torch.Tensor] = None,
  +    episode_lengths: Optional[List[int]] = None,
  +    extra: Optional[Dict[str, Any]] = None,
  +) -> None:
  +    if not _PPO_DEBUG:
  +        return
  +    try:
  +        rew_cpu = _flatten_cpu(rew)
  +        ret_cpu = _flatten_cpu(ret)
  +        adv_cpu = _flatten_cpu(adv)
  +        val_cpu = _flatten_cpu(val)
  +        next_cpu = _flatten_cpu(next_val)
  +        logp_cpu = _flatten_cpu(old_logp) if old_logp is not None else None
  +        done_cpu = _flatten_cpu(done_mask, dtype=torch.float32)
  +        seat_cpu = _flatten_cpu(seat_tensor, dtype=torch.float32) if seat_tensor is not None else None
  +
  +        rew_stats = _tensor_basic_stats(rew_cpu)
  +        ret_stats = _tensor_basic_stats(ret_cpu)
  +        adv_stats = _tensor_basic_stats(adv_cpu)
  +        val_stats = _tensor_basic_stats(val_cpu)
  +        next_stats = _tensor_basic_stats(next_cpu)
  +        logp_stats = _tensor_basic_stats(logp_cpu) if logp_cpu is not None else {'count': 0}
  +
  +        batch_size = ret_stats.get('count', 0)
  +        msg_parts = [f"[ppo-debug {label}] B={batch_size}"]
  +        msg_parts.append(_format_stats('rew', rew_stats))
  +        msg_parts.append(_format_stats('ret', ret_stats))
  +        msg_parts.append(_format_stats('val', val_stats))
  +        msg_parts.append(_format_stats('adv', adv_stats))
  +        msg_parts.append(_format_stats('next', next_stats))
  +        if logp_stats.get('count', 0) > 0:
  +            msg_parts.append(_format_stats('old_logp', logp_stats))
  +
  +        if ret_cpu is not None and val_cpu is not None and ret_stats.get('count', 0) == val_stats.get('count', 0) and ret_stats.get('count', 0) > 0:
  +            diff = ret_cpu - val_cpu
  +            mse = float((diff.pow(2).mean()).item())
  +            l1 = float(diff.abs().mean().item())
  +            msg_parts.append(f"ret-v mse={mse:.4f} l1={l1:.4f}")
  +
  +        if adv_cpu is not None and adv_stats.get('count', 0) > 0:
  +            adv_abs_max = float(adv_cpu.abs().max().item())
  +            msg_parts.append(f"adv|max|={adv_abs_max:.4f}")
  +
  +        if done_cpu is not None and done_cpu.numel() > 0:
  +            done_sum = float(done_cpu.sum().item())
  +            done_frac = done_sum / float(done_cpu.numel())
  +            msg_parts.append(f"done_sum={done_sum:.0f} done_frac={done_frac:.4f}")
  +
  +        if seat_tensor is not None and seat_cpu is not None and seat_cpu.numel() > 0 and ret_cpu is not None and rew_cpu is not None:
  +            seat_cpu_mat = seat_cpu.view(-1, 6)
  +            seat_idx = torch.argmax(seat_cpu_mat[:, :4], dim=1)
  +            seat_counts = tuple(int((seat_idx == i).sum().item()) for i in range(4))
  +            team0_mask = seat_cpu_mat[:, 4] > 0.5
  +            team1_mask = seat_cpu_mat[:, 5] > 0.5
  +            team_counts = (int(team0_mask.sum().item()), int(team1_mask.sum().item()))
  +            team_reward = (
  +                float(rew_cpu[team0_mask].sum().item()) if team_counts[0] > 0 else 0.0,
  +                float(rew_cpu[team1_mask].sum().item()) if team_counts[1] > 0 else 0.0,
  +            )
  +            msg_parts.append(f"seat_counts={seat_counts} team_counts={team_counts} team_rew={team_reward}")
  +
  +        if episode_lengths:
  +            ep_count = len(episode_lengths)
  +            ep_min = min(episode_lengths)
  +            ep_max = max(episode_lengths)
  +            ep_mean = sum(episode_lengths) / float(ep_count)
  +            msg_parts.append(f"ep_len(c={ep_count} min={ep_min} max={ep_max} mean={ep_mean:.2f})")
  +
  +        if extra:
  +            extra_str = ' '.join([f"{k}={v}" for k, v in extra.items()])
  +            if extra_str:
  +                msg_parts.append(extra_str)
  +
  +        try:
  +            tqdm.write(' '.join(msg_parts))
  +        except Exception:
  +            print(' '.join(msg_parts), flush=True)
  +    except Exception as exc:
  +        try:
  +            tqdm.write(f"[ppo-debug {label}] logging failed: {exc}")
  +        except Exception:
  +            print(f"[ppo-debug {label}] logging failed: {exc}", flush=True)
  +
  +
  +def _normalize_adv_tensor(label: str, adv_tensor: torch.Tensor) -> torch.Tensor:
  +    if adv_tensor.numel() == 0:
  +        return adv_tensor
  +    dbg_before = _tensor_basic_stats(_flatten_cpu(adv_tensor)) if _PPO_DEBUG else None
  +    mean = adv_tensor.mean()
  +    std = adv_tensor.std(unbiased=False)
  +    std = torch.clamp(std, min=1e-8)
  +    normalized = (adv_tensor - mean) / std
  +    if _PPO_DEBUG and dbg_before is not None:
  +        dbg_after = _tensor_basic_stats(_flatten_cpu(normalized))
  +        mean_val = float(mean.item()) if torch.is_tensor(mean) else float(mean)
  +        std_val = float(std.item()) if torch.is_tensor(std) else float(std)
  +        msg = (
  +            f"[ppo-debug adv-norm {label}] {_format_stats('adv_pre', dbg_before)} "
  +            f"{_format_stats('adv_post', dbg_after)} mean_before={mean_val:.4f} std_before={std_val:.4f}"
  +        )
  +        try:
  +            tqdm.write(msg)
  +        except Exception:
  +            print(msg, flush=True)
  +    return normalized
  +
  +# Sub-profilers (used whenever _PAR_TIMING is True)
  +_BATCHSEL_PROF = {
  +    'count': 0,
  +    't_stack': 0.0,
  +    't_leg_stack': 0.0,
  +    't_state_proj': 0.0,
  +    't_action_enc': 0.0,
  +    't_mask': 0.0,
  +    't_score': 0.0,
  +    't_softmax_sample': 0.0,
  +    't_total': 0.0,
  +    'sum_B': 0,
  +    'sum_M': 0,
  +    'cnt_hist': {},
  +}
  +
  +# get_reqs sub-profiler
  +_GETREQS_PROF = {
  +    'batches': 0,
  +    'total_reqs': 0,
  +    't_first_block': 0.0,
  +    't_nowait_drain': 0.0,
  +    't_topup_block': 0.0,
  +    't_post_drain': 0.0,
  +    'cnt_first_ok': 0,
  +    'cnt_first_to': 0,
  +    'cnt_nowait_ok': 0,
  +    'cnt_nowait_empty': 0,
  +    'cnt_topup_ok': 0,
  +    'cnt_topup_to': 0,
  +    'bs_hist': {},
  +}
  +
  +# One-time run flags
  +_HORIZON_ADJUST_LOGGED = False
  +
  +
  +def _seat_vec_for(cp: int) -> torch.Tensor:
  +    v = _SEAT_VEC_CACHE.get(cp)
  +    if v is None:
  +        base = torch.zeros(6, dtype=torch.float32)
  +        base[cp] = 1.0
  +        base[4] = 1.0 if cp in [0, 2] else 0.0
  +        base[5] = 1.0 if cp in [1, 3] else 0.0
  +        _SEAT_VEC_CACHE[cp] = base
  +        v = base
  +    return v
  +
  +
  +def _to_cpu_float32(tensor_like: torch.Tensor) -> torch.Tensor:
  +    if torch.is_tensor(tensor_like):
  +        if tensor_like.device.type == 'cpu' and tensor_like.dtype == torch.float32:
  +            return tensor_like.detach()
  +        return tensor_like.detach().to('cpu', dtype=torch.float32)
  +    return torch.as_tensor(tensor_like, dtype=torch.float32)
  +
  +
  +def _env_worker(worker_id: int,
  +                cfg: Dict,
  +                request_q: mp.Queue,
  +                action_q: mp.Queue,
  +                episode_q: mp.Queue):
  +    _WDBG = (os.environ.get('SCOPONE_PAR_DEBUG', '0') in ['1','true','yes','on'])
  +    def _wdbg(msg: str) -> None:
  +        if _WDBG:
  +            try:
  +                tqdm.write(f"[worker {int(worker_id)} pid={os.getpid()}] {msg}")
  +            except Exception:
  +                print(f"[worker {int(worker_id)} pid={os.getpid()}] {msg}", flush=True)
  +    # Limit CPU threads per worker to reduce contention on the host
  +    wt = int(os.environ.get('SCOPONE_WORKER_THREADS', '1'))
  +    os.environ['OMP_NUM_THREADS'] = str(wt)
  +    os.environ['MKL_NUM_THREADS'] = str(wt)
  +    torch.set_num_threads(wt)
  +    torch.set_num_interop_threads(1)
  +    # Ensure different RNG streams per worker for robustness
  +    set_global_seeds(int(cfg.get('seed', 0)) + int(worker_id))
  +    env = ScoponeEnvMA(rules=cfg.get('rules', {'shape_scopa': False}),
  +                       k_history=int(cfg.get('k_history', 39)))
  +    episodes_per_env = int(cfg.get('episodes_per_env', 1))
  +    send_legals = bool(cfg.get('send_legals', True))
  +    use_mcts = bool(cfg.get('use_mcts', False))
  +    train_both_teams = bool(cfg.get('train_both_teams', False))
  +    main_seats = cfg.get('main_seats', None)
  +    # MCTS config
  +    mcts_sims = int(cfg.get('mcts_sims', 128))
  +    mcts_dets = int(cfg.get('mcts_dets', 4))
  +    mcts_c_puct = float(cfg.get('mcts_c_puct', 1.0))
  +    mcts_root_temp = float(cfg.get('mcts_root_temp', 0.0))
  +    mcts_prior_smooth_eps = float(cfg.get('mcts_prior_smooth_eps', 0.0))
  +    mcts_dirichlet_alpha = float(cfg.get('mcts_dirichlet_alpha', 0.25))
  +    mcts_dirichlet_eps = float(cfg.get('mcts_dirichlet_eps', 0.25))
  +    mcts_progress_start = float(cfg.get('mcts_progress_start', 0.25))
  +    mcts_progress_full = float(cfg.get('mcts_progress_full', 0.75))
  +    mcts_min_sims = int(cfg.get('mcts_min_sims', 0))
  +    mcts_train_factor = float(cfg.get('mcts_train_factor', 1.0))
  +    rpc_timeout_s = float(os.environ.get('SCOPONE_RPC_TIMEOUT_S', '30'))
  +    episode_put_timeout_s = float(os.environ.get('SCOPONE_EP_PUT_TIMEOUT_S', '15'))
  +    belief_aux_coef = float(os.environ.get('BELIEF_AUX_COEF', '0.1'))
  +
  +    # Torch profiler in worker: enabled by default when torch profiler mode is active in the main process
  +    _tp_active = (os.environ.get('SCOPONE_TORCH_PROF', '0') == '1')
  +    _worker_prof = None
  +    from torch.profiler import record_function as _record_function  # type: ignore
  +    if _tp_active:
  +        from torch.profiler import profile as _tp_profile, ProfilerActivity as _PA, tensorboard_trace_handler as _tb_handler_factory  # type: ignore
  +        _tb_dir_env = os.environ.get('SCOPONE_TORCH_TB_DIR', '').strip()
  +        _tb_write_fn = (_tb_handler_factory(_tb_dir_env, worker_name=f"env-{int(worker_id)}-pid{os.getpid()}") if _tb_dir_env else None)
  +        _worker_prof = _tp_profile(
  +            activities=[_PA.CPU],
  +            record_shapes=False,
  +            profile_memory=False,
  +            with_stack=False,
  +            with_modules=False,
  +        )
  +        _worker_prof.start()
  +
  +    t_env_reset = 0.0; t_get_obs = 0.0; t_get_legals = 0.0; t_mcts = 0.0; t_rpc = 0.0; t_step = 0.0; t_pack = 0.0
  +    t0_glob = time.time()
  +    _wdbg(f"start (episodes_per_env={int(episodes_per_env)})")
  +    for ep in range(episodes_per_env):
  +        t0 = time.time(); env.reset(); t_env_reset += (time.time() - t0) if _PAR_TIMING else 0.0
  +        _wdbg(f"ep {ep} reset")
  +        obs_list, next_obs_list = [], []
  +        act_list = []
  +        rew_list, done_list = [], []
  +        seat_team_list = []
  +        belief_sum_list = []
  +        legals_list, legals_offset, legals_count = [], [], []
  +        chosen_index_list = []
  +        mcts_policy_list = []
  +        mcts_weight_list = []
  +        others_hands_list = []
  +
  +        done = False
  +        info = {}
  +        step_idx = 0
  +        while not done:
  +            t0 = time.time();
  +            with _record_function('env._get_observation'):
  +                obs = env._get_observation(env.current_player)
  +            t_get_obs += (time.time() - t0) if _PAR_TIMING else 0.0
  +            if step_idx < 3:
  +                _wdbg(f"ep {ep} step {step_idx}: got obs (cp={env.current_player})")
  +            t0 = time.time();
  +            with _record_function('env.get_valid_actions'):
  +                legal = env.get_valid_actions()
  +            t_get_legals += (time.time() - t0) if _PAR_TIMING else 0.0
  +            if step_idx < 3:
  +                _wdbg(f"ep {ep} step {step_idx}: legals={int(len(legal))}")
  +            if torch.is_tensor(legal) and (legal.size(0) == 0):
  +                raise RuntimeError(f"collect_trajectory_parallel: worker {worker_id} got 0 legal actions (player={env.current_player})")
  +            # Avoid ambiguous truth-value on tensors; treat empty action sets explicitly
  +            is_empty = (int(legal.numel()) == 0)
  +            if is_empty:
  +                raise RuntimeError(f"worker {worker_id}: no legal actions at episode start (player={env.current_player})")
  +            cp = env.current_player
  +            seat_vec = _seat_vec_for(cp)
  +            seat_cpu = seat_vec
  +            obs_cpu = _to_cpu_float32(obs)
  +            if torch.is_tensor(legal):
  +                legal_cpu_tensor = legal.detach().to('cpu', dtype=torch.float32)
  +                legal_cpu_entries = list(legal_cpu_tensor.unbind(0))
  +            else:
  +                legal_cpu_entries = [_to_cpu_float32(x) for x in legal]
  +                legal_cpu_tensor = torch.stack(legal_cpu_entries, dim=0) if len(legal_cpu_entries) > 0 else _EMPTY_LEGAL
  +            legal_count = len(legal_cpu_entries)
  +            is_main = True if train_both_teams else ((main_seats is None and cp in [0, 2]) or (main_seats is not None and cp in main_seats))
  +            store_sample = bool(train_both_teams or is_main)
  +            policy_entries: List[float] = []
  +            policy_weight = 0.0
  +            bsum_tensor = _BELIEF_ZERO
  +            # Only attempt MCTS in worker if explicitly enabled and mcts_sims > 0
  +            _both_sides = str(os.environ.get('SCOPONE_MCTS_BOTH_SIDES', '1')).strip().lower() in ['1','true','yes','on']
  +            if (is_main or _both_sides) and use_mcts and len(legal) > 0 and int(mcts_sims) > 0:
  +                # Build helper RPCs to master for batched scoring
  +                if send_legals:
  +                    leg_serial = [x.tolist() for x in legal_cpu_entries]
  +                else:
  +                    leg_serial = []
  +                def policy_fn_mcts(_obs, _legals):
  +                    nonlocal t_rpc
  +                    # Invia tensori CPU direttamente (no conversione a liste) per ridurre overhead IPC
  +                    o_cpu = (_obs.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(_obs) else torch.as_tensor(_obs, dtype=torch.float32))
  +                    leg_cpu = torch.stack([ (y.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(y) else torch.as_tensor(y, dtype=torch.float32)) for y in _legals ], dim=0)
  +                    t1 = time.time()
  +                    if step_idx < 3:
  +                        _wdbg(f"ep {ep} step {step_idx}: MCTS policy -> send, waiting priors")
  +                    request_q.put({
  +                        'type': 'score_policy',
  +                        'wid': worker_id,
  +                        'obs': o_cpu,
  +                        'legals': leg_cpu,
  +                        'seat': seat_cpu,
  +                    })
  +                    try:
  +                        resp = action_q.get(timeout=rpc_timeout_s)
  +                    except queue.Empty as e:
  +                        raise TimeoutError(f"worker {worker_id}: Timeout waiting for score_policy priors") from e
  +                    t_rpc_local = (time.time() - t1)
  +                    if _PAR_TIMING:
  +                        t_rpc = t_rpc + t_rpc_local
  +                    if step_idx < 3:
  +                        _wdbg(f"ep {ep} step {step_idx}: MCTS policy <- got priors")
  +                    pri = resp.get('priors', None)
  +                    import numpy as _np
  +                    if pri is None or (len(pri) != len(_legals)):
  +                        raise RuntimeError(f"worker {worker_id}: invalid priors from master (priors={type(pri)}, expected_len={len(_legals)})")
  +                    return _np.asarray(pri, dtype=_np.float32)
  +                def value_fn_mcts(_obs, _env):
  +                    s_vec = _seat_vec_for(_env.current_player)
  +                    o_cpu = (_obs.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(_obs) else torch.as_tensor(_obs, dtype=torch.float32))
  +                    if step_idx < 3:
  +                        _wdbg(f"ep {ep} step {step_idx}: MCTS value -> send, waiting value")
  +                    request_q.put({
  +                        'type': 'score_value',
  +                        'wid': worker_id,
  +                        'obs': o_cpu,
  +                        'seat': s_vec,
  +                    })
  +                    try:
  +                        resp = action_q.get(timeout=rpc_timeout_s)
  +                    except queue.Empty as e:
  +                        raise TimeoutError(f"worker {worker_id}: Timeout waiting for score_value") from e
  +                    if step_idx < 3:
  +                        _wdbg(f"ep {ep} step {step_idx}: MCTS value <- got value")
  +                    return float(resp.get('value', 0.0))
  +                def belief_sampler_neural(_env):
--                     o_cur = _env._get_observation(_env.current_player)
--                     s_vec = _seat_vec_for(_env.current_player)
--                     o_cpu = (o_cur.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(o_cur) else torch.as_tensor(o_cur, dtype=torch.float32))
--                     if step_idx < 3:
--                         _wdbg(f"ep {ep} step {step_idx}: MCTS belief -> send, waiting probs")
--                     request_q.put({
--                         'type': 'score_belief',
--                         'wid': worker_id,
--                         'obs': o_cpu,
--                         'seat': s_vec,
--                     })
--                     try:
--                         resp = action_q.get(timeout=rpc_timeout_s)
--                     except queue.Empty as e:
--                         raise TimeoutError(f"worker {worker_id}: Timeout waiting for score_belief") from e
--                     if step_idx < 3:
--                         _wdbg(f"ep {ep} step {step_idx}: MCTS belief <- got probs")
--                     probs_flat = resp.get('belief_probs', None)
--                     if probs_flat is None:
--                         return None
--                     import numpy as _np
--                     probs = _np.asarray(probs_flat, dtype=_np.float32).reshape(3, 40)
--                     # visible mask from obs on worker side
--                     if torch.is_tensor(o_cur):
--                         o_t = o_cur.detach().to('cpu', dtype=torch.float32).unsqueeze(0)
--                     else:
--                         o_t = torch.as_tensor(o_cur, dtype=torch.float32).unsqueeze(0)
--                     hand_table = o_t[:, :83]
--                     hand_mask = hand_table[:, :40] > 0.5
--                     table_mask = hand_table[:, 43:83] > 0.5
--                     captured = o_t[:, 83:165]
--                     cap0_mask = captured[:, :40] > 0.5
--                     cap1_mask = captured[:, 40:80] > 0.5
--                     vis = (hand_mask | table_mask | cap0_mask | cap1_mask).squeeze(0).numpy().astype(bool)
--                     unknown_ids = [cid for cid in range(40) if not vis[cid]]
--                     others = [(_env.current_player + 1) % 4, (_env.current_player + 2) % 4, (_env.current_player + 3) % 4]
--                     counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
--                     caps = [int(counts.get(pid, 0)) for pid in others]
--                     n = len(unknown_ids)
--                     if sum(caps) != n:
--                         caps[2] = max(0, n - caps[0] - caps[1])
--                         if sum(caps) != n:
--                             base = n // 3
--                             rem = n - 3 * base
--                             caps = [base, base, base]
--                             for i in range(rem):
--                                 caps[i] += 1
--                     import numpy as _np
+++                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
  +                    noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
--                     costs = []
--                     for cid in unknown_ids:
--                         pc = probs[:, cid]
--                         ps = pc / max(1e-12, pc.sum())
--                         c = [-_np.log(max(1e-12, ps[i])) for i in range(3)]
--                         if noise_scale > 0:
--                             u = _np.random.uniform(1e-9, 1.0-1e-9, size=3)
--                             g = -_np.log(-_np.log(u)) * noise_scale
--                             c = [c[i] + float(g[i]) for i in range(3)]
--                         costs.append(c)
--                     INF = 1e12
--                     cap0, cap1, cap2 = caps
--                     dp = [[[INF]*(cap1+1) for _ in range(cap0+1)] for __ in range(n+1)]
--                     bk = [[[-1]*(cap1+1) for _ in range(cap0+1)] for __ in range(n+1)]
--                     dp[0][0][0] = 0.0
--                     for t in range(n):
--                         c0, c1, c2 = costs[t]
--                         for a in range(0, min(t, cap0)+1):
--                             for b in range(0, min(t-a, cap1)+1):
--                                 cur = dp[t][a][b]
--                                 if cur >= INF:
--                                     continue
--                                 if a+1 <= cap0 and dp[t+1][a+1][b] > cur + c0:
--                                     dp[t+1][a+1][b] = cur + c0
--                                     bk[t+1][a+1][b] = 0
--                                 if b+1 <= cap1 and dp[t+1][a][b+1] > cur + c1:
--                                     dp[t+1][a][b+1] = cur + c1
--                                     bk[t+1][a][b+1] = 1
--                                 assigned2 = t - a - b
--                                 if assigned2 + 1 <= cap2 and dp[t+1][a][b] > cur + c2:
--                                     dp[t+1][a][b] = cur + c2
--                                     bk[t+1][a][b] = 2
--                     if dp[n][cap0][cap1] >= INF:
--                         return None
--                     det = {pid: [] for pid in others}
--                     a, b = cap0, cap1
--                     for t in range(n, 0, -1):
--                         choice = bk[t][a][b]
--                         cid = unknown_ids[t-1]
--                         if choice == 0:
--                             det[others[0]].append(cid)
--                             a -= 1
--                         elif choice == 1:
--                             det[others[1]].append(cid)
--                             b -= 1
--                         else:
--                             det[others[2]].append(cid)
--                     return det
+++                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
  +                # Progress-based scaling (uniform with single-env) — optionally gated by env
  +                progress = float(min(1.0, max(0.0, len(env.game_state.get('history', [])) / 40.0)))
  +                denom = max(1e-6, (mcts_progress_full - mcts_progress_start))
  +                alpha = min(1.0, max(0.0, (progress - mcts_progress_start) / denom))
  +                scaling_on = (str(os.environ.get('SCOPONE_MCTS_SCALING', '1')).strip().lower() in ['1','true','yes','on'])
  +                # Permetti 0 simulazioni se mcts_min_sims==0
  +                base_min = int(mcts_min_sims) if (mcts_min_sims is not None and int(mcts_min_sims) >= 0) else 0
  +                import math
  +                if mcts_train_factor is not None and float(mcts_train_factor) <= 0.0:
  +                    sims_scaled = 0
  +                else:
  +                    if scaling_on:
  +                        sims_base = math.ceil(mcts_sims * (0.25 + 0.75 * alpha))
  +                    else:
  +                        sims_base = math.ceil(mcts_sims * float(mcts_train_factor if mcts_train_factor is not None else 1.0))
  +                    if mcts_train_factor is not None and scaling_on:
  +                        sims_base = math.ceil(sims_base * float(mcts_train_factor))
  +                    sims_scaled = int(max(base_min, sims_base))
  +                    if int(mcts_sims) > 0 and sims_scaled <= 0:
  +                        sims_scaled = 1
  +                # Root temperature dynamic only when scaling is enabled (or explicitly overridden)
  +                root_temp_dyn = (float(mcts_root_temp) if (not scaling_on or float(mcts_root_temp) > 0)
  +                                 else float(max(0.0, 1.0 - alpha)))
  +
  +                if sims_scaled <= 0:
  +                    # Sims ended up <= 0 despite mcts_sims > 0 — log, optionally raise for debug, then fall back to master step
  +                    if os.environ.get('SCOPONE_RAISE_ON_INVALID_SIMS', '0') == '1':
  +                        raise RuntimeError(f"invalid sims_scaled: sims_scaled={sims_scaled} mcts_sims={mcts_sims} progress_start={mcts_progress_start} progress_full={mcts_progress_full} history_len={len(env.game_state.get('history', [])) if isinstance(env.game_state.get('history', []), list) else 'n/a'} alpha={alpha}")
  +                    leg_serial = legal_cpu_tensor if send_legals else _EMPTY_LEGAL
  +                    request_q.put({
  +                        'type': 'step',
  +                        'wid': worker_id,
  +                        'obs': obs_cpu,
  +                        'legals': leg_serial,
  +                        'seat': seat_cpu,
  +                    })
  +                    try:
  +                        resp = action_q.get(timeout=rpc_timeout_s)
  +                        idx = int(resp.get('idx', 0))
  +                    except queue.Empty as e:
  +                        raise TimeoutError('Timeout waiting for step index for MCTS (invalid_sims_scaled)') from e
  +                    idx = max(0, min(idx, len(legal) - 1))
  +                    act_t = legal[idx] if torch.is_tensor(legal[idx]) else torch.as_tensor(legal[idx], dtype=torch.float32)
  +                    with _record_function('env.step'):
  +                        next_obs, rew, done, info = env.step(act_t)
  +                    # No distillation target
  +                    policy_entries = [0.0] * legal_count
  +                    policy_weight = 0.0
  +                else:
  +                    from algorithms.is_mcts import run_is_mcts
  +                    # Dynamic defaults for smoothing and root Dirichlet based on context
  +                    t1 = time.time(); priors_probe = policy_fn_mcts(obs, legal); t_mcts += (time.time() - t1) if _PAR_TIMING else 0.0
  +                    pri_t = (priors_probe if torch.is_tensor(priors_probe) else torch.as_tensor(priors_probe, dtype=torch.float32))
  +                    peak = float(pri_t.max().item()) if pri_t.numel() > 0 else (1.0 / max(1, len(legal)))
  +                    A = int(len(legal))
  +                    sims_fac = 1.0 if sims_scaled < 128 else (0.5 if sims_scaled < 256 else 0.25)
  +                    peak_fac = min(1.0, max(0.0, (peak - 0.5) / 0.4))
  +                    prior_eps_eff = 0.1 * sims_fac * peak_fac * (1.0 - alpha)
  +                    prior_eps_eff = float(max(0.0, min(0.15, prior_eps_eff)))
  +                    if A <= 3:
  +                        dir_eps_eff = 0.0
  +                    else:
  +                        a_fac = min(1.0, max(0.0, (A - 3) / 10.0))
  +                        sim_att = (0.7 if sims_scaled >= 256 else 1.0)
  +                        prog_att = (0.7 if alpha > 0.7 else 1.0)
  +                        dir_eps_eff = 0.25 * a_fac * sim_att * prog_att
  +                        dir_eps_eff = float(max(0.0, min(0.3, dir_eps_eff)))
  +
  +                    mcts_action, mcts_visits = run_is_mcts(env,
  +                        policy_fn=policy_fn_mcts,
  +                        value_fn=value_fn_mcts,
  +                        num_simulations=int(sims_scaled),
  +                        c_puct=float(mcts_c_puct),
  +                        belief=None,
  +                        num_determinization=int(mcts_dets),
  +                        root_temperature=root_temp_dyn,
  +                        prior_smooth_eps=prior_eps_eff,
  +                        robust_child=True,
  +                        root_dirichlet_alpha=float(mcts_dirichlet_alpha),
  +                        root_dirichlet_eps=dir_eps_eff,
  +                        return_stats=True,
  +                        belief_sampler=belief_sampler_neural)
  +                    chosen_act = mcts_action if torch.is_tensor(mcts_action) else torch.as_tensor(mcts_action, dtype=torch.float32)
  +                    def _act_key(x_t: torch.Tensor):
  +                        xt = x_t if torch.is_tensor(x_t) else torch.as_tensor(x_t, dtype=torch.float32)
  +                        return tuple(torch.nonzero(xt > 0.5, as_tuple=False).flatten().tolist())
  +                    key_target = _act_key(chosen_act)
  +                    idx = 0
  +                    for i_a, a in enumerate(legal):
  +                        if _act_key(a) == key_target:
  +                            idx = int(i_a)
  +                            break
  +                    act_t = legal[idx] if torch.is_tensor(legal[idx]) else torch.as_tensor(legal[idx], dtype=torch.float32)
  +                    t1 = time.time();
  +                    with _record_function('env.step'):
  +                        next_obs, rew, done, info = env.step(act_t)
  +                    t_step += (time.time() - t1) if _PAR_TIMING else 0.0
  +                    # Distillation targets
  +                    mcts_probs = torch.as_tensor(mcts_visits, dtype=torch.float32)
  +                    ssum = float(mcts_probs.sum().item())
  +                    if ssum > 0:
  +                        mcts_probs = mcts_probs / ssum
  +                    policy_entries = (mcts_probs.tolist() if hasattr(mcts_probs, 'tolist') else list(mcts_probs))
  +                    policy_weight = 1.0
  +            else:
  +                # Request action selection from master (GPU)
  +                leg_serial = legal_cpu_tensor if send_legals else _EMPTY_LEGAL
  +                if step_idx < 3:
  +                    _wdbg(f"ep {ep} step {step_idx}: STEP -> send, waiting idx (A={len(legal)})")
  +                t1 = time.time(); request_q.put({
  +                    'type': 'step',
  +                    'wid': worker_id,
  +                    'obs': obs_cpu,
  +                    'legals': leg_serial,
  +                    'seat': seat_cpu,
  +                })
  +                try:
  +                    resp = action_q.get(timeout=rpc_timeout_s)
  +                    idx = int(resp.get('idx', 0))
  +                except queue.Empty as e:
  +                    raise TimeoutError(f"worker {worker_id}: Timeout waiting for step index (main path)") from e
  +                t_rpc += (time.time() - t1) if _PAR_TIMING else 0.0
  +                if step_idx < 3:
  +                    _wdbg(f"ep {ep} step {step_idx}: STEP <- got idx={int(idx)}")
  +                idx = max(0, min(idx, len(legal) - 1))
  +                act_t = legal[idx] if torch.is_tensor(legal[idx]) else torch.as_tensor(legal[idx], dtype=torch.float32)
  +                t1 = time.time();
  +                with _record_function('env.step'):
  +                    next_obs, rew, done, info = env.step(act_t)
  +                if step_idx < 3:
  +                    _wdbg(f"ep {ep} step {step_idx}: env.step done={bool(done)} rew={float(rew)}")
  +                step_idx += 1
  +                t_step += (time.time() - t1) if _PAR_TIMING else 0.0
  +                # No distillation target
  +                policy_entries = [0.0] * legal_count
  +                policy_weight = 0.0
  +
  +            next_obs_cpu = _to_cpu_float32(next_obs)
  +            act_cpu = _to_cpu_float32(act_t)
  +
  +            if store_sample:
  +                obs_list.append(obs_cpu)
  +                next_obs_list.append(next_obs_cpu)
  +                act_list.append(act_cpu)
  +                rew_list.append(float(rew))
  +                done_list.append(bool(done))
  +                seat_team_list.append(seat_cpu)
  +                belief_sum_list.append(bsum_tensor)
  +                legals_offset.append(len(legals_list))
  +                legals_count.append(legal_count)
  +                if send_legals:
  +                    legals_list.extend(legal_cpu_entries)
  +                else:
  +                    from utils.fallback import notify_fallback
  +                    notify_fallback('trainer.collect_trajectory.legals_missing_for_store')
  +                chosen_index_list.append(int(idx))
  +                entries = policy_entries if len(policy_entries) > 0 else ([0.0] * legal_count)
  +                mcts_policy_list.extend(entries)
  +                mcts_weight_list.append(policy_weight if len(policy_entries) > 0 else 0.0)
  +                # Others' hands supervision target (3x40) — skip if BELIEF_AUX_COEF <= 0
  +                if belief_aux_coef <= 0.0:
  +                    others_hands_list.append(_OTHERS_HANDS_ZERO)
  +                else:
  +                    hands = env.game_state.get('hands', None)
  +                    if hands is not None:
  +                        others = [ (cp + 1) % 4, (cp + 2) % 4, (cp + 3) % 4 ]
  +                        target = torch.zeros((3,40), dtype=torch.float32)
  +                        for i,pid in enumerate(others):
  +                            for c in hands[pid]:
  +                                if isinstance(c, int):
  +                                    cid = c
  +                                else:
  +                                    r, s = c
  +                                    suit_to_int = {'denari': 0, 'coppe': 1, 'spade': 2, 'bastoni': 3}
  +                                    cid = int((r - 1) * 4 + suit_to_int[s])
  +                                target[i, int(cid)] = 1.0
  +                        others_hands_list.append(target)
  +                    else:
  +                        others_hands_list.append(_OTHERS_HANDS_ZERO)
  +            else:
  +                # Non-main seats skipped when training only main seats; ensure last stored sample closes episode
  +                if done and len(done_list) > 0:
  +                    done_list[-1] = True
  +                    if len(next_obs_list) > 0:
  +                        next_obs_list[-1] = next_obs_cpu
  +
  +        # Ensure at least one step per episode to avoid empty payloads
  +        if len(obs_list) == 0:
  +            obs = env._get_observation(env.current_player)
  +            legal = env.get_valid_actions()
  +            is_empty = (int(legal.numel()) == 0)
  +            if not is_empty:
  +                cp = env.current_player
  +                seat_vec = _seat_vec_for(cp)
  +                seat_cpu = seat_vec
  +                obs_cpu = _to_cpu_float32(obs)
  +                if torch.is_tensor(legal):
  +                    legals_cpu_tensor = legal.detach().to('cpu', dtype=torch.float32)
  +                    legal_entries = list(legals_cpu_tensor.unbind(0))
  +                else:
  +                    legal_entries = [_to_cpu_float32(x) for x in legal]
  +                    legals_cpu_tensor = torch.stack(legal_entries, dim=0) if len(legal_entries) > 0 else _EMPTY_LEGAL
  +                leg_serial = legals_cpu_tensor if send_legals else _EMPTY_LEGAL
  +                request_q.put({
  +                    'type': 'step',
  +                    'wid': worker_id,
  +                    'obs': obs_cpu,
  +                    'legals': leg_serial,
  +                    'seat': seat_cpu,
  +                })
  +                try:
  +                    resp = action_q.get(timeout=rpc_timeout_s)
  +                    idx = int(resp.get('idx', 0))
  +                except queue.Empty as e:
  +                    raise TimeoutError(f"worker {worker_id}: Timeout waiting for step index (forced first step)") from e
  +                idx = max(0, min(idx, len(legal) - 1))
  +                act_t = legal[idx] if torch.is_tensor(legal[idx]) else torch.as_tensor(legal[idx], dtype=torch.float32)
  +                with _record_function('env.step'):
  +                    next_obs, rew, done, info = env.step(act_t)
  +                next_obs_cpu = _to_cpu_float32(next_obs)
  +                act_cpu = _to_cpu_float32(act_t)
  +                obs_list.append(obs_cpu)
  +                next_obs_list.append(next_obs_cpu)
  +                act_list.append(act_cpu)
  +                rew_list.append(float(rew))
  +                done_list.append(bool(done))
  +                seat_team_list.append(seat_cpu)
  +                belief_sum_list.append(_BELIEF_ZERO)
  +                legals_offset.append(len(legals_list))
  +                legals_count.append(len(legal))
  +                if send_legals:
  +                    legals_list.extend(legal_entries)
  +                chosen_index_list.append(int(idx))
  +                # default others_hands zero target for this forced step
  +                others_hands_list.append(_OTHERS_HANDS_ZERO)
  +        
  +        # Episode payload back to master using NumPy arrays (avoid Torch resource_sharer FDs entirely)
  +        import numpy as _np
  +        if len(obs_list) > 0:
  +            t1 = time.time(); obs_t = torch.stack(obs_list, dim=0).to('cpu', dtype=torch.float32).numpy(); t_pack += (time.time() - t1) if _PAR_TIMING else 0.0
  +        else:
  +            obs_t = _np.zeros((0, 1), dtype=_np.float32)
  +        if len(next_obs_list) > 0:
  +            t1 = time.time(); next_obs_t = torch.stack(next_obs_list, dim=0).to('cpu', dtype=torch.float32).numpy(); t_pack += (time.time() - t1) if _PAR_TIMING else 0.0
  +        else:
  +            next_obs_t = _np.zeros((0, 1), dtype=_np.float32)
  +        if len(act_list) > 0:
  +            t1 = time.time(); act_t = torch.stack(act_list, dim=0).to('cpu', dtype=torch.float32).numpy(); t_pack += (time.time() - t1) if _PAR_TIMING else 0.0
  +        else:
  +            act_t = _np.zeros((0, 80), dtype=_np.float32)
  +        if len(seat_team_list) > 0:
  +            t1 = time.time(); seat_t = torch.stack(seat_team_list, dim=0).to('cpu', dtype=torch.float32).numpy(); t_pack += (time.time() - t1) if _PAR_TIMING else 0.0
  +        else:
  +            seat_t = _np.zeros((0, 6), dtype=_np.float32)
  +        if len(belief_sum_list) > 0:
  +            t1 = time.time(); belief_t = torch.stack(belief_sum_list, dim=0).to('cpu', dtype=torch.float32).numpy(); t_pack += (time.time() - t1) if _PAR_TIMING else 0.0
  +        else:
  +            belief_t = _np.zeros((0, 120), dtype=_np.float32)
  +        if len(legals_list) > 0:
  +            t1 = time.time(); legals_t = torch.stack(legals_list, dim=0).to('cpu', dtype=torch.float32).numpy(); t_pack += (time.time() - t1) if _PAR_TIMING else 0.0
  +        else:
  +            legals_t = _np.zeros((0, 80), dtype=_np.float32)
  +        leg_off_t = _np.asarray(legals_offset, dtype=_np.int64)
  +        leg_cnt_t = _np.asarray(legals_count, dtype=_np.int64)
  +        chosen_idx_t = _np.asarray(chosen_index_list, dtype=_np.int64)
  +        mcts_policy_t = _np.asarray(mcts_policy_list, dtype=_np.float32) if len(mcts_policy_list) > 0 else _np.zeros((0,), dtype=_np.float32)
  +        mcts_weight_t = _np.asarray(mcts_weight_list, dtype=_np.float32) if len(mcts_weight_list) > 0 else _np.zeros((0,), dtype=_np.float32)
  +        if len(others_hands_list) > 0:
  +            t1 = time.time(); others_hands_t = torch.stack(others_hands_list, dim=0).to('cpu', dtype=torch.float32).numpy(); t_pack += (time.time() - t1) if _PAR_TIMING else 0.0
  +        else:
  +            others_hands_t = _np.zeros((0, 3, 40), dtype=_np.float32)
  +
  +        # Non-blocking put with timeout to avoid deadlocks if the master is slow
  +        payload = {
  +            'wid': worker_id,
  +            'obs': obs_t,
  +            'next_obs': next_obs_t,
  +            'act': act_t,
  +            'rew': rew_list,
  +            'done': done_list,
  +            'seat': seat_t,
  +            'belief_summary': belief_t,
  +            'legals': legals_t,
  +            'leg_off': leg_off_t,
  +            'leg_cnt': leg_cnt_t,
  +            'chosen_idx': chosen_idx_t,
  +            'team_rewards': (info.get('team_rewards', [0.0, 0.0]) if isinstance(info, dict) else [0.0, 0.0]),
  +            'mcts_policy': mcts_policy_t,
  +            'mcts_weight': mcts_weight_t,
  +            'others_hands': others_hands_t,
  +        }
  +        try:
  +            if len(obs_list) == 0:
  +                raise RuntimeError(f"worker {worker_id}: episode finished with zero steps — invalid episode")
  +            _wdbg(f"ep {ep} putting episode payload (steps={len(obs_list)})")
  +            episode_q.put(payload, timeout=episode_put_timeout_s)
  +            _wdbg(f"ep {ep} payload put ok")
  +        except Exception as e:
  +            raise RuntimeError(f"worker {worker_id}: episode_q.put failed") from e
  +        # Do not advance worker profiler schedule per-episode to avoid Kineto stack issues
  +    # Stop and export worker torch profiler if active
  +    if _worker_prof is not None:
  +        _worker_prof.stop()
  +        from datetime import datetime as _dt  # local import to avoid changing global imports
  +        _dir = os.environ.get('SCOPONE_TORCH_PROF_DIR', os.path.abspath(os.path.join(_PROJECT_ROOT, 'profiles')))
  +        _run = os.environ.get('SCOPONE_TORCH_PROF_RUN', _dt.now().strftime('%Y%m%d_%H%M%S'))
  +        os.makedirs(_dir, exist_ok=True)
  +        _out = os.path.abspath(os.path.join(_dir, f"tp_worker_{int(worker_id)}_{os.getpid()}_{_run}.json"))
  +        _worker_prof.export_chrome_trace(_out)
  +        # Also write into the combined TensorBoard trace directory, if configured
  +        if '_tb_write_fn' in locals() and _tb_write_fn is not None:
  +            _tb_write_fn(_worker_prof)
  +
  +    # Signal completion to master
  +    _wdbg("sending done marker")
  +    episode_q.put({'wid': worker_id, 'type': 'done'}, timeout=2.0)
  +    if _PAR_TIMING:
  +        total = time.time() - t0_glob
  +        episode_q.put({'wid': worker_id, 'type': 'timing',
  +                       't_env_reset': t_env_reset,
  +                       't_get_obs': t_get_obs,
  +                       't_get_legals': t_get_legals,
  +                       't_mcts': t_mcts,
  +                       't_rpc': t_rpc,
  +                       't_step': t_step,
  +                       't_pack': t_pack,
  +                       't_total': total})
  +
  +
  +def _batched_select_indices(agent: ActionConditionedPPO,
  +                            reqs: List[Dict]) -> List[Tuple[int, int]]:
  +    # Returns list of (wid, idx) per req
  +    if len(reqs) == 0:
  +        return []
  +    # Basic validation
  +    for r in reqs:
  +        if 'obs' not in r or 'legals' not in r or 'wid' not in r:
  +            raise KeyError("_batched_select_indices: missing 'obs'/'legals'/'wid' in request")
  +    # Stack CPU tensors; keep selection on CPU to avoid small GPU kernels
  +    _t_total0 = time.time() if _PAR_TIMING else 0.0
  +    _t0 = time.time() if _PAR_TIMING else 0.0
  +    obs_cpu = torch.stack([torch.as_tensor(r['obs'], dtype=torch.float32) for r in reqs], dim=0)
  +    seat_cpu = torch.stack([torch.as_tensor(r['seat'], dtype=torch.float32) for r in reqs], dim=0)
  +    if _PAR_TIMING:
  +        _BATCHSEL_PROF['t_stack'] += (time.time() - _t0)
  +    # Flatten legals and build offsets
  +    _t0 = time.time() if _PAR_TIMING else 0.0
  +    flat_legals = []
  +    offs, cnts = [], []
  +    for r in reqs:
  +        offs.append(len(flat_legals))
  +        cnt = len(r['legals'])
  +        cnts.append(cnt)
  +        if cnt > 0:
  +            flat_legals.extend(r['legals'])
  +    if len(flat_legals) == 0:
  +        # No legals, return zeros
  +        return [(int(reqs[i]['wid']), 0) for i in range(len(reqs))]
  +    if _PAR_TIMING:
  +        _BATCHSEL_PROF['t_leg_stack'] += (time.time() - _t0)
  +    _t0 = time.time() if _PAR_TIMING else 0.0
  +    leg_cpu = torch.stack([torch.as_tensor(x, dtype=torch.float32) for x in flat_legals], dim=0)
  +    if _PAR_TIMING:
  +        _BATCHSEL_PROF['t_leg_stack'] += (time.time() - _t0)
  +    with torch.no_grad():
  +        # Keep selection on CPU device
  +        cpu_device = torch.device('cpu')
  +        o_t = obs_cpu
  +        s_t = seat_cpu
  +        leg_t = leg_cpu
  +        # Validate legal encoding in STRICT mode only
  +        if os.environ.get('SCOPONE_STRICT_CHECKS', '0') == '1':
  +            ones = leg_t[:, :40].sum(dim=1)
  +            if not torch.allclose(ones, torch.ones_like(ones)):
  +                raise RuntimeError("_batched_select_indices: each legal must have one played bit in [:40]")
  +        _t0 = time.time() if _PAR_TIMING else 0.0
  +        state_proj = agent.actor.compute_state_proj(o_t, s_t)  # (B,64)
  +        if _PAR_TIMING:
  +            _BATCHSEL_PROF['t_state_proj'] += (time.time() - _t0)
  +        _t0 = time.time() if _PAR_TIMING else 0.0
  +        a_emb_all = agent.actor.action_enc(leg_t)             # (M,64)
  +        if _PAR_TIMING:
  +            _BATCHSEL_PROF['t_action_enc'] += (time.time() - _t0)
  +        B = o_t.size(0)
  +        cnts_t = torch.as_tensor(cnts, dtype=torch.long, device=device)
  +        max_cnt = int(cnts_t.max().item()) if B > 0 else 0
  +        pos = torch.arange(max_cnt, device=device, dtype=torch.long) if max_cnt > 0 else None
  +        out_idx = []
  +        if _PAR_TIMING:
  +            _BATCHSEL_PROF['sum_B'] += int(B)
  +            _BATCHSEL_PROF['sum_M'] += int(leg_t.size(0))
  +            ch = _BATCHSEL_PROF['cnt_hist']
  +            ch[max_cnt] = int(ch.get(int(max_cnt), 0)) + 1
  +        if max_cnt > 0:
  +            _t0 = time.time() if _PAR_TIMING else 0.0
  +            rel_pos_2d = pos.unsqueeze(0).expand(B, max_cnt)
  +            mask = rel_pos_2d < cnts_t.unsqueeze(1)
  +            offs_t = torch.as_tensor(offs, dtype=torch.long, device=device)
  +            abs_idx = (offs_t.unsqueeze(1) + rel_pos_2d)[mask]
  +            sample_idx = torch.arange(B, device=device, dtype=torch.long).unsqueeze(1).expand(B, max_cnt)[mask]
  +            if _PAR_TIMING:
  +                _BATCHSEL_PROF['t_mask'] += (time.time() - _t0)
  +            _t0 = time.time() if _PAR_TIMING else 0.0
  +            a_emb_mb = a_emb_all[abs_idx]
  +            legal_scores = (a_emb_mb * state_proj[sample_idx]).sum(dim=1)
  +            if _PAR_TIMING:
  +                _BATCHSEL_PROF['t_score'] += (time.time() - _t0)
  +            _t0 = time.time() if _PAR_TIMING else 0.0
  +            padded = torch.full((B, max_cnt), -1e9, device=device, dtype=legal_scores.dtype)
  +            padded[mask] = legal_scores
  +            logits = padded
  +            probs = torch.softmax(logits, dim=1)
  +            # Keep only legal positions; set others to zero
  +            probs = torch.where(mask, probs, torch.zeros_like(probs))
  +            # Select only rows with at least one legal action for sampling
  +            valid_rows = (cnts_t > 0)
  +            sel = torch.zeros((B,), dtype=torch.long, device=device)
  +            if bool(valid_rows.any()):
  +                pv = probs[valid_rows]
  +                mv = mask[valid_rows]
  +                # Sanitize probabilities: remove NaN/Inf and negatives; strictly validate
  +                pv = pv.nan_to_num(0.0)
  +                pv = torch.clamp(pv, min=0.0)
  +                rs = pv.sum(dim=1, keepdim=True)
  +                bad_rows = (~torch.isfinite(rs)) | (rs <= 0)
  +                if bool(bad_rows.any()):
  +                    raise RuntimeError("_batched_select_indices: invalid probability rows (NaN/Inf or zero-sum)")
  +                # Sample for exploration
  +                sub = torch.multinomial(pv, num_samples=1).squeeze(1)
  +                # Write back selections to full batch
  +                sel[valid_rows] = sub
  +            # rows with no legal actions keep sel=0
  +            sel_cpu = sel.detach().to('cpu')
  +            for i in range(B):
  +                out_idx.append((int(reqs[i]['wid']), int(sel_cpu[i].item())))
  +            if _PAR_TIMING:
  +                _BATCHSEL_PROF['t_softmax_sample'] += (time.time() - _t0)
  +        else:
  +            out_idx = [(int(reqs[i]['wid']), 0) for i in range(B)]
  +    if _PAR_TIMING:
  +        _BATCHSEL_PROF['t_total'] += (time.time() - _t_total0)
  +        _BATCHSEL_PROF['count'] += 1
  +    return out_idx
  +
  +def _batched_service(agent: ActionConditionedPPO, reqs: List[Dict]) -> List[Dict]:
  +    """Service batched policy/value/belief scoring requests.
  +    Returns a list of dicts with results in the same order as input reqs.
  +    """
  +    if len(reqs) == 0:
  +        return []
  +    # Stack obs/seat for all reqs once
  +    obs_cpu = torch.stack([torch.as_tensor(r['obs'], dtype=torch.float32) for r in reqs], dim=0)
  +    seat_cpu = torch.stack([torch.as_tensor(r.get('seat', [0,0,0,0,0,0]), dtype=torch.float32) for r in reqs], dim=0)
  +    with torch.no_grad():
  +        # Use CPU tensors directly when not targeting CUDA to avoid unnecessary pinned memory
  +        if getattr(device, 'type', str(device)) == 'cuda':
  +            if device.type == 'cuda':
  +                o_t = obs_cpu.pin_memory().to(device=device, non_blocking=True)
  +                s_t = seat_cpu.pin_memory().to(device=device, non_blocking=True)
  +            else:
  +                o_t = obs_cpu.to(device=device)
  +                s_t = seat_cpu.to(device=device)
  +        else:
  +            o_t = obs_cpu
  +            s_t = seat_cpu
  +        state_proj_all = agent.actor.compute_state_proj(o_t, s_t)  # (N,64)
  +
  +    results: List[Dict] = [{} for _ in reqs]
  +
  +    # 1) Batch all score_policy requests together
  +    policy_positions: List[int] = []
  +    policy_cnts: List[int] = []
  +    policy_legals_rows: List[torch.Tensor] = []
  +    for i, r in enumerate(reqs):
  +        if r.get('type') == 'score_policy':
  +            leg = r.get('legals', [])
  +            if isinstance(leg, torch.Tensor):
  +                if leg.dim() == 1:
  +                    leg = leg.unsqueeze(0)
  +                cnt = int(leg.size(0))
  +                if cnt > 0:
  +                    policy_positions.append(i)
  +                    policy_cnts.append(cnt)
  +                    policy_legals_rows.append(leg)
  +                else:
  +                    results[i] = {'priors': []}
  +            else:
  +                # list-like
  +                cnt = len(leg)
  +                if cnt > 0:
  +                    policy_positions.append(i)
  +                    policy_cnts.append(cnt)
  +                    policy_legals_rows.append(torch.stack([torch.as_tensor(x, dtype=torch.float32) for x in leg], dim=0))
  +                else:
  +                    results[i] = {'priors': []}
  +
  +    if len(policy_positions) > 0:
  +        with torch.no_grad():
  +            # Concatenate legals
  +            legals_flat = torch.cat(policy_legals_rows, dim=0)
  +            if device.type == 'cuda':
  +                legals_flat = legals_flat.pin_memory().to(device=device, non_blocking=True)
  +            else:
  +                legals_flat = legals_flat.to(device=device)
  +            Bp = len(policy_positions)
  +            cnts_t = torch.as_tensor(policy_cnts, dtype=torch.long, device=device)
  +            max_cnt = int(cnts_t.max().item()) if Bp > 0 else 0
  +            # Map from each legal to its policy-sample index
  +            sample_idx_per_legal = torch.repeat_interleave(torch.arange(Bp, device=device, dtype=torch.long), cnts_t)
  +            sp = state_proj_all[torch.as_tensor(policy_positions, dtype=torch.long, device=device)]  # (Bp,64)
  +
  +            # Card logits and mask per sample
  +            card_logits_all = torch.matmul(sp, agent.actor.card_emb_play.t())  # (Bp,40)
  +            played_ids_all = torch.argmax(legals_flat[:, :40], dim=1)  # (M_flat)
  +            logp_cards_all = torch.log_softmax(card_logits_all, dim=1)  # (Bp,40)
  +            logp_cards_per_legal = logp_cards_all[sample_idx_per_legal, played_ids_all]
  +
  +            # Capture logits per-legal
  +            a_emb_flat = agent.actor.action_enc(legals_flat)  # (M_flat,64)
  +            cap_logits = (a_emb_flat * sp[sample_idx_per_legal]).sum(dim=1)
  +            # Log-softmax within (sample,card) group
  +            group_ids = sample_idx_per_legal * 40 + played_ids_all
  +            num_groups = Bp * 40
  +            group_max = torch.full((num_groups,), float('-inf'), dtype=cap_logits.dtype, device=device)
  +            group_max.scatter_reduce_(0, group_ids, cap_logits, reduce='amax', include_self=True)
  +            gmax_per_legal = group_max[group_ids]
  +            exp_shifted = torch.exp(cap_logits - gmax_per_legal).to(cap_logits.dtype)
  +            group_sum = torch.zeros((num_groups,), dtype=cap_logits.dtype, device=device)
  +            group_sum.index_add_(0, group_ids, exp_shifted)
  +            lse_per_legal = gmax_per_legal + torch.log(torch.clamp_min(group_sum[group_ids], 1e-12))
  +            logp_cap_per_legal = cap_logits - lse_per_legal
  +
  +            logp_total_per_legal = logp_cards_per_legal + logp_cap_per_legal
  +            if not torch.isfinite(logp_total_per_legal).all():
  +                raise RuntimeError("_batched_service: non-finite log-probs for legals")
  +
  +            # Softmax over legals per sample (strict validation)
  +            if max_cnt > 0:
  +                pos = torch.arange(max_cnt, device=device, dtype=torch.long)
  +                rel_pos_2d = pos.unsqueeze(0).expand(Bp, max_cnt)
  +                mask = rel_pos_2d < cnts_t.unsqueeze(1)
  +                padded = torch.full((Bp, max_cnt), float('-inf'), dtype=logp_total_per_legal.dtype, device=device)
  +                # Write in flatten order
  +                # Compute absolute indices in flattened per-sample view (vectorized)
  +                if Bp > 0:
  +                    cnts_t = torch.as_tensor(policy_cnts, dtype=torch.long, device=device)
  +                    starts = torch.cumsum(torch.nn.functional.pad(cnts_t[:-1], (1, 0)), dim=0)
  +                    offs_t = torch.repeat_interleave(starts, cnts_t)
  +                else:
  +                    offs_t = torch.zeros((0,), dtype=torch.long, device=device)
  +                # Above is heavy; alternatively fill by iterating slices
  +                # Use a simple loop in CUDA graph-free context for clarity
  +                start = 0
  +                for j, c in enumerate(policy_cnts):
  +                    if c > 0:
  +                        padded[j, :c] = logp_total_per_legal[start:start+c]
  +                        start += c
  +                priors_padded = torch.softmax(padded, dim=1).nan_to_num(0.0)
  +                priors_padded = torch.clamp(priors_padded, min=0.0)
  +                row_sums = priors_padded.sum(dim=1, keepdim=True)
  +                if bool(((row_sums <= 0) | (~torch.isfinite(row_sums))).any()):
  +                    raise RuntimeError("_batched_service: invalid priors row (NaN/Inf or zero-sum)")
  +                priors_padded = priors_padded / row_sums
  +                # Scatter back to per-request lists
  +                start = 0
  +                for j, (pos_j, c) in enumerate(zip(policy_positions, policy_cnts)):
  +                    if c > 0:
  +                        pri = priors_padded[j, :c].detach().to('cpu').tolist()
  +                    else:
  +                        pri = []
  +                    results[pos_j] = {'priors': pri}
  +            else:
  +                for pos_j in policy_positions:
  +                    results[pos_j] = {'priors': []}
  +
  +    # 2) Process score_value and score_belief (can remain per-request)
  +    for i, r in enumerate(reqs):
  +        rtype = r.get('type')
  +        if rtype == 'score_value':
  +            with torch.no_grad():
  +                o_one = o_t[i:i+1]
  +                s_one = s_t[i:i+1]
  +                state_feat = agent.actor.state_enc(o_one, s_one)
  +                # Align dtype with BeliefNet parameters to avoid Half/Float mismatch
  +                bn_dtype = agent.actor.belief_net.fc_in.weight.dtype
  +                if state_feat.dtype != bn_dtype:
  +                    state_feat = state_feat.to(dtype=bn_dtype)
  +                logits = agent.actor.belief_net(state_feat)
  +                hand_table = o_one[:, :83]
  +                hand_mask = hand_table[:, :40] > 0.5
  +                table_mask = hand_table[:, 43:83] > 0.5
  +                captured = o_one[:, 83:165]
  +                cap0_mask = captured[:, :40] > 0.5
  +                cap1_mask = captured[:, 40:80] > 0.5
  +                visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
  +                probs_flat = agent.actor.belief_net.probs(logits, visible_mask)
  +                oh = probs_flat.view(1, 3, 40)
  +                val = agent.critic(o_one, s_one, oh).squeeze(0)
  +            results[i] = {'value': float(val.detach().cpu().item())}
  +        elif rtype == 'score_belief':
  +            with torch.no_grad():
  +                _state = agent.actor.state_enc(o_t[i:i+1], s_t[i:i+1])
  +                bn_dtype = agent.actor.belief_net.fc_in.weight.dtype
  +                if _state.dtype != bn_dtype:
  +                    _state = _state.to(dtype=bn_dtype)
  +                logits = agent.actor.belief_net(_state)
  +                hand_table = o_t[i:i+1, :83]
  +                hand_mask = hand_table[:, :40] > 0.5
  +                table_mask = hand_table[:, 43:83] > 0.5
  +                captured = o_t[i:i+1, 83:165]
  +                cap0_mask = captured[:, :40] > 0.5
  +                cap1_mask = captured[:, 40:80] > 0.5
  +                visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
  +                probs_flat = agent.actor.belief_net.probs(logits, visible_mask).squeeze(0).detach().cpu().tolist()
  +            results[i] = {'belief_probs': probs_flat}
  +
  +    return results
  +
  +def _batched_select_indices_with_actor(actor: ActionConditionedActor, reqs: List[Dict]) -> List[Tuple[int, int]]:
  +    """Like _batched_select_indices but using a provided frozen actor (no critic)."""
  +    if len(reqs) == 0:
  +        return []
  +    for r in reqs:
  +        if 'obs' not in r or 'legals' not in r or 'wid' not in r:
  +            raise KeyError("_batched_select_indices_with_actor: missing 'obs'/'legals'/'wid' in request")
  +    _t_total0 = time.time() if _PAR_TIMING else 0.0
  +    obs_cpu = torch.stack([torch.as_tensor(r['obs'], dtype=torch.float32) for r in reqs], dim=0)
  +    seat_cpu = torch.stack([torch.as_tensor(r['seat'], dtype=torch.float32) for r in reqs], dim=0)
  +    flat_legals = []
  +    offs, cnts = [], []
  +    for r in reqs:
  +        offs.append(len(flat_legals))
  +        cnt = len(r['legals'])
  +        cnts.append(cnt)
  +        if cnt > 0:
  +            flat_legals.extend(r['legals'])
  +    if len(flat_legals) == 0:
  +        return [(int(reqs[i]['wid']), 0) for i in range(len(reqs))]
  +    leg_cpu = torch.stack([torch.as_tensor(x, dtype=torch.float32) for x in flat_legals], dim=0)
  +    with torch.no_grad():
  +        # Use CPU to avoid device churn
  +        o_t = obs_cpu
  +        s_t = seat_cpu
  +        leg_t = leg_cpu
  +        # State projection via frozen actor's encoder
  +        state_proj = actor.compute_state_proj(o_t, s_t)
  +        a_emb_all = actor.action_enc(leg_t)
  +        B = o_t.size(0)
  +        cnts_t = torch.as_tensor(cnts, dtype=torch.long)
  +        max_cnt = int(cnts_t.max().item()) if B > 0 else 0
  +        pos = torch.arange(max_cnt, dtype=torch.long) if max_cnt > 0 else None
  +        out_idx = []
  +        if max_cnt > 0:
  +            rel_pos_2d = pos.unsqueeze(0).expand(B, max_cnt)
  +            mask = rel_pos_2d < cnts_t.unsqueeze(1)
  +            offs_t = torch.as_tensor(offs, dtype=torch.long)
  +            abs_idx = (offs_t.unsqueeze(1) + rel_pos_2d)[mask]
  +            sample_idx = torch.arange(B, dtype=torch.long).unsqueeze(1).expand(B, max_cnt)[mask]
  +            a_emb_mb = a_emb_all[abs_idx]
  +            legal_scores = (a_emb_mb * state_proj[sample_idx]).sum(dim=1)
  +            padded = torch.full((B, max_cnt), -1e9, dtype=legal_scores.dtype)
  +            padded[mask] = legal_scores
  +            probs = torch.softmax(padded, dim=1)
  +            probs = torch.where(mask, probs, torch.zeros_like(probs))
  +            valid_rows = (cnts_t > 0)
  +            sel = torch.zeros((B,), dtype=torch.long)
  +            if bool(valid_rows.any()):
  +                pv = probs[valid_rows]
  +                mv = mask[valid_rows]
  +                pv = pv.nan_to_num(0.0)
  +                pv = torch.clamp(pv, min=0.0)
  +                rs = pv.sum(dim=1, keepdim=True)
  +                bad_rows = (~torch.isfinite(rs)) | (rs <= 0)
  +                if bool(bad_rows.any()):
  +                    raise RuntimeError("_batched_select_indices_with_actor: invalid probability rows (NaN/Inf or zero-sum)")
  +                sub = torch.multinomial(pv, num_samples=1).squeeze(1)
  +                sel[valid_rows] = sub
  +            sel_cpu = sel.detach()
  +            for i in range(B):
  +                out_idx.append((int(reqs[i]['wid']), int(sel_cpu[i].item())))
  +        else:
  +            out_idx = [(int(reqs[i]['wid']), 0) for i in range(B)]
  +    if _PAR_TIMING:
  +        _BATCHSEL_PROF['t_total'] += (time.time() - _t_total0)
  +        _BATCHSEL_PROF['count'] += 1
  +    return out_idx
  +def _compute_per_seat_diagnostics(agent: ActionConditionedPPO, batch: Dict) -> Dict[str, torch.Tensor]:
  +    """Calcola approx_kl, entropia e clip_frac per gruppi seat 0/2 vs 1/3.
  +
  +    Tutte le operazioni restano su GPU; ritorna tensori 0-D su device.
  +    """
  +    obs = batch['obs'] if torch.is_tensor(batch['obs']) else torch.as_tensor(batch['obs'], dtype=torch.float32)
  +    obs = obs.to(device=device)
  +    seat = batch.get('seat_team', None)
  +    if seat is None:
  +        seat = torch.zeros((obs.size(0), 6), dtype=torch.float32)
  +    elif not torch.is_tensor(seat):
  +        seat = torch.as_tensor(seat, dtype=torch.float32)
  +    seat = seat.to(device=device)
  +    legals = batch['legals'] if torch.is_tensor(batch['legals']) else torch.as_tensor(batch['legals'], dtype=torch.float32)
  +    offs = batch['legals_offset'] if torch.is_tensor(batch['legals_offset']) else torch.as_tensor(batch['legals_offset'], dtype=torch.long)
  +    cnts = batch['legals_count'] if torch.is_tensor(batch['legals_count']) else torch.as_tensor(batch['legals_count'], dtype=torch.long)
  +    chosen_idx = batch['chosen_index'] if torch.is_tensor(batch['chosen_index']) else torch.as_tensor(batch['chosen_index'], dtype=torch.long)
  +    old_logp = torch.as_tensor(batch['old_logp'], dtype=torch.float32)
  +    legals = legals.to(device=device)
  +    offs = offs.to(device=device)
  +    cnts = cnts.to(device=device)
  +    chosen_idx = chosen_idx.to(device=device)
  +    old_logp = old_logp.to(device=device)
  +
  +    approx_kl = torch.zeros(obs.size(0), dtype=torch.float32, device=device)
  +    entropy = torch.zeros(obs.size(0), dtype=torch.float32, device=device)
  +    with torch.no_grad():
  +        B = obs.size(0)
  +        # Compute state projection (B,64) con belief neurale interno
  +        state_proj = agent.actor.compute_state_proj(obs, seat)
  +        # Action embeddings for all legals in the batch
  +        a_emb_global = agent.actor.action_enc(legals)  # (M_all,64)
  +        max_cnt = int(cnts.max().item()) if B > 0 else 0
  +        if max_cnt > 0:
  +            pos = torch.arange(max_cnt, device=device, dtype=torch.long)
  +            rel_pos_2d = pos.unsqueeze(0).expand(B, max_cnt)
  +            mask = rel_pos_2d < cnts.unsqueeze(1)
  +            sample_idx_per_legal = torch.arange(B, device=device, dtype=torch.long).unsqueeze(1).expand(B, max_cnt)[mask]
  +            abs_idx_2d = offs.unsqueeze(1) + rel_pos_2d
  +            abs_idx = abs_idx_2d[mask]
  +            a_emb_mb = a_emb_global[abs_idx]
  +            legal_scores = (a_emb_mb * state_proj[sample_idx_per_legal]).sum(dim=1)
  +            padded = torch.full((B, max_cnt), -float('inf'), device=device, dtype=legal_scores.dtype)
  +            padded[mask] = legal_scores
  +        else:
  +            padded = torch.full((B, 0), -float('inf'), device=device, dtype=state_proj.dtype)
  +        logp_group = torch.log_softmax(padded, dim=1)
  +        probs_group = torch.softmax(padded, dim=1)
  +        entropy[:] = (-(probs_group * logp_group).sum(dim=1))
  +        chosen_clamped = torch.minimum(chosen_idx, (cnts - 1).clamp_min(0)) if max_cnt > 0 else chosen_idx
  +        new_logp_chosen = logp_group[torch.arange(B, device=device), chosen_clamped]
  +        approx_kl[:] = (old_logp - new_logp_chosen).abs()
  +    # clip fraction per-sample sul chosen
  +    # ricalcolo ratio corretto: new_logp_chosen - old_logp
  +    ratio = torch.exp(new_logp_chosen - old_logp)
  +    clip_low = 1.0 - agent.clip_ratio
  +    clip_high = 1.0 + agent.clip_ratio
  +    clipped_mask = (ratio < clip_low) | (ratio > clip_high)
  +
  +    seats = torch.argmax(seat[:, :4], dim=1)
  +    mask_02 = (seats == 0) | (seats == 2)
  +    mask_13 = (seats == 1) | (seats == 3)
  +    out: Dict[str, torch.Tensor] = {}
  +    # Evita branching su CPU: usa conteggi e where per gestire gruppi vuoti
  +    one = torch.tensor(1.0, device=device, dtype=torch.float32)
  +    count_02 = mask_02.float().sum()
  +    count_13 = mask_13.float().sum()
  +    sum_kl_02 = (approx_kl * mask_02.float()).sum()
  +    sum_kl_13 = (approx_kl * mask_13.float()).sum()
  +    sum_en_02 = (entropy * mask_02.float()).sum()
  +    sum_en_13 = (entropy * mask_13.float()).sum()
  +    sum_cf_02 = (clipped_mask.float() * mask_02.float()).sum()
  +    sum_cf_13 = (clipped_mask.float() * mask_13.float()).sum()
  +    mean_kl_02 = torch.where(count_02 > 0, sum_kl_02 / count_02, torch.zeros((), device=device, dtype=torch.float32))
  +    mean_kl_13 = torch.where(count_13 > 0, sum_kl_13 / count_13, torch.zeros((), device=device, dtype=torch.float32))
  +    mean_en_02 = torch.where(count_02 > 0, sum_en_02 / count_02, torch.zeros((), device=device, dtype=torch.float32))
  +    mean_en_13 = torch.where(count_13 > 0, sum_en_13 / count_13, torch.zeros((), device=device, dtype=torch.float32))
  +    mean_cf_02 = torch.where(count_02 > 0, sum_cf_02 / count_02, torch.zeros((), device=device, dtype=torch.float32))
  +    mean_cf_13 = torch.where(count_13 > 0, sum_cf_13 / count_13, torch.zeros((), device=device, dtype=torch.float32))
  +    out['by_seat/kl_02'] = mean_kl_02
  +    out['by_seat/kl_13'] = mean_kl_13
  +    out['by_seat/entropy_02'] = mean_en_02
  +    out['by_seat/entropy_13'] = mean_en_13
  +    out['by_seat/clip_frac_02'] = mean_cf_02
  +    out['by_seat/clip_frac_13'] = mean_cf_13
  +    return out
  +
  +
  +def _load_frozen_actor(ckpt_path: str, obs_dim: int) -> ActionConditionedActor:
  +    actor = ActionConditionedActor(obs_dim=obs_dim)
  +    actor = maybe_compile_module(actor, name='ActionConditionedActor[trainer_partner]')
  +    try:
  +        ckpt = torch.load(ckpt_path, map_location=device)
  +        if isinstance(ckpt, dict) and 'actor' in ckpt:
  +            actor.load_state_dict(ckpt['actor'])
  +        # else: leave randomly init
  +    except Exception as e:
  +        # Detailed diagnostics on why loading failed
  +        import os as _os
  +        import pickle as _pickle  # noqa: F401
  +        exists = bool(ckpt_path and _os.path.isfile(ckpt_path))
  +        size_b = int(_os.path.getsize(ckpt_path)) if exists else -1
  +        etype = type(e).__name__
  +        emsg = str(e) if str(e) else repr(e)
  +        # Heuristics for human-readable reason
  +        if not exists:
  +            reason = "not_found"
  +        elif size_b <= 0:
  +            reason = "empty_file"
  +        elif etype == 'EOFError':
  +            reason = "truncated_or_corrupt"
  +        elif etype in ('UnpicklingError', 'AttributeError', 'ModuleNotFoundError'):
  +            reason = "invalid_pickle_or_missing_class"
  +        elif etype == 'RuntimeError' and ('PytorchStreamReader' in emsg or 'invalid header or archive' in emsg):
  +            reason = "invalid_archive_or_format"
  +        else:
  +            reason = "unknown_error"
  +        tqdm.write(f"[WARN] Failed to load checkpoint {ckpt_path}: reason={reason} type={etype} size={max(size_b,0)}B msg={emsg}")
  +        # Optional: escalate failure for CI/debug
  +        if _os.environ.get('SCOPONE_RAISE_ON_CKPT_FAIL', '0') == '1':
  +            raise RuntimeError(f"Checkpoint load failed: {ckpt_path} (reason={reason}, type={etype}, size={max(size_b,0)}B)") from e
  +    actor.eval()
  +    return actor
  +
  +
  +def _collect_trajectory_impl(env: ScoponeEnvMA, agent: ActionConditionedPPO, horizon: int = 128,
  +                       gamma: float = 1.0, lam: float = 0.95,
  +                       partner_actor: ActionConditionedActor = None,
  +                       opponent_actor: ActionConditionedActor = None,
  +                       main_seats: List[int] = None,
  +                       belief_particles: int = 512, belief_ess_frac: float = 0.5,
  +                       episodes: int = None, final_reward_only: bool = True,
  +                       use_mcts: bool = True,
  +                       mcts_sims: int = 128, mcts_dets: int = 4, mcts_c_puct: float = 1.0,
  +                       mcts_root_temp: float = 0.0, mcts_prior_smooth_eps: float = 0.0,
  +                       mcts_dirichlet_alpha: float = 0.25, mcts_dirichlet_eps: float = 0.0,
  +                       mcts_train_factor: float = 1.0,
  +                       mcts_progress_start: float = 0.25,
  +                       mcts_progress_full: float = 0.75,
  +                       mcts_min_sims: int = 0,
  +                       train_both_teams: bool = False) -> Dict:
  +    # Enforce minimum horizon of 40 (full hand length)
  +    horizon = max(40, int(horizon))
  +    # Enforce horizon multiple of LCM(minibatch_size, per-episode useful transitions)
  +    import os as _os
  +    import math as _math
  +    minibatch_size = 4096
  +    env_mb = int(_os.environ.get('SCOPONE_MINIBATCH', str(minibatch_size)))
  +    if env_mb > 0:
  +        minibatch_size = env_mb
  +    per_ep_util = 40 if bool(train_both_teams) else 20
  +    lcm_mb_ep = (abs(minibatch_size * per_ep_util) // _math.gcd(minibatch_size, per_ep_util)) if (minibatch_size > 0 and per_ep_util > 0) else max(minibatch_size, per_ep_util)
  +    if lcm_mb_ep > 0 and (horizon % lcm_mb_ep) != 0:
  +        new_h = ((horizon + lcm_mb_ep - 1) // lcm_mb_ep) * lcm_mb_ep
  +        global _HORIZON_ADJUST_LOGGED
  +        if not _HORIZON_ADJUST_LOGGED:
  +            tqdm.write(f"[horizon] adjusted to LCM(mb={minibatch_size}, per_ep={per_ep_util})={lcm_mb_ep}: {horizon} -> {new_h}")
  +            _HORIZON_ADJUST_LOGGED = True
  +        horizon = new_h
  +    # After alignment, horizon is divisible by per_ep_util by construction
  +    # Validate gamma/lam
  +    if float(gamma) < 0 or float(gamma) > 1:
  +        raise ValueError("collect_trajectory: gamma must be in [0,1]")
  +    if float(lam) < 0 or float(lam) > 1:
  +        raise ValueError("collect_trajectory: lam must be in [0,1]")
  +    obs_list, next_obs_list = [], []
  +    act_list = []
  +    rew_list, done_list = [], []
  +    legals_list, legals_offset, legals_count = [], [], []
  +    chosen_index_t_list = []
  +    seat_team_list = []
  +    belief_sum_list = []
  +    mcts_policy_flat = []  # concatenazione delle distribuzioni visite MCTS per ciascun sample main
  +    mcts_weight_list = []  # 1.0 se MCTS usato per il sample, altrimenti 0.0
  +    # supervision per belief aux: per ogni sample, vettore (3,40) one-hot di mani reali altrui
  +    others_hands_targets = []
  +
  +
  +    routing_log = []  # (player_id, source)
  +
  +    belief_aux_coef = float(os.environ.get('BELIEF_AUX_COEF', '0.1'))
  +    skip_step_validation = (os.environ.get('SCOPONE_SKIP_STEP_VALIDATION', '1') == '1')
  +
  +    # Profiling accumulators (shared naming with parallel collector for consistency)
  +    t_collect_start = time.time() if _PAR_TIMING else 0.0
  +    t_env_reset = 0.0
  +    t_get_obs = 0.0
  +    t_get_legals = 0.0
  +    t_mcts = 0.0
  +    t_step = 0.0
  +    t_select = 0.0
  +    t_build = 0.0
  +    t_state_proj = 0.0
  +    t_action_enc = 0.0
  +    t_sampling = 0.0
  +    t_values_gae = 0.0
  +    t_oldlogp = 0.0
  +    env_reset_count = 0
  +
  +    steps = 0
  +    if final_reward_only:
  +        # Raccogli per episodi completi: per-episodio util = 40 se alleni entrambe le squadre, altrimenti 20
  +        _per_ep_util = (40 if bool(train_both_teams) else 20)
  +        episodes = (max(1, horizon // _per_ep_util) if episodes is None else max(1, int(episodes)))
  +        episodes_done = 0
  +    # Tracciamento delle slice episodio e dei team rewards per reward flat
  +    current_ep_start_idx = 0
  +    ep_slices: List[Tuple[int, int]] = []
  +    ep_team_rewards: List[List[float]] = []
  +    while True:
  +        if env.done:
  +            if _PAR_TIMING:
  +                _t0_env_reset = time.time()
  +            env.reset()
  +            if _PAR_TIMING:
  +                t_env_reset += (time.time() - _t0_env_reset)
  +                env_reset_count += 1
  +            current_ep_start_idx = len(obs_list)
  +
  +        # All env logic on CPU
  +        if _PAR_TIMING:
  +            _t0_get_obs = time.time()
  +        obs = env._get_observation(env.current_player)
  +        if _PAR_TIMING:
  +            t_get_obs += (time.time() - _t0_get_obs)
  +        # Fast-path: get_valid_actions already caches by state; avoid recomputing identical lists
  +        if _PAR_TIMING:
  +            _t0_get_legals = time.time()
  +        legal = env.get_valid_actions()
  +        if _PAR_TIMING:
  +            t_get_legals += (time.time() - _t0_get_legals)
  +        if torch.is_tensor(legal) and (legal.size(0) == 0):
  +            cp = env.current_player
  +            hand_ids_dbg = list(env._hands_ids.get(cp, [])) if hasattr(env, '_hands_ids') else []
  +            table_ids_dbg = list(env._table_ids) if hasattr(env, '_table_ids') else []
  +            hist_len = len(env.game_state.get('history', [])) if isinstance(env.game_state, dict) else None
  +            rules_dbg = getattr(env, 'rules', {})
  +            raise RuntimeError(f"collect_trajectory: get_valid_actions returned 0 legals; player={cp}, hand={hand_ids_dbg}, table={table_ids_dbg}, done={bool(env.done)}, history_len={hist_len}, rules={rules_dbg}")
  +
  +        cp = env.current_player
  +        seat_vec = _seat_vec_for(cp)
  +
  +        # Selezione azione: se train_both_teams è True, tutti i seat sono "main"
  +        is_main = True if train_both_teams else ((main_seats is None and cp in [0, 2]) or (main_seats is not None and cp in main_seats))
  +        bsum_tensor = _BELIEF_ZERO
  +        use_mcts_cur = False
  +        sims_scaled = 0
  +        alpha = 0.0
  +        root_temp_dyn = float(mcts_root_temp)
  +        _both_sides = str(os.environ.get('SCOPONE_MCTS_BOTH_SIDES', '1')).strip().lower() in ['1','true','yes','on']
  +        if is_main or _both_sides:
  +            # MCTS sempre attivo (stile AlphaZero): poche simulazioni sempre, scala con il progresso della mano
  +            progress = float(min(1.0, max(0.0, len(env.game_state.get('history', [])) / 40.0)))
  +            if use_mcts and len(legal) > 0 and int(mcts_sims) > 0:
  +                if mcts_train_factor is None or float(mcts_train_factor) > 0.0:
  +                    denom = max(1e-6, (mcts_progress_full - mcts_progress_start))
  +                    alpha = min(1.0, max(0.0, (progress - mcts_progress_start) / denom))
  +                    import math
  +                    scaling_on = (str(os.environ.get('SCOPONE_MCTS_SCALING', '1')).strip().lower() in ['1','true','yes','on'])
  +                    if scaling_on:
  +                        sims_base = int(math.ceil(float(mcts_sims) * (0.25 + 0.75 * alpha)))
  +                        if mcts_train_factor is not None:
  +                            sims_base = int(math.ceil(sims_base * float(mcts_train_factor)))
  +                    else:
  +                        sims_base = int(math.ceil(float(mcts_sims) * float(mcts_train_factor if mcts_train_factor is not None else 1.0)))
  +                    base_min = int(mcts_min_sims) if (mcts_min_sims is not None and int(mcts_min_sims) >= 0) else 0
  +                    sims_scaled = max(base_min, sims_base)
  +                    if int(mcts_sims) > 0 and sims_scaled <= 0:
  +                        sims_scaled = 1
  +                    use_mcts_cur = sims_scaled > 0
  +                else:
  +                    sims_scaled = 0
  +            if use_mcts_cur:
  +                scaling_on = (str(os.environ.get('SCOPONE_MCTS_SCALING', '1')).strip().lower() in ['1','true','yes','on'])
  +                root_temp_dyn = float(mcts_root_temp) if (not scaling_on or float(mcts_root_temp) > 0) else float(max(0.0, 1.0 - alpha))
  +
  +            # Belief summary per il giocatore corrente: opzionale (disabilitato di default)
  +            _enable_bsum = (os.environ.get('ENABLE_BELIEF_SUMMARY', '0') == '1')
  +            if _enable_bsum and use_mcts_cur:
  +                o_cpu = obs.clone().detach().to('cpu', dtype=torch.float32) if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32, device='cpu')
  +                s_cpu = seat_vec.detach().to('cpu', dtype=torch.float32)
  +                if device.type == 'cuda':
  +                    o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
  +                    s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
  +                else:
  +                    o_t = o_cpu.unsqueeze(0).to(device=device)
  +                    s_t = s_cpu.unsqueeze(0).to(device=device)
  +                with torch.no_grad():
  +                    state_feat = agent.actor.state_enc(o_t, s_t)
  +                    bn_dtype = agent.actor.belief_net.fc_in.weight.dtype
  +                    if state_feat.dtype != bn_dtype:
  +                        state_feat = state_feat.to(dtype=bn_dtype)
  +                    logits = agent.actor.belief_net(state_feat)
  +                    hand_table = o_t[:, :83]
  +                    hand_mask = hand_table[:, :40] > 0.5
  +                    table_mask = hand_table[:, 43:83] > 0.5
  +                    captured = o_t[:, 83:165]
  +                    cap0_mask = captured[:, :40] > 0.5
  +                    cap1_mask = captured[:, 40:80] > 0.5
  +                    visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)
  +                    probs_flat = agent.actor.belief_net.probs(logits, visible_mask)
  +                bsum_tensor = probs_flat.squeeze(0).detach().to('cpu')
  +            if use_mcts_cur:
  +                if _PAR_TIMING:
  +                    _t0_mcts = time.time()
  +                # MCTS con determinizzazione dal belief del giocatore corrente
  +                from algorithms.is_mcts import run_is_mcts
  +                import numpy as _np
  +                # Policy: usa l'actor per generare prior sui legali
  +                def policy_fn_mcts(_obs, _legals):
  +                    # Prior coerenti con policy fattorizzata (carta ⊕ presa)
  +                    o_cpu = _obs if torch.is_tensor(_obs) else torch.as_tensor(_obs, dtype=torch.float32)
  +                    if torch.is_tensor(o_cpu):
  +                        o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
  +                    leg_cpu = torch.stack([
  +                        (x.detach().to('cpu', dtype=torch.float32) if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32))
  +                    for x in _legals], dim=0)
  +                    if device.type == 'cuda':
  +                        o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
  +                        leg_t = leg_cpu.pin_memory().to(device=device, non_blocking=True)
  +                    else:
  +                        o_t = o_cpu.unsqueeze(0).to(device=device)
  +                        leg_t = leg_cpu.to(device=device)
  +                    with torch.no_grad():
  +                        sp = agent.actor.compute_state_proj(o_t, _seat_vec_for(env.current_player).unsqueeze(0).to(device=device))
  +                        # Evita cast/copie ripetute del parametro
  +                        card_logits_all = torch.matmul(sp, agent.actor.card_emb_play.t()).squeeze(0)
  +                        played_ids = torch.argmax(leg_t[:, :40], dim=1)
  +                        unique_ids, inv_idx = torch.unique(played_ids, sorted=False, return_inverse=True)
  +                        allowed_logits = card_logits_all[unique_ids]
  +                        logp_cards_unique = torch.log_softmax(allowed_logits, dim=0)
  +                        logp_cards_per_legal = logp_cards_unique[inv_idx]
  +                        a_emb = agent.actor.action_enc(leg_t)
  +                        cap_logits = torch.matmul(a_emb, sp.squeeze(0))
  +                        group_ids = played_ids
  +                        num_groups = 40
  +                        group_max = torch.full((num_groups,), float('-inf'), dtype=cap_logits.dtype, device=leg_t.device)
  +                        group_max.scatter_reduce_(0, group_ids, cap_logits, reduce='amax', include_self=True)
  +                        gmax_per_legal = group_max[group_ids]
  +                        exp_shifted = torch.exp(cap_logits - gmax_per_legal).to(cap_logits.dtype)
  +                        group_sum = torch.zeros((num_groups,), dtype=cap_logits.dtype, device=leg_t.device)
  +                        group_sum.index_add_(0, group_ids, exp_shifted)
  +                        lse_per_legal = gmax_per_legal + torch.log(torch.clamp_min(group_sum[group_ids], 1e-12))
  +                        logp_cap_per_legal = cap_logits - lse_per_legal
  +                        logp_total = logp_cards_per_legal + logp_cap_per_legal
  +                        priors = torch.softmax(logp_total, dim=0).nan_to_num(0.0)
  +                        priors = torch.clamp(priors, min=0.0)
  +                        ssum = priors.sum()
  +                        if (not torch.isfinite(ssum)) or (ssum <= 0):
  +                            raise RuntimeError("collect_trajectory.policy_fn_mcts: invalid priors (NaN/Inf or zero-sum)")
  +                    return priors.detach().cpu().numpy()
  +                # Value: usa il critic con belief neurale interno e others_hands predetto
  +                def value_fn_mcts(_obs, _env):
  +                    # Prepara seat_team
  +                    o_cpu = _obs if torch.is_tensor(_obs) else torch.as_tensor(_obs, dtype=torch.float32)
  +                    if torch.is_tensor(o_cpu):
  +                        o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
  +                    # seat vector
  +                    seat_vec = torch.zeros(6, dtype=torch.float32)
  +                    cp_loc = _env.current_player
  +                    seat_vec[cp_loc] = 1.0
  +                    seat_vec[4] = 1.0 if cp_loc in [0, 2] else 0.0
  +                    seat_vec[5] = 1.0 if cp_loc in [1, 3] else 0.0
  +                    # To CUDA
  +                    if device.type == 'cuda':
  +                        o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
  +                        s_t = seat_vec.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
  +                    else:
  +                        o_t = o_cpu.unsqueeze(0).to(device=device)
  +                        s_t = seat_vec.unsqueeze(0).to(device=device)
  +                    # Costruisci others_hands predetto dal BeliefNet con masking delle carte visibili
  +                    with torch.no_grad():
  +                        state_feat = agent.actor.state_enc(o_t, s_t)
  +                        bn_dtype = agent.actor.belief_net.fc_in.weight.dtype
  +                        if state_feat.dtype != bn_dtype:
  +                            state_feat = state_feat.to(dtype=bn_dtype)
  +                        logits = agent.actor.belief_net(state_feat)  # (1,120)
  +                        hand_table = o_t[:, :83]
  +                        hand_mask = hand_table[:, :40] > 0.5
  +                        table_mask = hand_table[:, 43:83] > 0.5
  +                        captured = o_t[:, 83:165]
  +                        cap0_mask = captured[:, :40] > 0.5
  +                        cap1_mask = captured[:, 40:80] > 0.5
  +                        visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)  # (1,40)
  +                        probs_flat = agent.actor.belief_net.probs(logits, visible_mask)  # (1,120)
  +                        oh = probs_flat.view(1, 3, 40)
  +                        v = agent.critic(o_t, s_t, oh)
  +                    return float(v.squeeze(0).detach().cpu().item())
--                 # Belief determinization sampler dal BeliefNet: campiona assignment coerenti
+++                # Belief determinization sampler gerarchico
  +                def belief_sampler_neural(_env):
--                     # Costruisci marginali 3x40 dal BeliefNet
--                     obs_cur = _env._get_observation(_env.current_player)
--                     o_cpu = obs_cur if torch.is_tensor(obs_cur) else torch.as_tensor(obs_cur, dtype=torch.float32)
--                     if torch.is_tensor(o_cpu):
--                         o_cpu = o_cpu.detach().to('cpu', dtype=torch.float32)
--                     s_cpu = _seat_vec_for(_env.current_player).detach().to('cpu', dtype=torch.float32)
--                     if device.type == 'cuda':
--                         o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                         s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
--                     else:
--                         o_t = o_cpu.unsqueeze(0).to(device=device)
--                         s_t = s_cpu.unsqueeze(0).to(device=device)
--                     with torch.no_grad():
--                         state_feat = agent.actor.state_enc(o_t, s_t)
--                         bn_dtype = agent.actor.belief_net.fc_in.weight.dtype
--                         if state_feat.dtype != bn_dtype:
--                             state_feat = state_feat.to(dtype=bn_dtype)
--                         logits = agent.actor.belief_net(state_feat)  # (1,120)
--                         # Visibilità dall'osservazione
--                         hand_table = o_t[:, :83]
--                         hand_mask = hand_table[:, :40] > 0.5
--                         table_mask = hand_table[:, 43:83] > 0.5
--                         captured = o_t[:, 83:165]
--                         cap0_mask = captured[:, :40] > 0.5
--                         cap1_mask = captured[:, 40:80] > 0.5
--                         visible_mask = (hand_mask | table_mask | cap0_mask | cap1_mask)  # (1,40)
--                         probs_flat = agent.actor.belief_net.probs(logits, visible_mask)  # (1,120)
--                     probs = probs_flat.view(3, 40).detach().cpu().numpy()  # (3,40)
--                     vis = visible_mask.squeeze(0).detach().cpu().numpy().astype(bool)
--                     unknown_ids = [cid for cid in range(40) if not vis[cid]]
--                     # Capacità (conteggi mano) correnti degli altri giocatori
--                     others = [(_env.current_player + 1) % 4, (_env.current_player + 2) % 4, (_env.current_player + 3) % 4]
--                     counts = {pid: len(_env.game_state['hands'][pid]) for pid in others}
--                     caps = [int(counts.get(pid, 0)) for pid in others]
--                     n = len(unknown_ids)
--                     # riallinea capacità se necessario
--                     if sum(caps) != n:
--                         caps[2] = max(0, n - caps[0] - caps[1])
--                         if sum(caps) != n:
--                             from utils.fallback import notify_fallback
--                             notify_fallback('trainer.worker.belief_sampler.uniform_caps')
--                     # Costi = -log p con piccolo rumore per diversità
+++                    alpha = float(os.environ.get('SCOPONE_BELIEF_BLEND_ALPHA', '0.65'))
  +                    noise_scale = float(os.environ.get('DET_NOISE', '0.0'))
--                     costs = []  # shape (n,3)
--                     for cid in unknown_ids:
--                         pc = probs[:, cid]
--                         ps = pc / max(1e-12, pc.sum())
--                         c = [-_np.log(max(1e-12, ps[i])) for i in range(3)]
--                         if noise_scale > 0:
--                             u = _np.random.uniform(1e-9, 1.0-1e-9, size=3)
--                             g = -_np.log(-_np.log(u)) * noise_scale
--                             c = [c[i] + float(g[i]) for i in range(3)]
--                         costs.append(c)
--                     # DP ottimo per 3 giocatori con capacità note
--                     INF = 1e12
--                     cap0, cap1, cap2 = caps
--                     dp = [[[INF]*(cap1+1) for _ in range(cap0+1)] for __ in range(n+1)]
--                     bk = [[[-1]*(cap1+1) for _ in range(cap0+1)] for __ in range(n+1)]
--                     dp[0][0][0] = 0.0
--                     for t in range(n):
--                         c0, c1, c2 = costs[t]
--                         for a in range(0, min(t, cap0)+1):
--                             for b in range(0, min(t-a, cap1)+1):
--                                 cur = dp[t][a][b]
--                                 if cur >= INF: 
--                                     continue
--                                 # assign to player 0
--                                 if a+1 <= cap0:
--                                     if dp[t+1][a+1][b] > cur + c0:
--                                         dp[t+1][a+1][b] = cur + c0
--                                         bk[t+1][a+1][b] = 0
--                                 # assign to player 1
--                                 if b+1 <= cap1:
--                                     if dp[t+1][a][b+1] > cur + c1:
--                                         dp[t+1][a][b+1] = cur + c1
--                                         bk[t+1][a][b+1] = 1
--                                 # assign to player 2 (implicit count)
--                                 assigned2 = t - a - b
--                                 if assigned2 + 1 <= cap2:
--                                     if dp[t+1][a][b] > cur + c2:
--                                         dp[t+1][a][b] = cur + c2
--                                         bk[t+1][a][b] = 2
--                     if dp[n][cap0][cap1] >= INF:
--                         from utils.fallback import notify_fallback
--                         notify_fallback('trainer.belief_sampler.dp_infeasible')
--                     # Ricostruisci percorso
--                     det = {pid: [] for pid in others}
--                     a, b = cap0, cap1
--                     for t in range(n, 0, -1):
--                         choice = bk[t][a][b]
--                         cid = unknown_ids[t-1]
--                         if choice == 0:
--                             det[others[0]].append(cid)
--                             a -= 1
--                         elif choice == 1:
--                             det[others[1]].append(cid)
--                             b -= 1
--                         else:
--                             det[others[2]].append(cid)
--                     return det
+++                    return sample_determinization(_env, alpha=alpha, noise_scale=noise_scale)
  +                # temperatura radice dinamica: alta a inizio mano, bassa verso la fine (solo se scaling attivo)
  +                scaling_on = (str(os.environ.get('SCOPONE_MCTS_SCALING', '1')).strip().lower() in ['1','true','yes','on'])
  +                root_temp_dyn = float(mcts_root_temp) if (not scaling_on or float(mcts_root_temp) > 0) else float(max(0.0, 1.0 - alpha))
  +                # Auto-tune exploration at root: smoothing and Dirichlet
  +                priors_probe = policy_fn_mcts(obs, legal)
  +                pri_t = (priors_probe if torch.is_tensor(priors_probe) else torch.as_tensor(priors_probe, dtype=torch.float32))
  +                peak = float(pri_t.max().item()) if pri_t.numel() > 0 else (1.0 / max(1, len(legal)))
  +                A = int(len(legal))
  +                sims_fac = 1.0 if sims_scaled < 128 else (0.5 if sims_scaled < 256 else 0.25)
  +                peak_fac = min(1.0, max(0.0, (peak - 0.5) / 0.4))
  +                prior_eps_dyn = 0.1 * sims_fac * peak_fac * (1.0 - alpha)
  +                prior_eps_dyn = float(max(0.0, min(0.15, prior_eps_dyn)))
  +                if A <= 3:
  +                    dir_eps_dyn = 0.0
  +                else:
  +                    a_fac = min(1.0, max(0.0, (A - 3) / 10.0))
  +                    sim_att = (0.7 if sims_scaled >= 256 else 1.0)
  +                    prog_att = (0.7 if alpha > 0.7 else 1.0)
  +                    dir_eps_dyn = 0.25 * a_fac * sim_att * prog_att
  +                    dir_eps_dyn = float(max(0.0, min(0.3, dir_eps_dyn)))
  +                # Respect explicit overrides if provided (>0)
  +                prior_eps_eff = float(mcts_prior_smooth_eps) if float(mcts_prior_smooth_eps) > 0 else prior_eps_dyn
  +                dir_eps_eff = float(mcts_dirichlet_eps) if float(mcts_dirichlet_eps) > 0 else dir_eps_dyn
  +
  +                mcts_action, mcts_visits = run_is_mcts(env,
  +                                          policy_fn=policy_fn_mcts,
  +                                          value_fn=value_fn_mcts,
  +                                          num_simulations=int(sims_scaled),
  +                                          c_puct=float(mcts_c_puct),
  +                                          belief=None,
  +                                          num_determinization=int(mcts_dets),
  +                                          root_temperature=root_temp_dyn,
  +                                          prior_smooth_eps=prior_eps_eff,
  +                                          robust_child=True,
  +                                          root_dirichlet_alpha=float(mcts_dirichlet_alpha),
  +                                          root_dirichlet_eps=dir_eps_eff,
  +                                          return_stats=True,
  +                                          belief_sampler=belief_sampler_neural)
  +                chosen_act = mcts_action if torch.is_tensor(mcts_action) else torch.as_tensor(mcts_action, dtype=torch.float32)
  +                # trova indice dell'azione scelta tra i legali in O(A) vettoriale
  +                # Bitset hashing mapping: encode action as (played_id, capture_bits) to find index in O(A)
  +                def _encode_action_64(vec80: torch.Tensor) -> torch.Tensor:
  +                    played_id = torch.argmax(vec80[:40]).to(torch.int64)
  +                    cap_mask = (vec80[40:] > 0.5).to(torch.int64)
  +                    # pack 40 capture bits into 64-bit integer
  +                    idxs = torch.arange(40, dtype=torch.int64)
  +                    bits = (cap_mask << idxs).sum()
  +                    return (played_id | (bits << 6)).to(torch.int64)
  +                code_ch = _encode_action_64(chosen_act)
  +                if torch.is_tensor(legal):
  +                    legals_t = legal
  +                else:
  +                    legals_t = torch.stack([(x if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32)) for x in legal], dim=0)
  +                # vectorized encode for legals
  +                played_ids = torch.argmax(legals_t[:, :40], dim=1).to(torch.int64)
  +                cap_mask = (legals_t[:, 40:] > 0.5).to(torch.int64)
  +                idxs = torch.arange(40, dtype=torch.int64).unsqueeze(0)
  +                bits = (cap_mask << idxs).sum(dim=1)
  +                codes = (played_ids | (bits << 6)).to(torch.int64)
  +                matches = (codes == code_ch)
  +                nz = torch.nonzero(matches, as_tuple=False).flatten()
  +                idx_t = (nz[0] if nz.numel() > 0 else torch.tensor(0, dtype=torch.long))
  +                if _PAR_TIMING:
  +                    _t0_step = time.time()
  +                next_obs, rew, done, info = env.step(chosen_act)
  +                if _PAR_TIMING:
  +                    t_step += (time.time() - _t0_step)
  +                routing_log.append((cp, 'mcts'))
  +                # registra target distillazione per questo sample (ordine legali corrente)
  +                mcts_probs = torch.as_tensor(mcts_visits, dtype=torch.float32)
  +                # normalizza in caso di degenerazione
  +                s = float(mcts_probs.sum().item())
  +                if s > 0:
  +                    mcts_probs = mcts_probs / s
  +                mcts_policy_flat.extend((mcts_probs.tolist() if hasattr(mcts_probs, 'tolist') else list(mcts_probs)))
  +                mcts_weight_list.append(1.0)
  +                if _PAR_TIMING:
  +                    t_mcts += (time.time() - _t0_mcts)
  +            else:
  +                if _PAR_TIMING:
  +                    prof_bins = {'state_proj': 0.0, 'action_enc': 0.0, 'sampling': 0.0}
  +                    _t0_select = time.time()
  +                else:
  +                    prof_bins = None
  +                chosen_act, _logp, idx_t = agent.select_action(obs, legal, seat_vec, profiling_bins=prof_bins)
  +                if _PAR_TIMING:
  +                    select_elapsed = time.time() - _t0_select
  +                    t_select += select_elapsed
  +                    t_state_proj += prof_bins.get('state_proj', 0.0)
  +                    t_action_enc += prof_bins.get('action_enc', 0.0)
  +                    t_sampling += prof_bins.get('sampling', 0.0)
  +                    _t0_step = time.time()
  +                next_obs, rew, done, info = env.step(chosen_act)
  +                if _PAR_TIMING:
  +                    t_step += (time.time() - _t0_step)
  +                routing_log.append((cp, 'main'))
  +                # Per mantenere l'allineamento per-sample della distillazione, aggiungi zeri
  +                mcts_policy_flat.extend([0.0] * len(legal))
  +                mcts_weight_list.append(0.0)
  +
  +            obs_list.append(obs)
  +            next_obs_list.append(next_obs)
  +            act_list.append(chosen_act)
  +            rew_list.append(rew)
  +            done_list.append(done)
  +            seat_team_list.append(seat_vec)
  +            belief_sum_list.append(bsum_tensor)
  +            legals_offset.append(len(legals_list))
  +            legals_count.append(len(legal))
  +            chosen_index_t_list.append(idx_t)
  +            legals_list.extend(legal)
  +            # costruisci target mani reali altrui (3x40) vettoriale sfruttando bitset CPU
  +            if belief_aux_coef <= 0.0:
  +                others_hands_targets.append(_OTHERS_HANDS_ZERO)
  +            else:
  +                others = [ (cp + 1) % 4, (cp + 2) % 4, (cp + 3) % 4 ]
  +                # usa mirror bitset CPU se disponibile per evitare loop Python
  +                if hasattr(env, '_hands_bits_t') and hasattr(env, '_id_range'):
  +                    ids = env._id_range.detach().to('cpu', dtype=torch.long) if torch.is_tensor(env._id_range) else torch.arange(40, dtype=torch.long)
  +                    target = _OTHERS_HANDS_ZERO.clone()
  +                    for i, pid in enumerate(others):
  +                        bits_t = env._hands_bits_t[pid]
  +                        bits = int(bits_t.item()) if torch.is_tensor(bits_t) else int(bits_t)
  +                        mask = (((torch.tensor(bits, dtype=torch.int64) >> ids) & 1).to(torch.float32))
  +                        target[i] = mask
  +                    others_hands_targets.append(target)
  +                else:
  +                    from utils.fallback import notify_fallback
  +                    notify_fallback('trainer.others_hands_targets.slow_game_state_path')
  +                    others_hands_targets.append(_OTHERS_HANDS_ZERO)
  +        else:
  +            # partner congelato sui seat del compagno; opponent sugli avversari
  +            is_partner_seat = (cp in [0, 2] and (main_seats == [1, 3])) or (cp in [1, 3] and (main_seats == [0, 2]))
  +            frozen = partner_actor if (is_partner_seat and partner_actor is not None) else opponent_actor
  +            if frozen is not None:
  +                # Fast, inference-only path for frozen actor
  +                _inference = torch.inference_mode if hasattr(torch, 'inference_mode') else torch.no_grad
  +                with _inference():
  +                    # Use GPU for frozen actor scoring but keep env data on CPU
  +                    o_cpu = obs.clone().detach().to('cpu', dtype=torch.float32) if torch.is_tensor(obs) else torch.as_tensor(obs, dtype=torch.float32, device='cpu')
  +                    leg_cpu = torch.stack([x if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32, device='cpu') for x in legal], dim=0)
  +                    if device.type == 'cuda':
  +                        o_t = o_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
  +                        leg_t = leg_cpu.pin_memory().to(device=device, non_blocking=True)
  +                    else:
  +                        o_t = o_cpu.unsqueeze(0).to(device=device)
  +                        leg_t = leg_cpu.to(device=device)
  +                    # Build seat_team_vec (B,6) for the frozen actor forward
  +                    s_cpu = seat_vec.clone().detach().to('cpu', dtype=torch.float32)
  +                    if device.type == 'cuda':
  +                        s_t = s_cpu.pin_memory().unsqueeze(0).to(device=device, non_blocking=True)
  +                    else:
  +                        s_t = s_cpu.unsqueeze(0).to(device=device)
  +                    logits = frozen(o_t, leg_t, s_t)
  +                    idx_t = torch.argmax(logits).to('cpu')
  +                    act = leg_cpu[idx_t]
  +            else:
  +                idx_t = torch.randint(len(legal), (1,), device='cpu').squeeze(0)
  +                leg_t = torch.stack([
  +                    x if torch.is_tensor(x) else torch.as_tensor(x, dtype=torch.float32, device='cpu')
  +                for x in legal], dim=0)
  +                act = leg_t[idx_t]
  +                # Fast chosen index via 64-bit action code (played_id | capture_bits<<6)
  +                def _encode_action_64_cpu(vec80: torch.Tensor) -> torch.Tensor:
  +                    played_id = torch.argmax(vec80[:40]).to(torch.int64)
  +                    cap_mask = (vec80[40:] > 0.5).to(torch.int64)
  +                    idxs = torch.arange(40, dtype=torch.int64)
  +                    bits = (cap_mask << idxs).sum()
  +                    return (played_id | (bits << 6)).to(torch.int64)
  +                code_ch = _encode_action_64_cpu(act)
  +                played_ids = torch.argmax(leg_t[:, :40], dim=1).to(torch.int64)
  +                cap_mask = (leg_t[:, 40:] > 0.5).to(torch.int64)
  +                idxs = torch.arange(40, dtype=torch.int64).unsqueeze(0)
  +                bits = (cap_mask << idxs).sum(dim=1)
  +                codes = (played_ids | (bits << 6)).to(torch.int64)
  +                matches = (codes == code_ch)
  +                nz = torch.nonzero(matches, as_tuple=False).flatten()
  +                idx_t = (nz[0] if nz.numel() > 0 else torch.tensor(0, dtype=torch.long))
  +            if _PAR_TIMING:
  +                _t0_step = time.time()
  +            next_obs, rew, done, info = env.step(act)
  +            if _PAR_TIMING:
  +                t_step += (time.time() - _t0_step)
  +            routing_log.append((cp, 'partner' if is_partner_seat else 'opponent'))
  +            # Per i seat non-main mantieni l'allineamento dei target
  +            mcts_policy_flat.extend([0.0] * len(legal))
  +            mcts_weight_list.append(0.0)
  +
  +            # Quando l'episodio termina su un seat non-main, marca l'ultimo
  +            # sample "main" come terminale e azzera la sua next_obs per evitare
  +            # che il GAE bootstrappi sul valore dello stato dell'avversario.
  +            if done and len(done_list) > 0:
  +                done_list[-1] = True
  +                try:
  +                    final_obs = _to_cpu_float32(next_obs)
  +                except Exception:
  +                    final_obs = torch.zeros_like(next_obs_list[-1]) if len(next_obs_list) > 0 else _to_cpu_float32(next_obs)
  +                if len(next_obs_list) > 0:
  +                    next_obs_list[-1] = final_obs
  +
  +        steps += 1
  +        # Condizioni di uscita: per episodi o per passi
  +        if final_reward_only:
  +            if done:
  +                # Registra i confini dell'episodio corrente e i team rewards
  +                ep_slices.append((current_ep_start_idx, len(obs_list)))
  +                tr = info.get('team_rewards', [0.0, 0.0]) if isinstance(info, dict) else [0.0, 0.0]
  +                ep_team_rewards.append(tr)
  +                if episodes_done >= (episodes - 1):
  +                    break
  +                else:
  +                    episodes_done += 1
  +                    current_ep_start_idx = len(obs_list)
  +                    continue
  +        else:
  +            if steps >= horizon:
  +                break
  +
  +    # Precompute others_hands tensor (CPU) for CTDE targets and critic calls
  +    oh_cpu_tensor = torch.stack(others_hands_targets, dim=0) if len(others_hands_targets) > 0 else None
  +
  +    # CTDE: stima V(next) vettorizzata su GPU
  +    next_val_t = None
  +    oh_dev_tensor: Optional[torch.Tensor] = None
  +    if len(next_obs_list) > 0:
  +        with torch.no_grad():
  +            next_obs_t = torch.stack([torch.as_tensor(no, dtype=torch.float32, device=device) for no in next_obs_list], dim=0)
  +            s_all = torch.stack(seat_team_list, dim=0)
  +            done_mask_bool = torch.as_tensor([bool(d) for d in done_list], dtype=torch.bool, device=device)
  +            if oh_cpu_tensor is not None:
  +                if device.type == 'cuda':
  +                    oh_dev_tensor = oh_cpu_tensor.pin_memory().to(device=device, dtype=torch.float32, non_blocking=True)
  +                else:
  +                    oh_dev_tensor = oh_cpu_tensor.to(device=device, dtype=torch.float32)
  +                oh_next = torch.zeros_like(oh_dev_tensor)
  +                if oh_next.size(0) > 1:
  +                    oh_next[:-1] = oh_dev_tensor[1:]
  +            else:
  +                oh_dev_tensor = None
  +                oh_next = None
  +            next_val_t = agent.critic(next_obs_t, s_all, oh_next)
  +            next_val_t = torch.where(done_mask_bool, torch.zeros_like(next_val_t), next_val_t)
  +
  +    # Compute V(obs) in batch su GPU e GAE
  +    T = len(rew_list)
  +    # Consistency check: numero di transizioni utili per episodio (basato su obs_list)
  +    if final_reward_only:
  +        per_ep_util = 40 if bool(train_both_teams) else 20
  +        expected_util = int(episodes) * int(per_ep_util)
  +        obs_len = int(len(obs_list))
  +        if obs_len != expected_util:
  +            import torch as _t
  +            if len(seat_team_list) > 0:
  +                st = _t.stack(seat_team_list, dim=0)
  +                seat_idx = _t.argmax(st[:, :4], dim=1)
  +                c0 = int((seat_idx == 0).sum().item())
  +                c1 = int((seat_idx == 1).sum().item())
  +                c2 = int((seat_idx == 2).sum().item())
  +                c3 = int((seat_idx == 3).sum().item())
  +                seat_counts = (c0, c1, c2, c3)
  +            else:
  +                seat_counts = (0, 0, 0, 0)
  +            # distribuzione lunghezze episodiche
  +            ep_lengths = [int(j - i) for (i, j) in ep_slices]
  +            uniq = sorted(set(ep_lengths))
  +            freq = {L: ep_lengths.count(L) for L in uniq}
  +            raise RuntimeError(
  +                f"collect_trajectory: util transitions mismatch; got_obs={obs_len}, expected={expected_util}; "
  +                f"episodes_completed={len(ep_slices)} (requested={int(episodes)}), per_ep_expected={per_ep_util}, "
  +                f"length_stats={freq}, seat_counts={seat_counts}"
  +            )
  +        # I done provenienti dall'env possono rimanere tutti False quando l'ultimo
  +        # step utile dell'episodio è giocato da un seat non-main. In tal caso il
  +        # loop salta quel passo e la flag done non viene mai registrata, causando
  +        # un'unica traccia lunga attraverso più episodi. Forza quindi il flag
  +        # done sull'ultimo sample di ciascun episodio raccolto, così che il GAE
  +        # venga azzerato correttamente tra un episodio e il successivo.
  +        if len(ep_slices) > 0 and len(done_list) > 0:
  +            for start, end in ep_slices:
  +                if end > start:
  +                    done_list[end - 1] = True
  +            if next_val_t is not None:
  +                # Assicura che le stime V(s') dei terminali rispettino il flag done forzato
  +                updated_done_mask = torch.as_tensor(done_list, dtype=torch.bool, device=next_val_t.device)
  +                next_val_t = torch.where(updated_done_mask, torch.zeros_like(next_val_t), next_val_t)
  +    # Costruisci reward flat ±1 per tutte le transizioni dell'episodio, per entrambi i team
  +    if T > 0 and len(ep_slices) > 0:
  +        flat_rew = [0.0] * T
  +        for i_ep, (s, e) in enumerate(ep_slices):
  +            tr = ep_team_rewards[i_ep] if i_ep < len(ep_team_rewards) else [0.0, 0.0]
  +            # Determina la reward per team 0 e team 1 (proporzionale al risultato finale)
  +            t0 = float(tr[0])
  +            t1 = float(tr[1])
  +            for i in range(s, e):
  +                st = seat_team_list[i]
  +                team0_flag = bool(float(st[4]) > 0.5)
  +                flat_rew[i] = t0 if team0_flag else t1
  +        rew_t = torch.as_tensor(flat_rew, dtype=torch.float32, device=device)
  +    else:
  +        rew_t = torch.as_tensor(rew_list, dtype=torch.float32, device=device) if T>0 else torch.zeros((0,), dtype=torch.float32, device=device)
  +    done_mask = torch.as_tensor([0.0 if not d else 1.0 for d in done_list], dtype=torch.float32, device=device) if T>0 else torch.zeros((0,), dtype=torch.float32, device=device)
  +    if T > 0:
  +        with torch.no_grad():
  +            o_all = torch.stack([torch.as_tensor(o, dtype=torch.float32, device=device) for o in obs_list], dim=0)
  +            s_all = torch.stack(seat_team_list, dim=0)
  +            # others_hands per-step (CTDE)
  +            if oh_dev_tensor is None and oh_cpu_tensor is not None:
  +                if device.type == 'cuda':
  +                    oh_dev_tensor = oh_cpu_tensor.pin_memory().to(device=device, dtype=torch.float32, non_blocking=True)
  +                else:
  +                    oh_dev_tensor = oh_cpu_tensor.to(device=device, dtype=torch.float32)
  +            val_t = agent.critic(o_all, s_all, oh_dev_tensor)
  +            nval_t = next_val_t if next_val_t is not None else torch.zeros_like(val_t)
  +    else:
  +        val_t = torch.zeros((0,), dtype=torch.float32, device=device)
  +        nval_t = torch.zeros((0,), dtype=torch.float32, device=device)
  +    adv_vec = torch.zeros_like(rew_t)
  +    gae = torch.tensor(0.0, dtype=torch.float32, device=device)
  +    for t in reversed(range(T)):
  +        delta = rew_t[t] + gamma * nval_t[t] - val_t[t]
  +        gae = delta + gamma * lam * (1.0 - done_mask[t]) * gae
  +        adv_vec[t] = gae
  +    ret_vec = adv_vec + val_t
  +
  +    # Keep batch entirely as torch tensors on CUDA
  +    # Build CPU tensors first, then pin and transfer as a batch later in update
  +    if _PAR_TIMING:
  +        _t0_build = time.time()
  +    obs_cpu = torch.stack([o if torch.is_tensor(o) else torch.as_tensor(o, dtype=torch.float32) for o in obs_list], dim=0) if len(obs_list)>0 else torch.zeros((0, env.observation_space.shape[0]), dtype=torch.float32)
  +    act_cpu = torch.stack([a if torch.is_tensor(a) else torch.as_tensor(a, dtype=torch.float32) for a in act_list], dim=0) if len(act_list)>0 else torch.zeros((0, 80), dtype=torch.float32)
  +    legals_cpu = torch.stack([l if torch.is_tensor(l) else torch.as_tensor(l, dtype=torch.float32) for l in legals_list], dim=0) if legals_list else _EMPTY_LEGAL
  +    seat_team_cpu = torch.stack(seat_team_list, dim=0) if len(seat_team_list)>0 else torch.zeros((0,6), dtype=torch.float32)
  +    belief_sum_cpu = torch.stack(belief_sum_list, dim=0) if len(belief_sum_list)>0 else torch.zeros((0,120), dtype=torch.float32)
  +    legals_offset_cpu = torch.as_tensor(legals_offset, dtype=torch.long) if len(legals_offset)>0 else torch.zeros((0,), dtype=torch.long)
  +    legals_count_cpu = torch.as_tensor(legals_count, dtype=torch.long) if len(legals_count)>0 else torch.zeros((0,), dtype=torch.long)
  +    chosen_index_cpu = (torch.stack(chosen_index_t_list, dim=0).to(dtype=torch.long) if len(chosen_index_t_list)>0 else torch.zeros((0,), dtype=torch.long))
  +    # Evita tensori inutili pieni di zeri quando MCTS non è usato
  +    mcts_policy_cpu = torch.as_tensor(mcts_policy_flat, dtype=torch.float32) if any((x != 0.0) for x in mcts_policy_flat) else torch.zeros((0,), dtype=torch.float32)
  +    mcts_weight_cpu = torch.as_tensor(mcts_weight_list, dtype=torch.float32) if any((x != 0.0) for x in mcts_weight_list) else torch.zeros((0,), dtype=torch.float32)
  +    if oh_cpu_tensor is not None:
  +        others_hands_cpu = oh_cpu_tensor
  +    else:
  +        others_hands_cpu = torch.zeros((0,3,40), dtype=torch.float32)
  +    if _PAR_TIMING:
  +        t_build += (time.time() - _t0_build)
  +    # Sanitizza lunghezze: policy_flat deve avere somma(cnts) elementi e weight deve avere len(obs)
  +    total_legals = int(legals_count_cpu.sum().item()) if len(legals_count_cpu) > 0 else 0
  +    if mcts_policy_cpu.numel() != total_legals:
  +        if mcts_policy_cpu.numel() > total_legals:
  +            mcts_policy_cpu = mcts_policy_cpu[:total_legals]
  +        else:
  +            pad = torch.zeros((total_legals - mcts_policy_cpu.numel(),), dtype=torch.float32)
  +            mcts_policy_cpu = torch.cat([mcts_policy_cpu, pad], dim=0)
  +    total_samples = len(obs_list)
  +    if mcts_weight_cpu.numel() != total_samples:
  +        if mcts_weight_cpu.numel() > total_samples:
  +            mcts_weight_cpu = mcts_weight_cpu[:total_samples]
  +        else:
  +            padw = torch.zeros((total_samples - mcts_weight_cpu.numel(),), dtype=torch.float32)
  +            mcts_weight_cpu = torch.cat([mcts_weight_cpu, padw], dim=0)
  +    # Keep rewards/done on GPU for GAE already computed; store CPU too for logging if needed
  +    ret_t = ret_vec
  +    adv_t = adv_vec
  +    rew_t = rew_t
  +    done_t = torch.as_tensor(done_list, dtype=torch.bool, device=device) if len(done_list)>0 else torch.zeros((0,), dtype=torch.bool, device=device)
  +
  +    # Calcola old_logp in batch per evitare sincronizzazioni step-by-step (transfer to CUDA once)
  +    if obs_cpu.size(0) > 0:
  +        with torch.no_grad():
  +            # Ensure source are CPU tensors before pinning
  +            def to_pinned(x):
  +                x_cpu = (x.detach().to('cpu') if torch.is_tensor(x) else torch.as_tensor(x))
  +                return x_cpu.pin_memory() if device.type == 'cuda' else x_cpu
  +            nb = (device.type == 'cuda')
  +            obs_t = to_pinned(obs_cpu).to(device=device, dtype=torch.float32, non_blocking=nb)
  +            seat_team_t = to_pinned(seat_team_cpu).to(device=device, non_blocking=nb)
  +            legals_t = to_pinned(legals_cpu).to(device=device, non_blocking=nb)
  +            legals_offset_t = to_pinned(legals_offset_cpu).to(device=device, non_blocking=nb)
  +            legals_count_t = to_pinned(legals_count_cpu).to(device=device, non_blocking=nb)
  +            chosen_index_t = to_pinned(chosen_index_cpu).to(device=device, non_blocking=nb)
  +            # belief summary may be absent
  +            # belief non più richiesto per calcolare old_logp
  +            # Two-stage old_logp
  +            B = obs_t.size(0)
  +            max_cnt = int(legals_count_t.max().item()) if B > 0 else 0
  +            if max_cnt > 0:
  +                state_proj = agent.actor.compute_state_proj(obs_t, seat_team_t)  # (B,64)
  +                card_logits_all = torch.matmul(state_proj, agent.actor.card_emb_play.t())  # (B,40)
  +                pos = torch.arange(max_cnt, device=device, dtype=torch.long)
  +                rel_pos_2d = pos.unsqueeze(0).expand(B, max_cnt)
  +                mask = rel_pos_2d < legals_count_t.unsqueeze(1)
  +                abs_idx = (legals_offset_t.unsqueeze(1) + rel_pos_2d)[mask]
  +                sample_idx_per_legal = torch.arange(B, device=device, dtype=torch.long).unsqueeze(1).expand(B, max_cnt)[mask]
  +                legals_mb = legals_t[abs_idx].contiguous()
  +                played_ids_mb = torch.argmax(legals_mb[:, :40], dim=1)
  +                # Card log-prob restricted to allowed cards only (two-stage policy)
  +                allowed_mask = torch.zeros((B, 40), dtype=torch.bool, device=device)
  +                allowed_mask[sample_idx_per_legal, played_ids_mb] = True
  +                neg_inf = torch.full_like(card_logits_all, float('-inf'))
  +                masked_logits = torch.where(allowed_mask, card_logits_all, neg_inf)
  +                max_allowed = torch.amax(masked_logits, dim=1)
  +                exp_shift_allowed = torch.exp(card_logits_all - max_allowed.unsqueeze(1)) * allowed_mask.to(card_logits_all.dtype)
  +                sum_allowed = exp_shift_allowed.sum(dim=1)
  +                lse_allowed = max_allowed + torch.log(torch.clamp_min(sum_allowed, 1e-12))
  +                chosen_clamped = torch.minimum(chosen_index_t, (legals_count_t - 1).clamp_min(0))
  +                chosen_abs = (legals_offset_t + chosen_clamped)
  +                total_legals = legals_t.size(0)
  +                pos_map = torch.full((total_legals,), -1, dtype=torch.long, device=device)
  +                pos_map[abs_idx] = torch.arange(abs_idx.numel(), device=device, dtype=torch.long)
  +                chosen_pos = pos_map[chosen_abs]
  +                # Validate chosen_pos mapping succeeded for all rows
  +                if bool((chosen_pos < 0).any().item() if chosen_pos.numel() > 0 else False):
  +                    bad_rows = torch.nonzero(chosen_pos < 0, as_tuple=False).flatten().tolist()
  +                    raise RuntimeError(f"collect_trajectory: chosen_pos mapping failed for rows {bad_rows}")
  +                played_ids_all = torch.argmax(legals_t[:, :40], dim=1)
  +                chosen_card_ids = played_ids_all[chosen_abs]
  +                logp_card = card_logits_all[torch.arange(B, device=device), chosen_card_ids] - lse_allowed[torch.arange(B, device=device)]
  +                # capture
  +                a_emb_mb = agent.actor.action_enc(legals_mb)
  +                cap_logits = (a_emb_mb * state_proj[sample_idx_per_legal]).sum(dim=1)
  +                group_ids = sample_idx_per_legal * 40 + played_ids_mb
  +                num_groups = B * 40
  +                group_max = torch.full((num_groups,), float('-inf'), dtype=cap_logits.dtype, device=device)
  +                group_max.scatter_reduce_(0, group_ids, cap_logits, reduce='amax', include_self=True)
  +
  +                gmax_per_legal = group_max[group_ids]
  +                exp_shifted = torch.exp(cap_logits - gmax_per_legal)
  +                group_sum = torch.zeros((num_groups,), dtype=cap_logits.dtype, device=device)
  +                group_sum.index_add_(0, group_ids, exp_shifted)
  +                lse_per_legal = gmax_per_legal + torch.log(torch.clamp_min(group_sum[group_ids], 1e-12))
  +                logp_cap_per_legal = cap_logits - lse_per_legal
  +                logp_cap = logp_cap_per_legal[chosen_pos]
  +                old_logp_t = (logp_card + logp_cap)
  +                # Early validity for old_logp
  +                if not torch.isfinite(old_logp_t).all():
  +                    raise RuntimeError("collect_trajectory: old_logp contains non-finite values")
  +                if bool((old_logp_t > 1e-6).any().item() if old_logp_t.numel() > 0 else False):
  +                    mx = float(old_logp_t.max().item())
  +                    raise RuntimeError(f"collect_trajectory: old_logp contains positive values (max={mx})")
  +                if bool((old_logp_t < -100.0).any().item() if old_logp_t.numel() > 0 else False):
  +                    idx_bad = torch.nonzero(old_logp_t < -100.0, as_tuple=False).flatten()
  +                    r = int(idx_bad[0].item()) if idx_bad.numel() > 0 else 0
  +                    # diagnostics for the first bad row
  +                    chosen_card = int(chosen_card_ids[r].item()) if chosen_card_ids.numel() > r else -1
  +                    cnt_r = int(legals_count_t[r].item()) if legals_count_t.numel() > r else -1
  +                    # group size for (r, chosen_card)
  +                    grp_mask = (sample_idx_per_legal == r) & (played_ids_mb == chosen_card)
  +                    grp_size = int(grp_mask.sum().item())
  +                    cap_logits_grp = cap_logits[grp_mask]
  +                    def _stats(t):
  +                        return {
  +                            'min': float(t.min().item()) if t.numel() > 0 else None,
  +                            'max': float(t.max().item()) if t.numel() > 0 else None,
  +                            'mean': float(t.mean().item()) if t.numel() > 0 else None,
  +                            'numel': int(t.numel())
  +                        }
  +                    raise RuntimeError(
  +                        f"collect_trajectory: old_logp extremely small (min={float(old_logp_t.min().item())}); "
  +                        f"row={r}, chosen_card={chosen_card}, legals_count={cnt_r}, group_size={grp_size}, "
  +                        f"logp_card={float(logp_card[r].item())}, logp_cap={float(logp_cap[r].item())}, "
  +                        f"card_logits_all_stats={_stats(card_logits_all[r])}, cap_logits_grp_stats={_stats(cap_logits_grp)}"
  +                    )
  +            else:
  +                old_logp_t = torch.zeros((B,), dtype=torch.float32, device=device)
  +    else:
  +        old_logp_t = torch.zeros((0,), dtype=torch.float32, device=device)
  +
  +    ep_debug_lengths = [int(end - start) for (start, end) in ep_slices] if len(ep_slices) > 0 else None
  +    _maybe_log_ppo_batch(
  +        'serial',
  +        rew_t,
  +        ret_vec,
  +        adv_vec,
  +        val_t,
  +        nval_t,
  +        done_mask,
  +        old_logp=old_logp_t,
  +        seat_tensor=seat_team_cpu,
  +        episode_lengths=ep_debug_lengths,
  +        extra={'episodes': len(ep_slices)} if len(ep_slices) > 0 else None,
  +    )
  +
  +    # Validate batch structure sizes/coherence before returning
  +    if not skip_step_validation:
  +        B = int(obs_cpu.size(0)) if torch.is_tensor(obs_cpu) else len(obs_cpu)
  +        def _len(x):
  +            return (int(x.size(0)) if torch.is_tensor(x) else len(x))
  +        # next_obs length equals obs length by construction
  +        if not (B == _len(act_cpu) == _len(done_t) == _len(seat_team_cpu)):
  +            raise RuntimeError("collect_trajectory: per-step arrays length mismatch among obs/act/next_obs/done/seat")
  +        if int(legals_count_cpu.sum().item()) != int(legals_cpu.size(0)):
  +            raise RuntimeError("collect_trajectory: sum(legals_count) != len(legals)")
  +        if (chosen_index_cpu < 0).any() or ((chosen_index_cpu >= legals_count_cpu) & (legals_count_cpu > 0)).any():
  +            raise RuntimeError("collect_trajectory: chosen_index out of range for some rows")
  +        if seat_team_cpu.size(1) != 6 or not (seat_team_cpu[:, :4].sum(dim=1) == 1).all():
  +            raise RuntimeError("collect_trajectory: seat_team must be (B,6) with one-hot seat")
  +
  +    # Mantieni batch già tensori CPU; l'update ora li mapperà tutti su CUDA in un solo passaggio
  +    if _PAR_TIMING:
  +        total_collect = time.time() - t_collect_start
  +        episodes_total = len(ep_slices) if len(ep_slices) > 0 else (1 if len(obs_list) > 0 else 0)
  +        tqdm.write(
  +            f"[serial-collect] steps={len(obs_list)} episodes={episodes_total} resets={env_reset_count} "
  +            f"t_reset={t_env_reset:.3f}s t_obs={t_get_obs:.3f}s t_legals={t_get_legals:.3f}s "
  +            f"t_mcts={t_mcts:.3f}s t_select={t_select:.3f}s state={t_state_proj:.3f}s action_enc={t_action_enc:.3f}s sample={t_sampling:.3f}s t_step={t_step:.3f}s build={t_build:.3f}s "
  +            f"values={t_values_gae:.3f}s old_logp={t_oldlogp:.3f}s total={total_collect:.3f}s"
  +        )
  +        profiling_payload = {
  +            'serial/steps': len(obs_list),
  +            'serial/episodes': episodes_total,
  +            'serial/resets': env_reset_count,
  +            'serial/t_env_reset': t_env_reset,
  +            'serial/t_get_obs': t_get_obs,
  +            'serial/t_get_legals': t_get_legals,
  +            'serial/t_mcts': t_mcts,
  +            'serial/t_select': t_select,
  +            'serial/t_state_proj': t_state_proj,
  +            'serial/t_action_enc': t_action_enc,
  +            'serial/t_sampling': t_sampling,
  +            'serial/t_step': t_step,
  +            'serial/t_build': t_build,
  +            'serial/t_values_gae': t_values_gae,
  +            'serial/t_oldlogp': t_oldlogp,
  +            'serial/t_total': total_collect,
  +        }
  +        env_profile = {}
  +        consume_env = getattr(env, 'consume_profile_stats', None)
  +        if callable(consume_env):
  +            try:
  +                env_profile = consume_env() or {}
  +            except Exception as exc:
  +                tqdm.write(f"[serial-collect] env profiling failed: {exc}")
  +                env_profile = {}
  +        if env_profile:
  +            msg_parts = []
  +            step_count = env_profile.get('step/count', 0)
  +            if step_count:
  +                msg_parts.append(
  +                    f"step_total={env_profile.get('step/total', 0.0):.3f}s decode={env_profile.get('step/decode_total', 0.0):.3f}s "
  +                    f"validate={env_profile.get('step/validate_total', 0.0):.3f}s apply={env_profile.get('step/apply_total', 0.0):.3f}s avg={env_profile.get('step/total_avg', 0.0)*1000:.2f}ms"
  +                )
  +            leg_calls = env_profile.get('legals/calls', 0)
  +            if leg_calls:
  +                msg_parts.append(
  +                    f"legals_total={env_profile.get('legals/total', 0.0):.3f}s avg={env_profile.get('legals/avg', 0.0)*1000:.2f}ms"
  +                )
  +            if msg_parts:
  +                tqdm.write(f"[env-profile] {' '.join(msg_parts)}")
  +            profiling_payload.update({f'env/{k}': v for k, v in env_profile.items()})
  +    else:
  +        profiling_payload = None
  +        env_profile = {}
  +
  +    batch = {
  +        'obs': obs_cpu,
  +        'act': act_cpu,
  +        # lascia tensori chiave su CUDA quando presenti ma evita copie inutili se già device-correct
  +        'old_logp': (old_logp_t if (torch.is_tensor(old_logp_t) and old_logp_t.device.type == device.type) else old_logp_t.detach()),
  +        'ret': (ret_t if (torch.is_tensor(ret_t) and ret_t.device.type == device.type) else ret_t.detach()),
  +        'adv': (adv_t if (torch.is_tensor(adv_t) and adv_t.device.type == device.type) else adv_t.detach()),
  +        'rew': rew_t,
  +        'done': done_t,
  +        'seat_team': seat_team_cpu,
  +        'belief_summary': belief_sum_cpu,
  +        'legals': legals_cpu,
  +        'legals_offset': legals_offset_cpu,
  +        'legals_count': legals_count_cpu,
  +        'chosen_index': chosen_index_cpu,
  +        'mcts_policy': mcts_policy_cpu,
  +        'mcts_weight': mcts_weight_cpu,
  +        'others_hands': others_hands_cpu,
  +        'routing_log': routing_log,
  +    }
  +    if profiling_payload is not None:
  +        batch.setdefault('profiling', {}).update(profiling_payload)
  +    elif env_profile:
  +        batch.setdefault('profiling', {}).update({f'env/{k}': v for k, v in env_profile.items()})
  +    return batch
  +
  +
  +def collect_trajectory(env: ScoponeEnvMA, agent: ActionConditionedPPO, horizon: int = 128,
  +                       gamma: float = 1.0, lam: float = 0.95,
  +                       partner_actor: ActionConditionedActor = None,
  +                       opponent_actor: ActionConditionedActor = None,
  +                       main_seats: List[int] = None,
  +                       belief_particles: int = 512, belief_ess_frac: float = 0.5,
  +                       episodes: int = None, final_reward_only: bool = True,
  +                       use_mcts: bool = True,
  +                       mcts_sims: int = 128, mcts_dets: int = 4, mcts_c_puct: float = 1.0,
  +                       mcts_root_temp: float = 0.0, mcts_prior_smooth_eps: float = 0.0,
  +                       mcts_dirichlet_alpha: float = 0.25, mcts_dirichlet_eps: float = 0.0,
  +                       mcts_train_factor: float = 1.0,
  +                       mcts_progress_start: float = 0.25,
  +                       mcts_progress_full: float = 0.75,
  +                       mcts_min_sims: int = 0,
  +                       train_both_teams: bool = False,
  +                       seed: Optional[int] = None) -> Dict:
  +    seed_token = _serial_seed_enter(seed)
  +    try:
  +        return _collect_trajectory_impl(env=env,
  +                                        agent=agent,
  +                                        horizon=horizon,
  +                                        gamma=gamma,
  +                                        lam=lam,
  +                                        partner_actor=partner_actor,
  +                                        opponent_actor=opponent_actor,
  +                                        main_seats=main_seats,
  +                                        belief_particles=belief_particles,
  +                                        belief_ess_frac=belief_ess_frac,
  +                                        episodes=episodes,
  +                                        final_reward_only=final_reward_only,
  +                                        use_mcts=use_mcts,
  +                                        mcts_sims=mcts_sims,
  +                                        mcts_dets=mcts_dets,
  +                                        mcts_c_puct=mcts_c_puct,
  +                                        mcts_root_temp=mcts_root_temp,
  +                                        mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                        mcts_dirichlet_alpha=mcts_dirichlet_alpha,
  +                                        mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                        mcts_train_factor=mcts_train_factor,
  +                                        mcts_progress_start=mcts_progress_start,
  +                                        mcts_progress_full=mcts_progress_full,
  +                                        mcts_min_sims=mcts_min_sims,
  +                                        train_both_teams=train_both_teams)
  +    finally:
  +        _serial_seed_exit(seed_token)
  +
  +
  +def collect_trajectory_parallel(agent: ActionConditionedPPO,
  +                                num_envs: int = 32,
  +                                episodes_total_hint: int = 8,
  +                                k_history: int = 39,
  +                                gamma: float = 1.0,
  +                                lam: float = 0.95,
  +                                use_mcts: bool = True,
  +                                train_both_teams: bool = True,
  +                                main_seats: List[int] = None,
  +                                mcts_sims: int = 128,
  +                                mcts_dets: int = 4,
  +                                mcts_c_puct: float = 1.0,
  +                                mcts_root_temp: float = 0.0,
  +                                mcts_prior_smooth_eps: float = 0.0,
  +                                mcts_dirichlet_alpha: float = 0.25,
  +                                mcts_dirichlet_eps: float = 0.0,
  +                                mcts_progress_start: float = 0.25,
  +                                mcts_progress_full: float = 0.75,
  +                                mcts_min_sims: int = 0,
  +                                mcts_train_factor: float = 1.0,
  +                                seed: int = 0,
  +                                show_progress_env: bool = True,
  +                                tqdm_base_pos: int = 2,
  +                                frozen_actor: ActionConditionedActor = None,
  +                                frozen_non_main: bool = False) -> Dict:
  +    # Validate configuration invariants
  +    if int(num_envs) <= 0:
  +        raise ValueError("collect_trajectory_parallel: num_envs must be > 0")
  +    if int(episodes_total_hint) <= 0:
  +        raise ValueError("collect_trajectory_parallel: episodes_total_hint must be > 0")
  +    if int(k_history) <= 0:
  +        raise ValueError("collect_trajectory_parallel: k_history must be > 0")
  +    if float(gamma) < 0 or float(gamma) > 1:
  +        raise ValueError("collect_trajectory_parallel: gamma must be in [0,1]")
  +    if float(lam) < 0 or float(lam) > 1:
  +        raise ValueError("collect_trajectory_parallel: lam must be in [0,1]")
  +    if float(mcts_prior_smooth_eps) < 0 or float(mcts_prior_smooth_eps) > 1:
  +        raise ValueError("collect_trajectory_parallel: mcts_prior_smooth_eps must be in [0,1]")
  +    if float(mcts_dirichlet_eps) < 0 or float(mcts_dirichlet_eps) > 1:
  +        raise ValueError("collect_trajectory_parallel: mcts_dirichlet_eps must be in [0,1]")
  +    if float(mcts_dirichlet_alpha) < 0:
  +        raise ValueError("collect_trajectory_parallel: mcts_dirichlet_alpha must be >= 0")
  +    # Choose start method with env override and platform/device awareness.
  +    # - Windows: 'spawn'
  +    # - POSIX with CUDA or background threads: prefer 'forkserver' (avoid forking after CUDA/threads)
  +    # - Otherwise: 'fork'
  +    override = str(os.environ.get('SCOPONE_MP_START', '')).strip().lower()
  +    if override in {'spawn', 'fork', 'forkserver'}:
  +        start_method = override
  +    else:
  +        if platform.system().lower() == 'windows':
  +            start_method = 'spawn'
  +        else:
  +            # If CUDA is in use, prefer 'spawn' to avoid CUDA re-init issues in children
  +            if getattr(device, 'type', str(device)) == 'cuda' or (hasattr(torch, 'cuda') and torch.cuda.is_available()):
  +                start_method = 'spawn'
  +            else:
  +                start_method = 'fork'
  +    ctx = mp.get_context(start_method)
  +    request_q = ctx.Queue(maxsize=num_envs * 4)
  +    # Make episode queue large enough to avoid backpressure when multiple workers finish close together
  +    _episodes_hint = max(1, int(episodes_total_hint))
  +    episode_q = ctx.Queue(maxsize=max(num_envs * 4, _episodes_hint * 2))
  +    action_queues = [ctx.Queue(maxsize=2) for _ in range(num_envs)]
  +    # Distribute episodes exactly across workers without overshoot
  +    base = int(episodes_total_hint) // int(num_envs)
  +    rem = int(episodes_total_hint) % int(num_envs)
  +    episodes_per_env_list = [base + (1 if wid < rem else 0) for wid in range(num_envs)]
  +    total_eps = sum(episodes_per_env_list)
  +    tqdm.write(f"[collector] num_envs={num_envs} episodes_total_hint={episodes_total_hint} "
  +                   f"episodes_per_env_list(min..max)={min(episodes_per_env_list)}..{max(episodes_per_env_list)} total_env_episodes={total_eps}")
  +    workers = []
  +    cfg_base = {
  +        'rules': {'shape_scopa': False},
  +        'k_history': int(k_history),
  +        'send_legals': True,
  +        'use_mcts': bool(use_mcts),
  +        'train_both_teams': bool(train_both_teams),
  +        'main_seats': main_seats if main_seats is not None else [0,2],
  +        'frozen_non_main': bool(frozen_non_main),
  +        'mcts_sims': int(mcts_sims),
  +        'mcts_dets': int(mcts_dets),
  +        'mcts_c_puct': float(mcts_c_puct),
  +        'mcts_root_temp': float(mcts_root_temp),
  +        'mcts_prior_smooth_eps': float(mcts_prior_smooth_eps),
  +        'mcts_dirichlet_alpha': float(mcts_dirichlet_alpha),
  +        'mcts_dirichlet_eps': float(mcts_dirichlet_eps),
  +        'mcts_progress_start': float(mcts_progress_start),
  +        'mcts_progress_full': float(mcts_progress_full),
  +        'mcts_min_sims': int(mcts_min_sims),
  +        'mcts_train_factor': float(mcts_train_factor),
  +        'seed': int(seed),
  +    }
  +    for wid in range(num_envs):
  +        cfg = dict(cfg_base)
  +        cfg['episodes_per_env'] = int(episodes_per_env_list[wid])
  +        p = ctx.Process(target=_env_worker, args=(wid, cfg, request_q, action_queues[wid], episode_q), daemon=True)
  +        p.start()
  +        workers.append(p)
  +
  +    episodes_received = 0
  +    episodes_payloads = []
  +    produced_count = [0 for _ in range(num_envs)]
  +    # Optional timing buffers
  +    timing_from_workers = [] if _PAR_TIMING else None
  +    t_drain = 0.0; t_get_reqs = 0.0; t_batch_select = 0.0; t_batch_service = 0.0; t_dispatch = 0.0
  +    # Fine-grained get_reqs profiling (new consolidated struct)
  +    getreqs_batches = 0
  +    getreqs_total_reqs = 0
  +    t_get_nowait = 0.0
  +    t_get_block = 0.0
  +    cnt_nowait_ok = 0
  +    cnt_nowait_empty = 0
  +    cnt_block_ok = 0
  +    cnt_block_timeout = 0
  +    # Extra timers for post-collector breakdown
  +    t_build = 0.0; t_values_gae = 0.0; t_oldlogp = 0.0; t_teardown = 0.0
  +    t_master_start = time.time() if _PAR_TIMING else 0.0
  +    # Batch size histogram and request type counters (active when profiling)
  +    bs_hist = {}
  +    step_reqs_count = 0
  +    other_reqs_count = 0
  +    # Optional per-env progress bars
  +    env_pbars = []
  +    _TQDM_DISABLE = (os.environ.get('TQDM_DISABLE','0') in ['1','true','yes','on'])
  +    # Respect global per-env tqdm policy via env; default ON unless explicitly disabled
  +    _PER_ENV_TQDM = (str(os.environ.get('SCOPONE_PER_ENV_TQDM', '1')).strip().lower() in ['1','true','yes','on'])
  +    # Hide per-env progress bars when debug logs are enabled to avoid clutter
  +    _SHOW_PB = bool(show_progress_env) and _PER_ENV_TQDM and (not _TQDM_DISABLE) and (not _PAR_DEBUG)
  +    if _SHOW_PB:
  +        for wid in range(num_envs):
  +            b = tqdm(total=int(episodes_per_env_list[wid]), desc=f"env {wid}", position=(tqdm_base_pos + wid), leave=False, dynamic_ncols=True, disable=_TQDM_DISABLE)
  +            env_pbars.append(b)
  +    else:
  +        env_pbars = [None] * num_envs
  +    # Main loop: drain episodes first (to avoid backpressure), then service requests
  +    _last_activity_ts = time.time()
  +    _expected_total = sum(episodes_per_env_list)
  +    _done_flags = [False] * num_envs
  +    while episodes_received < _expected_total:
  +        # 1) Drain any completed episodes first to free episode_q
  +        drained_any = False
  +        t0_drain = time.time() if _PAR_TIMING else 0.0
  +        while True:
  +            try:
  +                ep = episode_q.get_nowait()
  +                # Handle completion markers
  +                if isinstance(ep, dict) and ep.get('type') == 'done':
  +                    wid = int(ep.get('wid', -1))
  +                    if 0 <= wid < len(_done_flags):
  +                        _done_flags[wid] = True
  +                        expected_eps = episodes_per_env_list[wid] if 0 <= wid < len(episodes_per_env_list) else None
  +                        if (expected_eps is None) or (expected_eps > 0):
  +                            if produced_count[wid] == 0:
  +                                raise RuntimeError(f"collector: worker wid={wid} signaled done without producing any episode")
  +                        if 0 <= wid < len(env_pbars) and env_pbars[wid] is not None:
  +                            env_pbars[wid].close()
  +                            env_pbars[wid] = None
  +                    drained_any = True
  +                    continue
  +                elif _PAR_TIMING and isinstance(ep, dict) and ep.get('type') == 'timing':
  +                    timing_from_workers.append(ep)
  +                    drained_any = True
  +                else:
  +                    # validate episode payload
  +                    if (not isinstance(ep, dict)) or ('obs' not in ep) or (len(ep['obs']) == 0):
  +                        wid = (ep.get('wid') if isinstance(ep, dict) else 'unknown')
  +                        raise RuntimeError(f"collector: empty or invalid episode payload from wid={wid}")
  +                    episodes_payloads.append(ep)
  +                    episodes_received += 1
  +                    wid = int(ep.get('wid', -1))
  +                    if 0 <= wid < len(produced_count):
  +                        produced_count[wid] += 1
  +                    drained_any = True
  +                    # Update per-env progress bar
  +                    wid = int(ep.get('wid', -1))
  +                    if 0 <= wid < len(env_pbars) and env_pbars[wid] is not None:
  +                        env_pbars[wid].update(1)
  +            except queue.Empty:
  +                break
  +        if drained_any:
  +            _last_activity_ts = time.time()
  +        if _PAR_TIMING:
  +            t_drain += (time.time() - t0_drain)
  +
  +        # 2) Gather a micro-batch of requests without blocking
  +        reqs = []
  +        min_batch = int(os.environ.get('SCOPONE_COLLECT_MIN_BATCH', '0'))
  +        if min_batch <= 0:
  +            min_batch = max(32, 2 * int(num_envs))
  +        batch_target = max(min_batch, 64)
  +        max_latency_ms = float(os.environ.get('SCOPONE_COLLECT_MAX_LATENCY_MS', '3.0'))
  +        max_latency_s = max(0.0, max_latency_ms / 1000.0)
  +        t0_get = time.time() if _PAR_TIMING else 0.0
  +        deadline = (t0_get + max_latency_s)
  +        # Block briefly for the first item (up to max_latency)
  +        try:
  +            t0_blk = time.time() if _PAR_TIMING else 0.0
  +            r0 = request_q.get(timeout=max_latency_s)
  +            reqs.append(r0)
  +            if _PAR_TIMING:
  +                cnt_block_ok += 1
  +                t_get_block += (time.time() - t0_blk)
  +                _GETREQS_PROF['cnt_first_ok'] += 1
  +                _GETREQS_PROF['t_first_block'] += (time.time() - t0_blk)
  +        except queue.Empty:
  +            if _PAR_TIMING:
  +                cnt_block_timeout += 1
  +                t_get_block += (time.time() - t0_blk)
  +                _GETREQS_PROF['cnt_first_to'] += 1
  +                _GETREQS_PROF['t_first_block'] += (time.time() - t0_blk)
  +            # No requests arrived within latency budget
  +            if _PAR_TIMING:
  +                t_get_reqs += (time.time() - t0_get)
  +            # Continue to next loop iteration
  +            continue
  +        # After first item, drain quickly without spinning
  +        t0_now = time.time() if _PAR_TIMING else 0.0
  +        while len(reqs) < batch_target:
  +            try:
  +                r = request_q.get_nowait()
  +                # If configured, label non-main requests for shadow (frozen) selection
  +                if bool(cfg_base.get('frozen_non_main', False)) and isinstance(r, dict) and r.get('type') == 'step':
  +                    seat = r.get('seat', None)
  +                    is_main = False
  +                    if seat is not None:
  +                        st = torch.as_tensor(seat, dtype=torch.float32)
  +                        seat_idx = int(torch.argmax(st[:4]).item())
  +                        main_set = set(cfg_base.get('main_seats', [0, 2]))
  +                        is_main = (seat_idx in main_set)
  +                    if not is_main:
  +                        r = dict(r)
  +                        r['type'] = 'step_shadow'
  +                reqs.append(r)
  +                if _PAR_TIMING:
  +                    cnt_nowait_ok += 1
  +                    _GETREQS_PROF['cnt_nowait_ok'] += 1
  +            except queue.Empty:
  +                if _PAR_TIMING:
  +                    cnt_nowait_empty += 1
  +                    _GETREQS_PROF['cnt_nowait_empty'] += 1
  +                break
  +        if _PAR_TIMING:
  +            dt_now = (time.time() - t0_now)
  +            t_get_nowait += dt_now
  +            _GETREQS_PROF['t_nowait_drain'] += dt_now
  +        # If under min_batch and we still have latency budget, do a short block to top up
  +        while (len(reqs) < min_batch) and (time.time() < deadline):
  +            try:
  +                remaining = max(0.0, deadline - time.time())
  +                t0_blk = time.time() if _PAR_TIMING else 0.0
  +                r = request_q.get(timeout=remaining)
  +                reqs.append(r)
  +                if _PAR_TIMING:
  +                    cnt_block_ok += 1
  +                    t_get_block += (time.time() - t0_blk)
  +                    _GETREQS_PROF['cnt_topup_ok'] += 1
  +                    _GETREQS_PROF['t_topup_block'] += (time.time() - t0_blk)
  +                # Opportunistically drain any burst
  +                t0_now = time.time() if _PAR_TIMING else 0.0
  +                while len(reqs) < batch_target:
  +                    try:
  +                        r2 = request_q.get_nowait()
  +                        reqs.append(r2)
  +                        if _PAR_TIMING:
  +                            cnt_nowait_ok += 1
  +                            _GETREQS_PROF['cnt_nowait_ok'] += 1
  +                    except queue.Empty:
  +                        if _PAR_TIMING:
  +                            cnt_nowait_empty += 1
  +                            _GETREQS_PROF['cnt_nowait_empty'] += 1
  +                        break
  +                if _PAR_TIMING:
  +                    dt_now2 = (time.time() - t0_now)
  +                    t_get_nowait += dt_now2
  +                    _GETREQS_PROF['t_post_drain'] += dt_now2
  +            except queue.Empty:
  +                if _PAR_TIMING:
  +                    cnt_block_timeout += 1
  +                    t_get_block += (time.time() - t0_blk)
  +                    _GETREQS_PROF['cnt_topup_to'] += 1
  +                    _GETREQS_PROF['t_topup_block'] += (time.time() - t0_blk)
  +                break
  +        if _PAR_TIMING:
  +            t_get_reqs += (time.time() - t0_get)
  +        # 3) Process batch on GPU
  +        if len(reqs) > 0:
  +            _last_activity_ts = time.time()
  +            if _PAR_TIMING:
  +                getreqs_batches += 1
  +                getreqs_total_reqs += len(reqs)
  +                _GETREQS_PROF['batches'] += 1
  +                _GETREQS_PROF['total_reqs'] += len(reqs)
  +                _GETREQS_PROF['bs_hist'][len(reqs)] = int(_GETREQS_PROF['bs_hist'].get(len(reqs), 0)) + 1
  +            if _PAR_TIMING:
  +                bs_hist[len(reqs)] = bs_hist.get(len(reqs), 0) + 1
  +            step_reqs = [r for r in reqs if r.get('type') == 'step']
  +            shadow_reqs = [r for r in reqs if r.get('type') == 'step_shadow']
  +            other_reqs = [r for r in reqs if r.get('type') != 'step']
  +            other_reqs = [r for r in other_reqs if r.get('type') != 'step_shadow']
  +            if _PAR_TIMING:
  +                step_reqs_count += len(step_reqs)
  +                other_reqs_count += len(other_reqs)
  +            if len(step_reqs) > 0:
  +                _t0 = time.time() if _PAR_TIMING else 0.0
  +                sel = _batched_select_indices(agent, step_reqs)
  +                if _PAR_TIMING:
  +                    t_batch_select += (time.time() - _t0)
  +                _t0d = time.time() if _PAR_TIMING else 0.0
  +                for (wid, idx) in sel:
  +                    if _PAR_DEBUG:
  +                        _dbg(f"[master] dispatch step wid={int(wid)} idx={int(idx)}")
  +                    action_queues[wid].put({'idx': int(idx)}, block=False)
  +                if _PAR_TIMING:
  +                    t_dispatch += (time.time() - _t0d)
  +            if len(shadow_reqs) > 0:
  +                # Use frozen actor to score non-main seats if provided
  +                if frozen_actor is None:
  +                    # Fallback: select with main actor if frozen not provided
  +                    _t0 = time.time() if _PAR_TIMING else 0.0
  +                    sel = _batched_select_indices(agent, shadow_reqs)
  +                    if _PAR_TIMING:
  +                        t_batch_select += (time.time() - _t0)
  +                else:
  +                    _t0 = time.time() if _PAR_TIMING else 0.0
  +                    sel = _batched_select_indices_with_actor(frozen_actor, shadow_reqs)
  +                    if _PAR_TIMING:
  +                        t_batch_select += (time.time() - _t0)
  +                _t0d = time.time() if _PAR_TIMING else 0.0
  +                for (wid, idx) in sel:
  +                    if _PAR_DEBUG:
  +                        _dbg(f"[master] dispatch shadow wid={int(wid)} idx={int(idx)}")
  +                    action_queues[wid].put({'idx': int(idx)}, block=False)
  +                if _PAR_TIMING:
  +                    t_dispatch += (time.time() - _t0d)
  +            if len(other_reqs) > 0:
  +                _t0 = time.time() if _PAR_TIMING else 0.0
  +                outs = _batched_service(agent, other_reqs)
  +                if _PAR_TIMING:
  +                    t_batch_service += (time.time() - _t0)
  +                _t0d = time.time() if _PAR_TIMING else 0.0
  +                for r, out in zip(other_reqs, outs):
  +                    wid = int(r.get('wid', 0))
  +                    if _PAR_DEBUG:
  +                        _dbg(f"[master] dispatch {r.get('type')} wid={int(wid)} keys={list(out.keys())}")
  +                    action_queues[wid].put(out, block=False)
  +                if _PAR_TIMING:
  +                    t_dispatch += (time.time() - _t0d)
  +
  +        # 4) Detect worker crashes and stale state
  +        for _wid, _p in enumerate(workers):
  +            if (not _p.is_alive()) and (_p.exitcode is not None) and (int(_p.exitcode) != 0):
  +                raise RuntimeError(f"Env worker wid={_wid} pid={_p.pid} crashed with exitcode={_p.exitcode}")
  +        # 5) Watchdog: if no activity for long, attempt graceful shutdown based on done flags
  +        if (time.time() - _last_activity_ts) > float(os.environ.get('SCOPONE_COLLECTOR_STALL_S', '30')):
  +            # If all workers have signaled 'done', break even if some episodes were dropped
  +            if all(_done_flags):
  +                break
  +            alive = [(i, p.pid, p.is_alive(), p.exitcode) for i, p in enumerate(workers)]
  +            raise RuntimeError(f"Collector stalled: episodes_received={episodes_received}/{_expected_total}; workers={alive}")
  +
  +    # Close per-env progress bars
  +    _t_td0 = time.time() if _PAR_TIMING else 0.0
  +    for b in env_pbars:
  +        if b is not None:
  +            b.close()
  +    # Join workers, then terminate any stragglers and close queues
  +    for p in workers:
  +        p.join(timeout=0.1)
  +    # If any worker exited with non-zero code, surface it now
  +    for _wid, _p in enumerate(workers):
  +        if (_p.exitcode is not None) and (int(_p.exitcode) != 0):
  +            raise RuntimeError(f"Env worker wid={_wid} pid={_p.pid} exited with exitcode={_p.exitcode}")
  +    for p in workers:
  +        if p.is_alive():
  +            p.terminate()
  +            p.join(timeout=0.1)
  +    # Close queues to free resources
  +    request_q.close(); request_q.join_thread()
  +    episode_q.close(); episode_q.join_thread()
  +    for q in action_queues:
  +        q.close(); q.join_thread()
  +    if _PAR_TIMING:
  +        t_teardown = (time.time() - _t_td0)
  +
  +    # If no episodes were produced, raise with context (no fallback)
  +    if len(episodes_payloads) == 0:
  +        raise RuntimeError(f"collector: No episodes produced (num_envs={num_envs}, episodes_total_hint={episodes_total_hint}, use_mcts={use_mcts}, mcts_sims={mcts_sims}, mcts_dets={mcts_dets})")
  +
  +    # End-of-collection invariant: each episode must contribute exactly E transitions
  +    # E = 40 if training both teams, else 20 (main seats only)
  +    E = 40 if bool(train_both_teams) else 20
  +    episode_lengths = [len(ep.get('obs', [])) for ep in episodes_payloads]
  +    episodes = len(episodes_payloads)
  +    total_steps = sum(episode_lengths)
  +    # Compute per-seat counts over the whole collection for diagnostics
  +    import torch as _t
  +    if episodes > 0 and len(episodes_payloads[0].get('seat', [])) > 0:
  +        seats_all = []
  +        for ep in episodes_payloads:
  +            seats_all.extend(ep.get('seat', []))
  +        st = _t.stack([_t.as_tensor(x, dtype=_t.float32) for x in seats_all], dim=0)
  +        seat_idx = _t.argmax(st[:, :4], dim=1)
  +        c0 = int((seat_idx == 0).sum().item())
  +        c1 = int((seat_idx == 1).sum().item())
  +        c2 = int((seat_idx == 2).sum().item())
  +        c3 = int((seat_idx == 3).sum().item())
  +        seat_counts = (c0, c1, c2, c3)
  +    else:
  +        seat_counts = (0, 0, 0, 0)
  +    if total_steps != episodes * E:
  +        # Try alternate E (20<->40) to accommodate upstream per-episode util choice
  +        alt_E = (40 if E == 20 else 20)
  +        if total_steps == episodes * alt_E:
  +            # Adopt observed per-episode util and continue with a warning
  +            from tqdm import tqdm as _tqdm
  +            _tqdm.write(
  +                f"[collector] Adjusting per-episode util from {E} to observed {alt_E} (episodes={episodes}, total_steps={total_steps})"
  +            )
  +            E = alt_E
  +        else:
  +            # Build compact stats of episode lengths
  +            lengths_unique = sorted(set(episode_lengths))
  +            # frequency per length
  +            freq = {L: episode_lengths.count(L) for L in lengths_unique}
  +            raise RuntimeError(
  +                f"collector: util transitions mismatch; got_total={total_steps}, expected_total={episodes * E}; "
  +                f"episodes_done={episodes}, per_ep_expected={E}, length_stats={freq}, seat_counts={seat_counts}"
  +            )
  +
  +    # Additional guard: ensure we produced exactly the requested number of episodes overall
  +    if episodes != int(episodes_total_hint):
  +        raise RuntimeError(
  +            f"collector: episodes produced {episodes} != requested {int(episodes_total_hint)} (num_envs={num_envs}, "
  +            f"episodes_per_env_list={episodes_per_env_list}, produced_per_worker={produced_count})"
  +        )
  +    # Print timing after validation
  +    if _PAR_TIMING:
  +        master_total = (time.time() - t_master_start)
  +        import numpy as _np
  +        if timing_from_workers is not None and len(timing_from_workers) > 0:
  +            def _agg(key):
  +                arr = _np.asarray([x.get(key, 0.0) for x in timing_from_workers], dtype=_np.float64)
  +                return float(arr.sum()), float(arr.mean()), float(arr.max())
  +            s_env, m_env, M_env = _agg('t_env_reset')
  +            s_obs, m_obs, M_obs = _agg('t_get_obs')
  +            s_leg, m_leg, M_leg = _agg('t_get_legals')
  +            s_mcts, m_mcts, M_mcts = _agg('t_mcts')
  +            s_rpc, m_rpc, M_rpc = _agg('t_rpc')
  +            s_step, m_step, M_step = _agg('t_step')
  +            s_pack, m_pack, M_pack = _agg('t_pack')
  +            s_tot, m_tot, M_tot = _agg('t_total')
  +            tqdm.write(f"[par-timing] workers={num_envs} workers_total={s_tot:.3f}s (avg={m_tot:.3f}, max={M_tot:.3f})\n"
  +                        f"  env.reset={s_env:.3f} get_obs={s_obs:.3f} get_legals={s_leg:.3f} rpc_wait={s_rpc:.3f} mcts={s_mcts:.3f} env.step={s_step:.3f} pack={s_pack:.3f}")
  +        tqdm.write(f"[par-timing-master] total={master_total:.3f}s drain={t_drain:.3f} get_reqs={t_get_reqs:.3f} batch_select={t_batch_select:.3f} batch_service={t_batch_service:.3f} dispatch={t_dispatch:.3f}")
  +        # Detailed get_reqs breakdown
  +        avg_batch = (getreqs_total_reqs / max(getreqs_batches, 1))
  +        tqdm.write(f"[par-getreqs] batches={getreqs_batches} total_reqs={getreqs_total_reqs} nowait_ok={cnt_nowait_ok} nowait_empty={cnt_nowait_empty} block_ok={cnt_block_ok} block_to={cnt_block_timeout} t_nowait={t_get_nowait:.3f} t_block={t_get_block:.3f} avg_batch={avg_batch:.2f}")
  +        # New sub-breakdown for get_reqs gather loop
  +        if _PAR_TIMING:
  +            b = max(1, int(_GETREQS_PROF['batches']))
  +            avg_b = float(_GETREQS_PROF['total_reqs']) / float(b)
  +            top_bins = sorted(_GETREQS_PROF['bs_hist'].items(), key=lambda kv: kv[1], reverse=True)[:8]
  +            bins_str = ' '.join([f"{k}x{v}" for k, v in top_bins]) if len(top_bins) > 0 else ''
  +            tqdm.write(
  +                f"[par-getreqs-sub] batches={_GETREQS_PROF['batches']} total_reqs={_GETREQS_PROF['total_reqs']} avg_batch={avg_b:.2f}\n"
  +                f"  first_block={_GETREQS_PROF['t_first_block']:.3f}s(now={_GETREQS_PROF['cnt_first_ok']} to={_GETREQS_PROF['cnt_first_to']}) "
  +                f"nowait_drain={_GETREQS_PROF['t_nowait_drain']:.3f}s(ok={_GETREQS_PROF['cnt_nowait_ok']} empty={_GETREQS_PROF['cnt_nowait_empty']})\n"
  +                f"  topup_block={_GETREQS_PROF['t_topup_block']:.3f}s(ok={_GETREQS_PROF['cnt_topup_ok']} to={_GETREQS_PROF['cnt_topup_to']}) post_drain={_GETREQS_PROF['t_post_drain']:.3f}s\n"
  +                f"  bs_top_bins={bins_str}"
  +            )
  +        if _PAR_TIMING:
  +            # Batch size histogram summary (top bins)
  +            if len(bs_hist) > 0:
  +                import math as _math
  +                tot_batches = int(sum(bs_hist.values()))
  +                top_bins = sorted(bs_hist.items(), key=lambda kv: kv[1], reverse=True)[:8]
  +                bins_str = ' '.join([f"{k}x{v}" for k,v in top_bins])
  +                tqdm.write(f"[par-getreqs-hist] batches={tot_batches} step_reqs={step_reqs_count} other_reqs={other_reqs_count} top_bins={bins_str}")
  +            # Sub-timers for batched_select
  +            if _BATCHSEL_PROF['count'] > 0:
  +                c = max(1, int(_BATCHSEL_PROF['count']))
  +                avg_B = _BATCHSEL_PROF['sum_B'] / c
  +                avg_M = _BATCHSEL_PROF['sum_M'] / c
  +                top_cnt = sorted(_BATCHSEL_PROF['cnt_hist'].items(), key=lambda kv: kv[1], reverse=True)[:8]
  +                cnt_str = ' '.join([f"{k}x{v}" for k,v in top_cnt]) if len(top_cnt) > 0 else ''
  +                tqdm.write(
  +                    f"[par-batch_select-sub] calls={_BATCHSEL_PROF['count']} total={_BATCHSEL_PROF['t_total']:.3f}s avg_B={avg_B:.1f} avg_M={avg_M:.1f}\n"
  +                    f"  stack={_BATCHSEL_PROF['t_stack']:.3f}s leg_stack={_BATCHSEL_PROF['t_leg_stack']:.3f}s "
  +                    f"state_proj={_BATCHSEL_PROF['t_state_proj']:.3f}s action_enc={_BATCHSEL_PROF['t_action_enc']:.3f}s "
  +                    f"mask={_BATCHSEL_PROF['t_mask']:.3f}s score={_BATCHSEL_PROF['t_score']:.3f}s softmax_sample={_BATCHSEL_PROF['t_softmax_sample']:.3f}s\n"
  +                    f"  cnt_max_hist={cnt_str}")
  +            # Actor.compute_state_proj internal breakdown (if available)
  +            from utils.prof import snapshot_actor_stateproj, reset_actor_stateproj
  +            _sp = snapshot_actor_stateproj()
  +            if int(_sp.get('count', 0)) > 0:
  +                tqdm.write(
  +                    f"[par-state_proj-sub] calls={int(_sp['count'])} state_enc={_sp['t_state_enc']:.3f}s "
  +                    f"belief_logits={_sp['t_belief_logits']:.3f}s belief_probs={_sp['t_belief_probs']:.3f}s "
  +                    f"gates={_sp['t_belief_gates']:.3f}s merge={_sp['t_merge']:.3f}s proj={_sp['t_proj']:.3f}s"
  +                )
  +            reset_actor_stateproj()
  +        tqdm.write(f"[par-collect-post] build={t_build:.3f} values_gae={t_values_gae:.3f} old_logp={t_oldlogp:.3f} teardown={t_teardown:.3f}")
  +
  +    # Build batch CPU tensors from payloads
  +    _t_build0 = time.time() if _PAR_TIMING else 0.0
  +    obs_cpu = []
  +    act_cpu = []
  +    next_obs_cpu = []
  +    rew_list = []
  +    done_list = []
  +    seat_cpu = []
  +    belief_cpu = []
  +    legals_cpu = []
  +    leg_off = []
  +    leg_cnt = []
  +    chosen_idx = []
  +    for ep in episodes_payloads:
  +        # Incoming payload is NumPy; convert lazily per-sample
  +        obs_ep = [torch.as_tensor(x, dtype=torch.float32) for x in ep['obs']]
  +        next_obs_ep = [torch.as_tensor(x, dtype=torch.float32) for x in ep['next_obs']]
  +        act_ep = [torch.as_tensor(x, dtype=torch.float32) for x in ep['act']]
  +        seat_ep = [torch.as_tensor(x, dtype=torch.float32) for x in ep['seat']]
  +        # Determina reward episodio dal team 0/1 (proporzionale al risultato)
  +        tr = ep.get('team_rewards', [0.0, 0.0])
  +        t0 = float(tr[0])
  +        t1 = float(tr[1])
  +        rew_ep = []
  +        for st in seat_ep:
  +            team0_flag = bool(float(st[4]) > 0.5)
  +            if not team0_flag:
  +                seat_idx = int(torch.argmax(st[:4]).item())
  +                team0_flag = seat_idx in [0, 2]
  +            rew_ep.append(t0 if team0_flag else t1)
  +
  +        obs_cpu.extend(obs_ep)
  +        next_obs_cpu.extend(next_obs_ep)
  +        act_cpu.extend(act_ep)
  +        rew_list.extend(rew_ep)
  +        done_list.extend(ep['done'])
  +        seat_cpu.extend(seat_ep)
  +        belief_cpu.extend([torch.as_tensor(x, dtype=torch.float32) for x in ep['belief_summary']])
  +        # Adjust offsets by current length of legals_cpu
  +        base = len(legals_cpu)
  +        leg_off.extend([base + off for off in ep['leg_off']])
  +        leg_cnt.extend(ep['leg_cnt'])
  +        legals_cpu.extend([torch.as_tensor(x, dtype=torch.float32) for x in ep['legals']])
  +        chosen_idx.extend(ep['chosen_idx'])
  +        # distill/belief aux
  +        if 'mcts_policy' in ep:
  +            # adjust offsets later after concat
  +            pass
  +
  +    # Validate per-episode structure coherence before stacking
  +    if not (len(obs_cpu) == len(act_cpu) == len(next_obs_cpu) == len(done_list) == len(seat_cpu)):
  +        raise RuntimeError("collector: per-step arrays length mismatch among obs/act/next_obs/done/seat")
  +    # Stack to tensors
  +    _obs_dim = int(episodes_payloads[0]['obs'][0].__len__()) if len(episodes_payloads) > 0 and len(episodes_payloads[0]['obs']) > 0 else 1
  +    obs_cpu_t = torch.stack(obs_cpu, dim=0) if len(obs_cpu) > 0 else torch.zeros((0, _obs_dim), dtype=torch.float32)
  +    next_obs_cpu_t = torch.stack(next_obs_cpu, dim=0) if len(next_obs_cpu) > 0 else torch.zeros_like(obs_cpu_t)
  +    act_cpu_t = torch.stack(act_cpu, dim=0) if len(act_cpu) > 0 else torch.zeros((0, 80), dtype=torch.float32)
  +    seat_cpu_t = torch.stack(seat_cpu, dim=0) if len(seat_cpu) > 0 else torch.zeros((0, 6), dtype=torch.float32)
  +    belief_cpu_t = torch.stack(belief_cpu, dim=0) if len(belief_cpu) > 0 else torch.zeros((0, 120), dtype=torch.float32)
  +    legals_cpu_t = torch.stack(legals_cpu, dim=0) if len(legals_cpu) > 0 else torch.zeros((0, 80), dtype=torch.float32)
  +    leg_off_t = torch.as_tensor(leg_off, dtype=torch.long)
  +    leg_cnt_t = torch.as_tensor(leg_cnt, dtype=torch.long)
  +    chosen_idx_t = torch.as_tensor(chosen_idx, dtype=torch.long)
  +    # Distill/belief aux targets from episodes
  +    # Ricostruisci mcts_policy_flat per-sample mantenendo allineamento con leg_cnt/leg_off
  +    mcts_policy_flat = []
  +    mcts_weight = []
  +    others_hands = []
  +    for ep in episodes_payloads:
  +        cnts_ep = ep['leg_cnt']
  +        # policy vector per episodio: concatena, inserendo zeri per sample senza MCTS
  +        mcts_policy_ep = ep.get('mcts_policy', [])
  +        mcts_weight_ep = ep.get('mcts_weight', [0.0]*len(ep.get('obs', [])))
  +        # Sanity: se mancano, crea zeri del giusto totale
  +        total_legals_ep = sum(cnts_ep) if isinstance(cnts_ep, list) else int(torch.as_tensor(cnts_ep).sum().item())
  +        if len(mcts_policy_ep) == 0:
  +            mcts_policy_ep = [0.0] * total_legals_ep
  +        if len(mcts_weight_ep) == 0:
  +            mcts_weight_ep = [0.0] * len(ep.get('obs', []))
  +        # Append
  +        mcts_policy_flat.extend(mcts_policy_ep)
  +        mcts_weight.extend(mcts_weight_ep)
  +        if 'others_hands' in ep:
  +            for oh in ep['others_hands']:
  +                others_hands.append(torch.as_tensor(oh, dtype=torch.float32))
  +    mcts_policy_t = torch.as_tensor(mcts_policy_flat, dtype=torch.float32) if len(mcts_policy_flat)>0 else torch.zeros((0,), dtype=torch.float32)
  +    mcts_weight_t = torch.as_tensor(mcts_weight, dtype=torch.float32) if len(mcts_weight)>0 else torch.zeros((0,), dtype=torch.float32)
  +    others_hands_t = torch.stack(others_hands, dim=0) if len(others_hands)>0 else torch.zeros((0,3,40), dtype=torch.float32)
  +
  +    # End build
  +    if _PAR_TIMING:
  +        t_build = (time.time() - _t_build0)
  +
  +    # Validate ragged legals structure before computing values
  +    total_legals = len(legals_cpu)
  +    if int(torch.as_tensor(leg_cnt, dtype=torch.long).sum().item()) != total_legals:
  +        raise RuntimeError("collector: sum(legals_count) != len(legals)")
  +    # chosen indices must be within per-row counts
  +    for i, (off_i, cnt_i, ch_i) in enumerate(zip(leg_off, leg_cnt, chosen_idx)):
  +        if cnt_i <= 0:
  +            continue
  +        if not (0 <= int(ch_i) < int(cnt_i)):
  +            raise RuntimeError(f"collector: chosen_index out of range at row {i} (idx={int(ch_i)}, cnt={int(cnt_i)})")
  +    # seat_team sanity: shape and one-hot seat
  +    if len(seat_cpu) > 0:
  +        seat_mat = torch.stack(seat_cpu, dim=0)
  +        if seat_mat.size(1) != 6:
  +            raise RuntimeError("collector: seat_team must have shape (B,6)")
  +        if not (seat_mat[:, :4].sum(dim=1) == 1).all():
  +            raise RuntimeError("collector: seat one-hot invalid (sum != 1)")
  +    # Compute values and advantages on GPU similar to collect_trajectory (CTDE coerente)
  +    rew_t = torch.as_tensor(rew_list, dtype=torch.float32, device=device) if len(rew_list) > 0 else torch.zeros((0,), dtype=torch.float32, device=device)
  +    done_mask = torch.as_tensor([0.0 if not d else 1.0 for d in done_list], dtype=torch.float32, device=device) if len(done_list) > 0 else torch.zeros((0,), dtype=torch.float32, device=device)
  +    _t_val0 = time.time() if _PAR_TIMING else 0.0
  +    if len(rew_list) > 0:
  +        with torch.no_grad():
  +            if device.type == 'cuda':
  +                o_all = obs_cpu_t.pin_memory().to(device=device, dtype=torch.float32, non_blocking=True)
  +                s_all = seat_cpu_t.pin_memory().to(device=device, dtype=torch.float32, non_blocking=True)
  +            else:
  +                o_all = obs_cpu_t.to(device=device, dtype=torch.float32)
  +                s_all = seat_cpu_t.to(device=device, dtype=torch.float32)
  +            # others_hands per-step (CTDE): se raccolti dagli env worker, usa quelli
  +            if others_hands_t.numel() > 0 and others_hands_t.size(0) == o_all.size(0):
  +                oh_all = (others_hands_t.pin_memory().to(device=device, dtype=torch.float32, non_blocking=True) if device.type == 'cuda' else others_hands_t.to(device=device, dtype=torch.float32))
  +            else:
  +                oh_all = None
  +            val_t = agent.critic(o_all, s_all, oh_all)
  +            n_all = (next_obs_cpu_t.pin_memory().to(device=device, dtype=torch.float32, non_blocking=True) if device.type == 'cuda' else next_obs_cpu_t.to(device=device, dtype=torch.float32))
  +            # costruisci others_hands next (shift) se disponibile
  +            if oh_all is not None:
  +                oh_next = torch.zeros_like(oh_all)
  +                if oh_all.size(0) > 1:
  +                    oh_next[:-1] = oh_all[1:]
  +            else:
  +                oh_next = None
  +            nval_t = agent.critic(n_all, s_all, oh_next)
  +            nval_t = torch.where(done_mask.bool(), torch.zeros_like(nval_t), nval_t)
  +    else:
  +        val_t = torch.zeros((0,), dtype=torch.float32, device=device)
  +        nval_t = torch.zeros((0,), dtype=torch.float32, device=device)
  +    adv_vec = torch.zeros_like(rew_t)
  +    gae = torch.tensor(0.0, dtype=torch.float32, device=device)
  +    T = int(rew_t.numel())
  +    for t in reversed(range(T)):
  +        delta = rew_t[t] + nval_t[t] * gamma - val_t[t]
  +        gae = delta + gamma * lam * (1.0 - done_mask[t]) * gae
  +        adv_vec[t] = gae
  +    ret_vec = adv_vec + val_t
  +    if _PAR_TIMING:
  +        t_values_gae = (time.time() - _t_val0)
  +
  +    # Compute old_logp in batch on GPU (factored card + capture scheme)
  +    _t_olp0 = time.time() if _PAR_TIMING else 0.0
  +    if obs_cpu_t.size(0) > 0:
  +        with torch.no_grad():
  +            def to_pinned(x):
  +                x_cpu = (x.detach().to('cpu') if torch.is_tensor(x) else torch.as_tensor(x))
  +                return x_cpu.pin_memory() if device.type == 'cuda' else x_cpu
  +            nb = (device.type == 'cuda')
  +            obs_t = to_pinned(obs_cpu_t).to(device=device, dtype=torch.float32, non_blocking=nb)
  +            seat_t = to_pinned(seat_cpu_t).to(device=device, non_blocking=nb)
  +            leg_t = to_pinned(legals_cpu_t).to(device=device, non_blocking=nb)
  +            offs = (leg_off_t.pin_memory().to(device=device, non_blocking=nb) if device.type == 'cuda' else leg_off_t.to(device=device))
  +            cnts = (leg_cnt_t.pin_memory().to(device=device, non_blocking=nb) if device.type == 'cuda' else leg_cnt_t.to(device=device))
  +            B = obs_t.size(0)
  +            max_cnt = int(cnts.max().item()) if B > 0 else 0
  +            if max_cnt > 0:
  +                # State projection and card logits
  +                state_proj = agent.actor.compute_state_proj(obs_t, seat_t)  # (B,64)
  +                card_logits_all = torch.matmul(state_proj, agent.actor.card_emb_play.t())  # (B,40)
  +                pos = torch.arange(max_cnt, device=device, dtype=torch.long)
  +                rel_pos_2d = pos.unsqueeze(0).expand(B, max_cnt)
  +                mask = rel_pos_2d < cnts.unsqueeze(1)
  +                abs_idx = (offs.unsqueeze(1) + rel_pos_2d)[mask]
  +                sample_idx = torch.arange(B, device=device, dtype=torch.long).unsqueeze(1).expand(B, max_cnt)[mask]
  +                legals_mb = leg_t[abs_idx].contiguous()                   # (M_mb,80)
  +                played_ids_mb = torch.argmax(legals_mb[:, :40], dim=1)    # (M_mb)
  +                # Card log-prob restricted to allowed cards only (two-stage policy)
  +                allowed_mask = torch.zeros((B, 40), dtype=torch.bool, device=device)
  +                allowed_mask[sample_idx, played_ids_mb] = True
  +                neg_inf = torch.full_like(card_logits_all, float('-inf'))
  +                masked_logits = torch.where(allowed_mask, card_logits_all, neg_inf)
  +                max_allowed = torch.amax(masked_logits, dim=1)
  +                exp_shift_allowed = torch.exp(card_logits_all - max_allowed.unsqueeze(1)) * allowed_mask.to(card_logits_all.dtype)
  +                sum_allowed = exp_shift_allowed.sum(dim=1)
  +                lse_allowed = max_allowed + torch.log(torch.clamp_min(sum_allowed, 1e-12))
  +                # Map chosen indices to absolute and card ids
  +                chosen_clamped = (torch.minimum(chosen_idx_t.pin_memory().to(device=device), (cnts - 1).clamp_min(0)) if device.type == 'cuda' else torch.minimum(chosen_idx_t.to(device=device), (cnts - 1).clamp_min(0)))
  +                chosen_abs = (offs + chosen_clamped)
  +                total_legals = leg_t.size(0)
  +                pos_map = torch.full((total_legals,), -1, dtype=torch.long, device=device)
  +                pos_map[abs_idx] = torch.arange(abs_idx.numel(), device=device, dtype=torch.long)
  +                chosen_pos = pos_map[chosen_abs]
  +                # Validate chosen_pos mapping succeeded for all rows
  +                if bool((chosen_pos < 0).any().item() if chosen_pos.numel() > 0 else False):
  +                    bad_rows = torch.nonzero(chosen_pos < 0, as_tuple=False).flatten().tolist()
  +                    raise RuntimeError(f"collect_trajectory_parallel: chosen_pos mapping failed for rows {bad_rows}")
  +                played_ids_all = torch.argmax(leg_t[:, :40], dim=1)
  +                chosen_card_ids = played_ids_all[chosen_abs]
  +                logp_card = card_logits_all[torch.arange(B, device=device), chosen_card_ids] - lse_allowed[torch.arange(B, device=device)]
  +                # Capture log-softmax within card groups
  +                a_emb_mb = agent.actor.action_enc(legals_mb)               # (M_mb,64)
  +                cap_logits = (a_emb_mb * state_proj[sample_idx]).sum(dim=1)
  +                group_ids = sample_idx * 40 + played_ids_mb
  +                num_groups = B * 40
  +                group_max = torch.full((num_groups,), float('-inf'), dtype=cap_logits.dtype, device=device)
  +                group_max.scatter_reduce_(0, group_ids, cap_logits, reduce='amax', include_self=True)
  +                gmax_per_legal = group_max[group_ids]
  +                exp_shifted = torch.exp(cap_logits - gmax_per_legal)
  +                group_sum = torch.zeros((num_groups,), dtype=cap_logits.dtype, device=device)
  +                group_sum.index_add_(0, group_ids, exp_shifted)
  +                lse_per_legal = gmax_per_legal + torch.log(torch.clamp_min(group_sum[group_ids], 1e-12))
  +                logp_cap_per_legal = cap_logits - lse_per_legal
  +                logp_cap = logp_cap_per_legal[chosen_pos]
  +                old_logp_t = (logp_card + logp_cap)
  +                # Early validity for old_logp
  +                if not torch.isfinite(old_logp_t).all():
  +                    raise RuntimeError("collect_trajectory_parallel: old_logp contains non-finite values")
  +                if bool((old_logp_t > 1e-6).any().item() if old_logp_t.numel() > 0 else False):
  +                    mx = float(old_logp_t.max().item())
  +                    raise RuntimeError(f"collect_trajectory_parallel: old_logp contains positive values (max={mx})")
  +                if bool((old_logp_t < -100.0).any().item() if old_logp_t.numel() > 0 else False):
  +                    idx_bad = torch.nonzero(old_logp_t < -100.0, as_tuple=False).flatten()
  +                    r = int(idx_bad[0].item()) if idx_bad.numel() > 0 else 0
  +                    chosen_card = int(chosen_card_ids[r].item()) if chosen_card_ids.numel() > r else -1
  +                    cnt_r = int(cnts[r].item()) if cnts.numel() > r else -1
  +                    grp_mask = (sample_idx == r) & (played_ids_mb == chosen_card)
  +                    cap_logits_grp = cap_logits[grp_mask]
  +                    def _stats(t):
  +                        return {
  +                            'min': float(t.min().item()) if t.numel() > 0 else None,
  +                            'max': float(t.max().item()) if t.numel() > 0 else None,
  +                            'mean': float(t.mean().item()) if t.numel() > 0 else None,
  +                            'numel': int(t.numel())
  +                        }
  +                    raise RuntimeError(
  +                        f"collect_trajectory_parallel: old_logp extremely small (min={float(old_logp_t.min().item())}); "
  +                        f"row={r}, chosen_card={chosen_card}, legals_count={cnt_r}, "
  +                        f"logp_card={float(logp_card[r].item())}, logp_cap={float(logp_cap[r].item())}, "
  +                        f"card_logits_all_stats={_stats(card_logits_all[r])}, cap_logits_grp_stats={_stats(cap_logits_grp)}"
  +                    )
  +            else:
  +                old_logp_t = torch.zeros((B,), dtype=torch.float32, device=device)
  +    else:
  +        old_logp_t = torch.zeros((0,), dtype=torch.float32, device=device)
  +    if _PAR_TIMING:
  +        t_oldlogp = (time.time() - _t_olp0)
  +
  +    ep_debug_lengths = episode_lengths if len(episode_lengths) > 0 else None
  +    _maybe_log_ppo_batch(
  +        'parallel',
  +        rew_t,
  +        ret_vec,
  +        adv_vec,
  +        val_t,
  +        nval_t,
  +        done_mask,
  +        old_logp=old_logp_t,
  +        seat_tensor=seat_cpu_t,
  +        episode_lengths=ep_debug_lengths,
  +        extra={'episodes': episodes},
  +    )
  +
  +    batch = {
  +        'obs': obs_cpu_t,
  +        'act': act_cpu_t,
  +        # Mantieni tensori chiave su CUDA per evitare D2H→H2D
  +        'old_logp': old_logp_t.detach(),
  +        'ret': ret_vec.detach(),
  +        'adv': adv_vec.detach(),
  +        'rew': rew_t,
  +        'done': torch.as_tensor(done_list, dtype=torch.bool, device=device) if len(done_list)>0 else torch.zeros((0,), dtype=torch.bool, device=device),
  +        'seat_team': seat_cpu_t,
  +        'belief_summary': belief_cpu_t,
  +        'legals': legals_cpu_t,
  +        'legals_offset': leg_off_t,
  +        'legals_count': leg_cnt_t,
  +        'chosen_index': chosen_idx_t,
  +        'mcts_policy': mcts_policy_t,
  +        'mcts_weight': mcts_weight_t,
  +        'others_hands': others_hands_t,
  +        'routing_log': [('parallel', 'main')],
  +    }
  +    return batch
  +
  +
  +def train_ppo(num_iterations: int = 1000, horizon: int = 256, save_every: int = 10, ckpt_path: str = 'checkpoints/ppo_ac.pth', k_history: int = 39, seed: int = 0,
  +              entropy_schedule_type: str = 'linear', eval_every: int = 0, eval_games: int = 10, belief_particles: int = 512, belief_ess_frac: float = 0.5,
  +              mcts_in_eval: bool = True, mcts_train: bool = True, mcts_sims: int = 128, mcts_sims_eval: Optional[int] = None, mcts_dets: int = 4, mcts_c_puct: float = 1.0, mcts_root_temp: float = 0.0,
  +              mcts_prior_smooth_eps: float = 0.0, mcts_dirichlet_alpha: float = 0.25, mcts_dirichlet_eps: float = 0.25,
  +              num_envs: int = 32,
  +              train_both_teams: bool = True,
  +              use_selfplay: bool = True,
  +              mcts_warmup_iters: Optional[int] = 500,
  +              on_iter_end: Optional[Callable[[int], None]] = None):
  +    # Enforce minimum horizon of 40 and align horizon to minibatch size
  +    horizon = max(40, int(horizon))
  +    # Read minibatch size once and align horizon to LCM(minibatch_size, per-episode useful transitions)
  +    import os as _os
  +    import math as _math
  +    minibatch_size = 4096
  +    _mb_env = int(_os.environ.get('SCOPONE_MINIBATCH', str(minibatch_size)))
  +    if _mb_env > 0:
  +        minibatch_size = _mb_env
  +    # Determine effective per-episode useful transitions based on desired collection semantics
  +    _tfb_env = str(_os.environ.get('SCOPONE_TRAIN_FROM_BOTH_TEAMS', '0')).strip().lower() in ['1','true','yes','on']
  +    _frozen_env = str(_os.environ.get('SCOPONE_OPP_FROZEN', '1')).strip().lower() in ['1','true','yes','on']
  +    # In selfplay, collecting from both teams only makes sense when opponent is not frozen
  +    _collect_both = (bool(use_selfplay) and _tfb_env and (not _frozen_env))
  +    _per_ep_util = (40 if _collect_both else 20)
  +    lcm_mb_ep = (abs(minibatch_size * _per_ep_util) // _math.gcd(minibatch_size, _per_ep_util)) if (minibatch_size > 0 and _per_ep_util > 0) else max(minibatch_size, _per_ep_util)
  +    if lcm_mb_ep > 0 and (horizon % lcm_mb_ep) != 0:
  +        new_h = ((horizon + lcm_mb_ep - 1) // lcm_mb_ep) * lcm_mb_ep
  +        tqdm.write(f"[horizon] adjusted to LCM(mb={minibatch_size}, per_ep={_per_ep_util})={lcm_mb_ep}: {horizon} -> {new_h}")
  +        horizon = new_h
  +    # Resolve and announce final seed (seed<0 => random per run)
  +    seed = resolve_seed(seed)
  +    set_global_seeds(seed)
  +    tqdm.write(f"[seed] Using seed={seed}")
  +    # Disattiva reward shaping intermedio: solo reward finale
  +    env = ScoponeEnvMA(rules={'shape_scopa': False}, k_history=k_history)
  +    obs_dim = env.observation_space.shape[0]
  +    # Passa k_history al modello per evitare inferenze fragili di k
  +    agent = ActionConditionedPPO(obs_dim=obs_dim, k_history=k_history)
  +    # Allow big-compute-on-GPU during update while env/collection stays on CPU.
  +    # Controlled by env var SCOPONE_TRAIN_DEVICE (cpu|cuda[:id]). Defaults to cpu.
  +    # Models live on CPU for collection; moved to train device inside agent.update.
  +
  +    # Discover existing checkpoints to possibly resume later (actual load happens after league init)
  +    best_ckpt_path = ckpt_path.replace('.pth', '_best.pth') if isinstance(ckpt_path, str) else 'checkpoints/ppo_ac_best.pth'
  +    best_wr_ckpt_path = ckpt_path.replace('.pth', '_bestwr.pth') if isinstance(ckpt_path, str) else 'checkpoints/ppo_ac_bestwr.pth'
  +    resume_ckpt = None
  +    def _is_bootstrap_path(p: str) -> bool:
  +        try:
  +            return ('bootstrap' in os.path.basename(p).lower())
  +        except Exception:
  +            return False
  +
  +    for _p in [ckpt_path, best_ckpt_path, best_wr_ckpt_path]:
  +        if _p and (not _is_bootstrap_path(_p)) and os.path.isfile(_p) and os.path.getsize(_p) > 0:
  +            resume_ckpt = _p
  +            break
  +    if resume_ckpt is None:
  +        import glob as _glob
  +        _all = [p for p in _glob.glob(os.path.join('checkpoints', '*.pth')) if (not _is_bootstrap_path(p)) and os.path.isfile(p) and os.path.getsize(p) > 0]
  +        if _all:
  +            resume_ckpt = max(_all, key=lambda p: os.path.getmtime(p))
  +
  +    warmup_iters = 0 if mcts_warmup_iters is None else max(0, int(mcts_warmup_iters))
  +
  +    # Cosine annealing LR schedulers
  +    actor_sched = optim.lr_scheduler.CosineAnnealingLR(agent.opt_actor, T_max=max(1, num_iterations))
  +    critic_sched = optim.lr_scheduler.CosineAnnealingLR(agent.opt_critic, T_max=max(1, num_iterations))
  +    agent.add_lr_schedulers(actor_sched, critic_sched)
  +
  +    # entropy schedules
  +    def entropy_schedule_linear(step: int, start: float = 0.01, end: float = 0.001, decay_steps: int = 100000):
  +        if step >= decay_steps:
  +            return end
  +        frac = (decay_steps - step) / decay_steps
  +        return end + (start - end) * frac
  +
  +    def entropy_schedule_cosine(step: int, start: float = 0.01, end: float = 0.001, period: int = 100000):
  +        import math
  +        t = min(step, period)
  +        cos = (1 + math.cos(math.pi * t / period)) / 2.0
  +        return end + (start - end) * cos
  +
  +    if entropy_schedule_type == 'cosine':
  +        agent.set_entropy_schedule(lambda s: entropy_schedule_cosine(s))
  +    else:
  +        agent.set_entropy_schedule(lambda s: entropy_schedule_linear(s))
  +
  +    writer = None
  +    if os.environ.get('SCOPONE_DISABLE_TB', '0') != '1':
  +        from torch.utils.tensorboard import SummaryWriter as _SummaryWriter
  +        writer = _SummaryWriter(log_dir='runs/ppo_ac')
  +    if writer is not None:
  +        # Spiega le metriche chiave in TensorBoard
  +        writer.add_text('help/metrics',
  +                        '\n'.join([
  +                            'train/loss_pi: Surrogate loss PPO (policy). Negativo = miglioramento dell\'obiettivo.',
  +                            'train/loss_v: Value loss (MSE). Alto = critico lontano dai return.',
  +                            'train/entropy: Entropia media della policy (azioni legali). Più alta = più esplorazione.',
  +                            'train/approx_kl: KL media (stima) tra policy nuova e vecchia (per early stop).',
  +                            'train/avg_kl: KL media aggregata su tutti i minibatch dell\'update.',
  +                            'train/clip_frac: Frazione di campioni con rapporto clipppato (|r-1| > ε).',
  +                            'train/avg_clip_frac: Media del clip_frac sui minibatch.',
  +                            'train/grad_norm_actor / train/grad_norm_critic: Norma L2 dei gradienti (diagnostica stabilità).',
  +                            'train/lr_actor / train/lr_critic: Learning rate correnti (post scheduler).',
  +                            'train/episode_time_s: Tempo per iterazione (raccolta + update).',
  +                            'train/avg_return: Return medio del batch raccolto (proxy qualità corrente).',
  +                            'by_seat/ret_02, by_seat/ret_13: Return medio per gruppo di posti (0/2 vs 1/3).',
  +                            'by_seat/kl_02, by_seat/kl_13: KL media per gruppo di posti.',
  +                            'by_seat/entropy_02, by_seat/entropy_13: Entropia media per gruppo di posti.',
  +                            'by_seat/clip_frac_02, by_seat/clip_frac_13: Frazione di clipping per gruppo.',
  +                            'league/mini_eval_wr: Win-rate nella mini-valutazione vs checkpoint precedente.',
  +                            'league/elo_current / league/elo_previous: Elo nel league per corrente/precedente.',
  +                        ]), 0)
  +    league = League(base_dir='checkpoints/league')
  +    # Helper to build MCTS config for eval consistently from trainer flags
  +    def _make_mcts_cfg_for_eval():
  +        if not mcts_in_eval:
  +            return None
  +        return {
  +            'sims': int(mcts_sims_eval if mcts_sims_eval is not None else mcts_sims),
  +            'dets': mcts_dets,
  +            'c_puct': mcts_c_puct,
  +            'root_temp': mcts_root_temp,
  +            'prior_smooth_eps': mcts_prior_smooth_eps,
  +            'root_dirichlet_alpha': mcts_dirichlet_alpha,
  +            'root_dirichlet_eps': mcts_dirichlet_eps,
  +            'robust_child': True,
  +        }
  +    # Optional startup refresh: SCOPONE_LEAGUE_REFRESH in {1,0}
  +    _refresh_flag = str(os.environ.get('SCOPONE_LEAGUE_REFRESH', '1')).strip().lower() in ['1','true','yes','on']
  +    if _refresh_flag:
  +        tqdm.write("[league-refresh] starting league refresh from disk …")
  +        # Refresh league from disk: scan for existing checkpoints not yet registered.
  +        # We scan both 'checkpoints/' and 'checkpoints/league/' and ignore bootstrap/empty files.
  +        def _refresh_league_from_disk(_league):
  +            old_hist = list(getattr(_league, 'history', []) or [])
  +            roots = ['checkpoints', os.path.join('checkpoints', 'league')]
  +            seen = set(getattr(_league, 'history', []) or [])
  +            added = 0
  +            scanned = 0
  +            pth_seen = 0
  +            skipped = 0
  +            added_paths = []
  +            for root in roots:
  +                if not os.path.isdir(root):
  +                    tqdm.write(f"[league-refresh] root '{root}' missing; skipping root scan")
  +                    continue
  +                tqdm.write(f"[league-refresh] scanning root: {os.path.abspath(root)}")
  +                for dirpath, _dirs, files in os.walk(root):
  +                    for fname in files:
  +                        scanned += 1
  +                        fpath = os.path.join(dirpath, fname)
  +                        if not fname.endswith('.pth'):
  +                            tqdm.write(f"[league-refresh] ignore {fpath}: reason=ext_mismatch")
  +                            skipped += 1
  +                            continue
  +                        pth_seen += 1
  +                        low = fname.lower()
  +                        reason = None
  +                        if 'bootstrap' in low:
  +                            reason = 'bootstrap_file'
  +                        elif fpath in seen:
  +                            reason = 'already_registered'
  +                        elif not os.path.isfile(fpath):
  +                            reason = 'not_a_file'
  +                        else:
  +                            if os.path.getsize(fpath) <= 0:
  +                                reason = 'empty_file'
  +                        if reason is not None:
  +                            tqdm.write(f"[league-refresh] skip {fpath}: reason={reason}")
  +                            skipped += 1
  +                            continue
  +                        # Deep validation: try loading the checkpoint to detect truncated/invalid formats
  +                        try:
  +                            _sz = os.path.getsize(fpath)
  +                            _ck = torch.load(fpath, map_location='cpu')
  +                            del _ck
  +                        except Exception as _load_e:
  +                            tqdm.write(
  +                                f"[league-refresh] skip {fpath}: reason=load_error type={type(_load_e).__name__} size={_sz} msg={_load_e!r}"
  +                            )
  +                            skipped += 1
  +                            continue
  +                        _league.register(fpath)
  +                        seen.add(fpath)
  +                        added += 1
  +                        added_paths.append(fpath)
  +            tqdm.write(f"[league] Refresh scan: roots={len(roots)} scanned_files={scanned} pth_seen={pth_seen} added={added} skipped={skipped} total_now={len(_league.history)}")
  +            if added > 0:
  +                tqdm.write(f"[league] Refreshed from disk: +{added} new checkpoints (total={len(_league.history)})")
  +            # Evaluate newly added checkpoints to update Elo
  +            if added > 0:
  +                # Reference is the current top1 among the pre-existing history, if any
  +                ref_candidates = [p for p in old_hist if os.path.isfile(p)]
  +                if len(ref_candidates) >= 1:
  +                    # pick highest Elo among old history
  +                    ref = max(ref_candidates, key=lambda p: float(_league.elo.get(p, 1000.0)))
  +                    for npth in added_paths:
  +                        if npth == ref:
  +                            continue
  +                        diff, bd = evaluate_pair_actors(npth, ref, games=eval_games, k_history=k_history,
  +                                                     mcts=_make_mcts_cfg_for_eval(),
  +                                                     belief_particles=(belief_particles if mcts_in_eval else 0),
  +                                                     belief_ess_frac=belief_ess_frac,
  +                                                     tqdm_desc=f"League refresh: {os.path.basename(npth)} vs top",
  +                                                     tqdm_position=2)
  +                        _league.update_elo_from_diff(npth, ref, diff)
  +                elif len(added_paths) >= 2:
  +                    # Evaluate sequentially among new paths to seed relative Elo
  +                    anchor = added_paths[0]
  +                    for npth in added_paths[1:]:
  +                        diff, bd = evaluate_pair_actors(npth, anchor, games=eval_games, k_history=k_history,
  +                                                     mcts=_make_mcts_cfg_for_eval(),
  +                                                     belief_particles=(belief_particles if mcts_in_eval else 0),
  +                                                     belief_ess_frac=belief_ess_frac,
  +                                                     tqdm_desc=f"League refresh: {os.path.basename(npth)} vs anchor",
  +                                                     tqdm_position=2)
  +                        _league.update_elo_from_diff(npth, anchor, diff)
  +
  +        _LP = globals().get('LINE_PROFILE_DECORATOR', None)
  +        if _LP is not None:
  +            _refresh_league_from_disk = _LP(_refresh_league_from_disk)
  +        tqdm.write("[league-refresh] invoking refresh function")
  +        _refresh_league_from_disk(league)
  +    else:
  +        tqdm.write("[league] Startup refresh disabled (SCOPONE_LEAGUE_REFRESH=0)")
  +        
  +    # Purge any bootstrap entries from league history on startup
  +    if getattr(league, 'history', None):
  +        boot_hist = [p for p in list(league.history) if 'bootstrap' in os.path.basename(p).lower()]
  +        if boot_hist:
  +            league.history = [p for p in league.history if p not in boot_hist]
  +            for p in boot_hist:
  +                if p in league.elo:
  +                    del league.elo[p]
  +            # Save cleaned league state
  +            league._save()
  +            tqdm.write(f"[league] Purged bootstrap entries from league: {len(boot_hist)} removed")
  +    if len(getattr(league, 'history', [])) == 0:
  +        # Warm-start the league with the resume checkpoint if present (ignore bootstrap files)
  +        if resume_ckpt and (('bootstrap' not in os.path.basename(resume_ckpt).lower())) and os.path.isfile(resume_ckpt) and os.path.getsize(resume_ckpt) > 0:
  +            league.register(resume_ckpt)
  +            tqdm.write(f"[league] Warm-started with existing checkpoint: {resume_ckpt}")
  +
  +    # Helper to get top-k checkpoints by Elo (fallback Elo=1000.0 for missing entries)
  +    def _league_top_k(_league: 'League', k: int) -> List[str]:
  +        hist = list(getattr(_league, 'history', []) or [])
  +        if not hist:
  +            return []
  +        elo_map = getattr(_league, 'elo', {}) or {}
  +        ranked = sorted(hist, key=lambda p: float(elo_map.get(p, 1000.0)), reverse=True)
  +        return ranked[:max(0, int(k))]
  +
  +    
  +
  +    # Warm-start policy controlled by SCOPONE_WARM_START: '0' start-from-scratch, '1' force top1 clone, '2' use top2 if available
  +    warm_start_mode = str(os.environ.get('SCOPONE_WARM_START', '2')).strip()
  +    league_hist = list(getattr(league, 'history', []) or [])
  +    topN = _league_top_k(league, 2) if league_hist else []
  +    # Decide resume checkpoint(s)
  +    resume_A = None
  +    resume_B = None
  +    if warm_start_mode == '0':
  +        resume_A = None; resume_B = None
  +    elif warm_start_mode == '1':
  +        resume_A = (topN[0] if len(topN) >= 1 else resume_ckpt)
  +        resume_B = resume_A
  +    else:  # '2' default
  +        resume_A = (topN[0] if len(topN) >= 1 else resume_ckpt)
  +        resume_B = (topN[1] if len(topN) >= 2 else resume_A)
  +    # Apply resume to main agent (always)
  +    if resume_A:
  +        agent.load(resume_A, map_location='cpu')
  +        tqdm.write(f"[resume] Loaded agent(A) from {resume_A}")
  +
  +    partner_actor = None
  +    opponent_actor = None
  +    # Semantics:
  +    # - SELFPLAY decides number of nets: True -> 1 net; False -> 2 nets (A,B)
  +    # - SCOPONE_TRAIN_FROM_BOTH_TEAMS decides transitions usage when applicable
  +    #   NOTE: TRAIN_FROM_BOTH_TEAMS is effective ONLY when SELFPLAY=1 and OPP_FROZEN=0.
  +    #         Rationale: on-policy PPO must update a policy with its own data. In dual-nets or
  +    #         when the opponent is frozen, we never mix opponent (non-learning) transitions into
  +    #         the learner's update.
  +    train_from_both = str(os.environ.get('SCOPONE_TRAIN_FROM_BOTH_TEAMS', '0')).strip().lower() in ['1','true','yes','on']
  +    opp_frozen_env = str(os.environ.get('SCOPONE_OPP_FROZEN', '1')).strip().lower() in ['1','true','yes','on']
  +    # Validate combinations under new semantics
  +    # With SELFPLAY=False we always have two nets; frozen=0 means co-training live; frozen=1 means alternate.
  +
  +    if use_selfplay:
  +        # In selfplay we have a single net. train_from_both means use both teams' transitions.
  +        # If opp_frozen_env is True, we must not use opponent transitions (they come from a frozen mirror).
  +        train_both_teams = bool(train_from_both and (not opp_frozen_env))
  +        dual_team_nets = False
  +    else:
  +        # In non-selfplay we always maintain two separate nets (A,B)
  +        dual_team_nets = True
  +        # In dual-nets collection, each update call uses only main seats for that net
  +        # so treat per-episode util as 20 everywhere in collection paths
  +        train_both_teams = False
  +        agent_teamB = ActionConditionedPPO(obs_dim=obs_dim, k_history=k_history)
  +        # Load resume_B according to warm-start policy
  +        if warm_start_mode == '0':
  +            resumeB = None
  +        elif warm_start_mode == '1':
  +            resumeB = resume_A
  +        else:
  +            resumeB = resume_B
  +        if resumeB:
  +            agent_teamB.load(resumeB, map_location='cpu')
  +            tqdm.write(f"[resume] Loaded team-B agent from {resumeB}")
  +    # alterna il main actor tra seat 0/2 e 1/3 per episodi
  +    even_main_seats = [0, 2]
  +    odd_main_seats = [1, 3]
  +
  +    best_return = -1e9
  +    best_ckpt_path = ckpt_path.replace('.pth', '_best.pth')
  +    best_wr = -1e9
  +    best_wr_ckpt_path = ckpt_path.replace('.pth', '_bestwr.pth')
  +
  +    # Two-line UI: top line shows metrics, second line is the progress bar
  +    _TQDM_DISABLE = (os.environ.get('TQDM_DISABLE','0') in ['1','true','yes','on'])
  +    metrics_bar = tqdm(total=1, desc="", position=0, dynamic_ncols=True, bar_format='{desc}', leave=True, disable=_TQDM_DISABLE)
  +    pbar = tqdm(range(num_iterations), desc="PPO iterations", dynamic_ncols=True, position=1, leave=True, disable=_TQDM_DISABLE)
  +    shadow_actor = None
  +    shadow_every = max(1, int(os.environ.get('SCOPONE_FROZEN_UPDATE_EVERY', '1')))
  +    for it in pbar:
  +        t0 = time.time()
  +        # Iteration-level timing breakdown (enabled by SCOPONE_PROFILE=1)
  +        _iter_t_collect = 0.0
  +        _iter_t_preproc = 0.0
  +        _iter_t_update = 0.0
  +        # Partner/opponent selection policy
  +        # Opponent selection
  +        # SELFPLAY=1: no frozen unless explicitly requested via OPP_FROZEN (still treated as single learner vs frozen)
  +        # SELFPLAY=0: choose according to SCOPONE_START_OPP='top1'|'top2'|'bootstrap' when OPP_FROZEN=1, else live opponent (requires BOTH=1)
  +        # Auto-bootstrap only when a frozen opponent is required but the league is empty
  +        if opp_frozen_env and (len(getattr(league, 'history', [])) == 0):
  +            _bootstrap_dir = 'checkpoints'
  +            _bootstrap_path = os.path.join(_bootstrap_dir, 'bootstrap_random.pth')
  +            os.makedirs(_bootstrap_dir, exist_ok=True)
  +            if (not os.path.isfile(_bootstrap_path)) or (os.path.getsize(_bootstrap_path) <= 0):
  +                tqdm.write("[league] Bootstrapping empty league with random frozen actor → checkpoints/bootstrap_random.pth")
  +                _rand_actor = ActionConditionedActor(obs_dim=obs_dim)
  +                torch.save({'actor': _rand_actor.state_dict()}, _bootstrap_path)
  +            # Do NOT register bootstrap into league; use ad-hoc only
  +        if use_selfplay:
  +            p_ckpt, o_ckpt = None, None
  +            if opp_frozen_env:
  +                # Single learner vs frozen mirror
  +                top1 = _league_top_k(league, 1)
  +                p_ckpt = o_ckpt = (top1[0] if top1 else None)
  +        else:
  +            if opp_frozen_env:
  +                start_opp = str(os.environ.get('SCOPONE_START_OPP', 'top1')).strip().lower()
  +                hist = list(getattr(league, 'history', []) or [])
  +                if start_opp == 'bootstrap' and not hist:
  +                    _bootstrap_dir = 'checkpoints'
  +                    _bootstrap_path = os.path.join(_bootstrap_dir, 'bootstrap_random.pth')
  +                    os.makedirs(_bootstrap_dir, exist_ok=True)
  +                    if (not os.path.isfile(_bootstrap_path)) or (os.path.getsize(_bootstrap_path) <= 0):
  +                        tqdm.write("[league] Bootstrapping empty league with random frozen actor → checkpoints/bootstrap_random.pth")
  +                        _rand_actor = ActionConditionedActor(obs_dim=obs_dim)
  +                        torch.save({'actor': _rand_actor.state_dict()}, _bootstrap_path)
  +                    # Do NOT register bootstrap; use directly for this start if needed
  +                    hist = []
  +                if start_opp == 'top2':
  +                    top2 = _league_top_k(league, 2)
  +                    if len(top2) >= 2:
  +                        p_ckpt, o_ckpt = top2[0], top2[1]
  +                    elif len(top2) == 1:
  +                        p_ckpt = o_ckpt = top2[0]
  +                    else:
  +                        # Fallback to bootstrap pair only if history empty
  +                        _bootstrap_path = os.path.join('checkpoints', 'bootstrap_random.pth')
  +                        p_ckpt = o_ckpt = (_bootstrap_path if os.path.isfile(_bootstrap_path) and os.path.getsize(_bootstrap_path) > 0 else None)
  +                else:
  +                    # default 'top1': both sides face strongest
  +                    top1 = _league_top_k(league, 1)
  +                    if top1:
  +                        p_ckpt = o_ckpt = top1[0]
  +                    else:
  +                        _bootstrap_path = os.path.join('checkpoints', 'bootstrap_random.pth')
  +                        p_ckpt = o_ckpt = (_bootstrap_path if os.path.isfile(_bootstrap_path) and os.path.getsize(_bootstrap_path) > 0 else None)
  +            else:
  +                # live opponent (co-training). Requires BOTH=1; validated above
  +                p_ckpt, o_ckpt = None, None
  +        # Simple local cache to avoid reloading frozen actors every iter
  +        if not use_selfplay:
  +            if not hasattr(league, '_frozen_cache'):
  +                league._frozen_cache = {}
  +            if p_ckpt and os.path.isfile(p_ckpt):
  +                partner_actor = league._frozen_cache.get(p_ckpt)
  +                if partner_actor is None:
  +                    partner_actor = _load_frozen_actor(p_ckpt, obs_dim)
  +                    league._frozen_cache[p_ckpt] = partner_actor
  +            if o_ckpt and os.path.isfile(o_ckpt):
  +                opponent_actor = league._frozen_cache.get(o_ckpt)
  +                if opponent_actor is None:
  +                    opponent_actor = _load_frozen_actor(o_ckpt, obs_dim)
  +                    league._frozen_cache[o_ckpt] = opponent_actor
  +            # Live co-training override: use teamB live actor as partner/opponent
  +            if dual_team_nets and (not opp_frozen_env):
  +                partner_actor = agent_teamB.actor
  +                opponent_actor = agent_teamB.actor
  +        # Alternation controls for dual nets with frozen opponent
  +        # SCOPONE_ALTERNATE_ITERS: in dual-nets+frozen, number of iterations to train one net
  +        #   before swapping roles (A<->B). Larger values give more stable short-term learning
  +        #   per net; smaller values promote frequent role switches.
  +        alt_k = max(1, int(os.environ.get('SCOPONE_ALTERNATE_ITERS', '1')))
  +        train_A_now = ((it // alt_k) % 2 == 0)
  +        main_seats = even_main_seats if (it % 2 == 0) else odd_main_seats
  +        # Complementary seats for the other team (used when swapping A<->B)
  +        main_seats_B = odd_main_seats if (main_seats == even_main_seats) else even_main_seats
  +        use_parallel = (num_envs is not None and int(num_envs) > 1)
  +        if bool(mcts_train):
  +            mcts_train_factor = 0.0 if it < warmup_iters else 1.0
  +        else:
  +            mcts_train_factor = 0.0
  +        # Eval mode during data collection (dropout/BN off)
  +        agent.actor.eval()
  +        agent.critic.eval()
  +        if use_parallel:
  +            # Mirror single-env logic: choose episodes so that batch B is a multiple of minibatch_size
  +            per_ep_util = (40 if train_both_teams else 20)
  +            # Compute LCM(mb, per_ep_util) and derive episodes from aligned horizon
  +            import math as _math
  +            lcm_mb_ep = (abs(minibatch_size * per_ep_util) // _math.gcd(minibatch_size, per_ep_util)) if (minibatch_size > 0 and per_ep_util > 0) else max(minibatch_size, per_ep_util)
  +            if lcm_mb_ep > 0 and (horizon % lcm_mb_ep) != 0:
  +                new_h = ((horizon + lcm_mb_ep - 1) // lcm_mb_ep) * lcm_mb_ep
  +                print(f"[horizon] adjusted to LCM(mb={minibatch_size}, per_ep={per_ep_util})={lcm_mb_ep}: {horizon} -> {new_h}")
  +                horizon = new_h
  +            episodes_hint = max(1, horizon // per_ep_util)
  +            # Debug: mostra hint e distribuzione per-env
  +            eps_per_env_dbg = max(1, (episodes_hint + int(num_envs) - 1) // int(num_envs))
  +            total_eps_dbg = eps_per_env_dbg * int(num_envs)
  +            tqdm.write(f"[episodes] it={it+1} horizon={horizon} num_envs={num_envs} episodes_hint={episodes_hint} "
  +                        f"episodes_per_env={eps_per_env_dbg} total_env_episodes={total_eps_dbg}")
  +            # Abilita/disabilita MCTS in parallelo in base al flag di training
  +            parallel_use_mcts = bool(mcts_train and mcts_train_factor > 0.0)
  +            if dual_team_nets and (not opp_frozen_env):
  +                # Collect for team A vs live team B
  +                _t_c0 = time.time() if _PAR_TIMING else 0.0
  +                batch_A = collect_trajectory_parallel(agent,
  +                                                       num_envs=int(num_envs),
  +                                                       episodes_total_hint=episodes_hint,
  +                                                       k_history=k_history,
  +                                                       gamma=1.0,
  +                                                       lam=0.95,
  +                                                       use_mcts=parallel_use_mcts,
  +                                                       train_both_teams=False,
  +                                                       main_seats=main_seats,
  +                                                       mcts_sims=mcts_sims,
  +                                                       mcts_dets=mcts_dets,
  +                                                       mcts_c_puct=mcts_c_puct,
  +                                                       mcts_root_temp=mcts_root_temp,
  +                                                       mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                                       mcts_dirichlet_alpha=mcts_dirichlet_alpha,
  +                                                       mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                                       mcts_train_factor=mcts_train_factor,
  +                                                       seed=int(seed),
  +                                                       show_progress_env=True,
  +                                                       tqdm_base_pos=3)
  +                # Collect for team B vs live team A (swap seats)
  +                main_seats_B = odd_main_seats if (main_seats == even_main_seats) else even_main_seats
  +                batch_B = collect_trajectory_parallel(agent_teamB,
  +                                                       num_envs=int(num_envs),
  +                                                       episodes_total_hint=episodes_hint,
  +                                                       k_history=k_history,
  +                                                       gamma=1.0,
  +                                                       lam=0.95,
  +                                                       use_mcts=parallel_use_mcts,
  +                                                       train_both_teams=False,
  +                                                       main_seats=main_seats_B,
  +                                                       mcts_sims=mcts_sims,
  +                                                       mcts_dets=mcts_dets,
  +                                                       mcts_c_puct=mcts_c_puct,
  +                                                       mcts_root_temp=mcts_root_temp,
  +                                                       mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                                       mcts_dirichlet_alpha=mcts_dirichlet_alpha,
  +                                                       mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                                       mcts_train_factor=mcts_train_factor,
  +                                                       seed=int(seed + 1),
  +                                                       show_progress_env=True,
  +                                                       tqdm_base_pos=4)
  +                if _PAR_TIMING:
  +                    _iter_t_collect = (time.time() - _t_c0)
  +            elif dual_team_nets and opp_frozen_env:
  +                # Alternate training: one team learns while the other team acts frozen (no update)
  +                _t_c0 = time.time() if _PAR_TIMING else 0.0
  +                if train_A_now:
  +                    batch_A = collect_trajectory_parallel(agent,
  +                                                           num_envs=int(num_envs),
  +                                                           episodes_total_hint=episodes_hint,
  +                                                           k_history=k_history,
  +                                                           gamma=1.0,
  +                                                           lam=0.95,
  +                                                           use_mcts=parallel_use_mcts,
  +                                                           train_both_teams=False,
  +                                                           main_seats=main_seats,
  +                                                           mcts_sims=mcts_sims,
  +                                                           mcts_dets=mcts_dets,
  +                                                           mcts_c_puct=mcts_c_puct,
  +                                                           mcts_root_temp=mcts_root_temp,
  +                                                           mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                                           mcts_dirichlet_alpha=mcts_dirichlet_alpha,
  +                                                           mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                                           mcts_train_factor=mcts_train_factor,
  +                                                           seed=int(seed),
  +                                                           show_progress_env=True,
  +                                                           tqdm_base_pos=3,
  +                                                           frozen_actor=agent_teamB.actor,
  +                                                           frozen_non_main=True)
  +                else:
  +                    batch_B = collect_trajectory_parallel(agent_teamB,
  +                                                           num_envs=int(num_envs),
  +                                                           episodes_total_hint=episodes_hint,
  +                                                           k_history=k_history,
  +                                                           gamma=1.0,
  +                                                           lam=0.95,
  +                                                           use_mcts=parallel_use_mcts,
  +                                                           train_both_teams=False,
  +                                                           main_seats=main_seats_B,
  +                                                           mcts_sims=mcts_sims,
  +                                                           mcts_dets=mcts_dets,
  +                                                           mcts_c_puct=mcts_c_puct,
  +                                                           mcts_root_temp=mcts_root_temp,
  +                                                           mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                                           mcts_dirichlet_alpha=mcts_dirichlet_alpha,
  +                                                           mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                                           mcts_train_factor=mcts_train_factor,
  +                                                           seed=int(seed + 1),
  +                                                           show_progress_env=True,
  +                                                           tqdm_base_pos=4,
  +                                                           frozen_actor=agent.actor,
  +                                                           frozen_non_main=True)
  +                if _PAR_TIMING:
  +                    _iter_t_collect = (time.time() - _t_c0)
  +            else:
  +                _t_c0 = time.time() if _PAR_TIMING else 0.0
  +                # Refresh shadow actor on schedule in selfplay+frozen
  +                # SCOPONE_FROZEN_UPDATE_EVERY: in selfplay+frozen, how often to refresh the
  +                #   "shadow" opponent from the current live net. The shadow remains fixed
  +                #   between refreshes to create a stationary opponent for the non-main seats.
  +                if use_selfplay and opp_frozen_env:
  +                    if (shadow_actor is None) or ((it % shadow_every) == 0):
  +                        # clone lightweight copy of actor state
  +                        shadow_actor = ActionConditionedActor(obs_dim=obs_dim)
  +                        shadow_actor.load_state_dict(agent.actor.state_dict())
  +                batch = collect_trajectory_parallel(agent,
  +                                                    num_envs=int(num_envs),
  +                                                    episodes_total_hint=episodes_hint,
  +                                                    k_history=k_history,
  +                                                    gamma=1.0,
  +                                                    lam=0.95,
  +                                                    use_mcts=parallel_use_mcts,
  +                                                    train_both_teams=train_both_teams,
  +                                                    main_seats=main_seats,
  +                                                    mcts_sims=mcts_sims,
  +                                                    mcts_dets=mcts_dets,
  +                                                    mcts_c_puct=mcts_c_puct,
  +                                                    mcts_root_temp=mcts_root_temp,
  +                                                    mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                                    mcts_dirichlet_alpha=mcts_dirichlet_alpha,
  +                                                    mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                                    mcts_train_factor=mcts_train_factor,
  +                                                    seed=int(seed),
  +                                                    show_progress_env=True,
  +                                                    tqdm_base_pos=3,
  +                                                    frozen_actor=(shadow_actor if (use_selfplay and opp_frozen_env) else None),
  +                                                    frozen_non_main=bool(use_selfplay and opp_frozen_env))
  +                if _PAR_TIMING:
  +                    _iter_t_collect = (time.time() - _t_c0)
  +        else:
  +            # Strategia MCTS: warmup senza MCTS per le prime iterazioni, poi scala con il progresso mano
  +            if dual_team_nets and (not opp_frozen_env):
  +                _t_c0 = time.time() if _PAR_TIMING else 0.0
  +                # Team A as main vs live team B
  +                batch_A = collect_trajectory(env, agent, horizon=horizon, partner_actor=agent_teamB.actor, opponent_actor=agent_teamB.actor, main_seats=main_seats,
  +                                                 belief_particles=belief_particles, belief_ess_frac=belief_ess_frac,
  +                                                 episodes=None, final_reward_only=True,
  +                                                 use_mcts=bool(mcts_train),
  +                                                 mcts_sims=mcts_sims, mcts_dets=mcts_dets, mcts_c_puct=mcts_c_puct,
  +                                                 mcts_root_temp=mcts_root_temp, mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                                 mcts_dirichlet_alpha=mcts_dirichlet_alpha, mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                                 mcts_train_factor=mcts_train_factor,
  +                                                 mcts_progress_start=0.25, mcts_progress_full=0.75,
  +                                                 mcts_min_sims=0,
  +                                                 train_both_teams=False,
  +                                                 gamma=1.0,
  +                                                 lam=0.95,
  +                                                 seed=int(seed))
  +                # Team B as main vs live team A (swap seats)
  +                main_seats_B = odd_main_seats if (main_seats == even_main_seats) else even_main_seats
  +                batch_B = collect_trajectory(env, agent_teamB, horizon=horizon, partner_actor=agent.actor, opponent_actor=agent.actor, main_seats=main_seats_B,
  +                                                 belief_particles=belief_particles, belief_ess_frac=belief_ess_frac,
  +                                                 episodes=None, final_reward_only=True,
  +                                                 use_mcts=bool(mcts_train),
  +                                                 mcts_sims=mcts_sims, mcts_dets=mcts_dets, mcts_c_puct=mcts_c_puct,
  +                                                 mcts_root_temp=mcts_root_temp, mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                                 mcts_dirichlet_alpha=mcts_dirichlet_alpha, mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                                 mcts_train_factor=mcts_train_factor,
  +                                                 mcts_progress_start=0.25, mcts_progress_full=0.75,
  +                                                 mcts_min_sims=0,
  +                                                 train_both_teams=False,
  +                                                 gamma=1.0,
  +                                                 lam=0.95,
  +                                                 seed=int(seed + 1))
  +                if _PAR_TIMING:
  +                    _iter_t_collect = (time.time() - _t_c0)
  +            elif dual_team_nets and opp_frozen_env:
  +                _t_c0 = time.time() if _PAR_TIMING else 0.0
  +                if train_A_now:
  +                    # Team A learns vs frozen Team B
  +                    batch_A = collect_trajectory(env, agent, horizon=horizon, partner_actor=agent_teamB.actor, opponent_actor=agent_teamB.actor, main_seats=main_seats,
  +                                                 belief_particles=belief_particles, belief_ess_frac=belief_ess_frac,
  +                                                 episodes=None, final_reward_only=True,
  +                                                 use_mcts=bool(mcts_train),
  +                                                 mcts_sims=mcts_sims, mcts_dets=mcts_dets, mcts_c_puct=mcts_c_puct,
  +                                                 mcts_root_temp=mcts_root_temp, mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                                 mcts_dirichlet_alpha=mcts_dirichlet_alpha, mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                                 mcts_train_factor=mcts_train_factor,
  +                                                 mcts_progress_start=0.25, mcts_progress_full=0.75,
  +                                                 mcts_min_sims=0,
  +                                                 train_both_teams=False,
  +                                                 gamma=1.0,
  +                                                 lam=0.95,
  +                                                 seed=int(seed))
  +                else:
  +                    # Team B learns vs frozen Team A
  +                    batch_B = collect_trajectory(env, agent_teamB, horizon=horizon, partner_actor=agent.actor, opponent_actor=agent.actor, main_seats=main_seats,
  +                                                 belief_particles=belief_particles, belief_ess_frac=belief_ess_frac,
  +                                                 episodes=None, final_reward_only=True,
  +                                                 use_mcts=bool(mcts_train),
  +                                                 mcts_sims=mcts_sims, mcts_dets=mcts_dets, mcts_c_puct=mcts_c_puct,
  +                                                 mcts_root_temp=mcts_root_temp, mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                                 mcts_dirichlet_alpha=mcts_dirichlet_alpha, mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                                 mcts_train_factor=mcts_train_factor,
  +                                                 mcts_progress_start=0.25, mcts_progress_full=0.75,
  +                                                 mcts_min_sims=0,
  +                                                 train_both_teams=False,
  +                                                 gamma=1.0,
  +                                                 lam=0.95,
  +                                                 seed=int(seed + 1))
  +                if _PAR_TIMING:
  +                    _iter_t_collect = (time.time() - _t_c0)
  +            else:
  +                _t_c0 = time.time() if _PAR_TIMING else 0.0
  +                batch = collect_trajectory(env, agent, horizon=horizon, partner_actor=partner_actor, opponent_actor=opponent_actor, main_seats=main_seats,
  +                                           belief_particles=belief_particles, belief_ess_frac=belief_ess_frac,
  +                                           episodes=None, final_reward_only=True,
  +                                           use_mcts=bool(mcts_train),
  +                                           mcts_sims=mcts_sims, mcts_dets=mcts_dets, mcts_c_puct=mcts_c_puct,
  +                                           mcts_root_temp=mcts_root_temp, mcts_prior_smooth_eps=mcts_prior_smooth_eps,
  +                                           mcts_dirichlet_alpha=mcts_dirichlet_alpha, mcts_dirichlet_eps=mcts_dirichlet_eps,
  +                                           mcts_train_factor=mcts_train_factor,
  +                                           mcts_progress_start=0.25, mcts_progress_full=0.75,
  +                                           mcts_min_sims=0,
  +                                           train_both_teams=train_both_teams,
  +                                           gamma=1.0,
  +                                           lam=0.95,
  +                                           seed=int(seed))
  +                if _PAR_TIMING:
  +                    _iter_t_collect = (time.time() - _t_c0)
  +        if dual_team_nets and (not opp_frozen_env):
  +            if len(batch_A['obs']) == 0 or len(batch_B['obs']) == 0:
  +                continue
  +        elif dual_team_nets and opp_frozen_env:
  +            if train_A_now:
  +                if len(batch_A['obs']) == 0:
  +                    continue
  +            else:
  +                if len(batch_B['obs']) == 0:
  +                    continue
  +        else:
  +            if len(batch['obs']) == 0:
  +                continue
  +        # normalizza vantaggi completamente su GPU (no sync)
  +        _t_p0 = time.time() if _PAR_TIMING else 0.0
  +        if dual_team_nets and (not opp_frozen_env):
  +            for label, _b in (('dual-teamA', batch_A), ('dual-teamB', batch_B)):
  +                adv = _b['adv']
  +                if adv.numel() > 0:
  +                    _b['adv'] = _normalize_adv_tensor(label, adv)
  +        elif dual_team_nets and opp_frozen_env:
  +            if train_A_now:
  +                adv = batch_A['adv']
  +                if adv.numel() > 0:
  +                    batch_A['adv'] = _normalize_adv_tensor('frozen-teamA', adv)
  +            else:
  +                adv = batch_B['adv']
  +                if adv.numel() > 0:
  +                    batch_B['adv'] = _normalize_adv_tensor('frozen-teamB', adv)
  +        else:
  +            adv = batch['adv']
  +            if adv.numel() > 0:
  +                batch['adv'] = _normalize_adv_tensor('single', adv)
  +        # Ensure training mode on update
  +        agent.actor.train()
  +        agent.critic.train()
  +        if dual_team_nets and (not opp_frozen_env):
  +            agent_teamB.actor.train(); agent_teamB.critic.train()
  +        # Pre-filter: require all rows to have at least one legal action
  +        def _ensure_legals(_b):
  +            lc = _b['legals_count']
  +            keep = (lc > 0)
  +            if bool((~keep).any().item()):
  +                num_drop = int((~keep).sum().item()); total = int(lc.size(0))
  +                bad_idx = torch.nonzero(~keep, as_tuple=False).flatten().tolist()[:10]
  +                raise RuntimeError(f"train_ppo: refusing to drop transitions with zero legals (would drop {num_drop}/{total}); examples idx={bad_idx}")
  +        if dual_team_nets and (not opp_frozen_env):
  +            _ensure_legals(batch_A); _ensure_legals(batch_B)
  +        elif dual_team_nets and opp_frozen_env:
  +            if train_A_now:
  +                _ensure_legals(batch_A)
  +            else:
  +                _ensure_legals(batch_B)
  +        else:
  +            _ensure_legals(batch)
  +        # Enforce B multiple of minibatch_size by dropping tail
  +        def _ensure_mb(_b):
  +            B_now = int(_b['obs'].size(0)) if 'obs' in _b and torch.is_tensor(_b['obs']) else 0
  +            mb = minibatch_size
  +            drop = (B_now % mb)
  +            if B_now > 0 and drop > 0:
  +                raise RuntimeError(f"train_ppo: batch size {B_now} is not a multiple of minibatch_size={mb}; adjust horizon/collection to avoid dropping {drop} transitions")
  +        if dual_team_nets and (not opp_frozen_env):
  +            _ensure_mb(batch_A); _ensure_mb(batch_B)
  +        elif dual_team_nets and opp_frozen_env:
  +            if train_A_now:
  +                _ensure_mb(batch_A)
  +            else:
  +                _ensure_mb(batch_B)
  +        else:
  +            _ensure_mb(batch)
  +        if _PAR_TIMING:
  +            _iter_t_preproc = (time.time() - _t_p0)
  +        # Aumenta minibatch_size approfittando della VRAM ampia
  +        if dual_team_nets and (not opp_frozen_env):
  +            _t_u0 = time.time() if _PAR_TIMING else 0.0
  +            info_A = agent.update(batch_A, epochs=4, minibatch_size=minibatch_size)
  +            info_B = agent_teamB.update(batch_B, epochs=4, minibatch_size=minibatch_size)
  +            if _PAR_TIMING:
  +                _iter_t_update = (time.time() - _t_u0)
  +        elif dual_team_nets and opp_frozen_env:
  +            _t_u0 = time.time() if _PAR_TIMING else 0.0
  +            if train_A_now:
  +                info_A = agent.update(batch_A, epochs=4, minibatch_size=minibatch_size)
  +            else:
  +                info_B = agent_teamB.update(batch_B, epochs=4, minibatch_size=minibatch_size)
  +            if _PAR_TIMING:
  +                _iter_t_update = (time.time() - _t_u0)
  +        else:
  +            _t_u0 = time.time() if _PAR_TIMING else 0.0
  +            info = agent.update(batch, epochs=4, minibatch_size=minibatch_size)
  +            if _PAR_TIMING:
  +                _iter_t_update = (time.time() - _t_u0)
  +        dt = time.time() - t0
  +
  +        # proxy per best: media return del batch
  +        # All device tensors; compute small stats without moving large arrays
  +        if dual_team_nets and (not opp_frozen_env):
  +            avg_return_A = float(batch_A['ret'].mean().detach().cpu().item()) if len(batch_A['ret']) else 0.0
  +            avg_return_B = float(batch_B['ret'].mean().detach().cpu().item()) if len(batch_B['ret']) else 0.0
  +            avg_return = avg_return_A
  +            if avg_return_A > best_return:
  +                best_return = avg_return_A
  +                os.makedirs(os.path.dirname(best_ckpt_path), exist_ok=True)
  +                agent.save(best_ckpt_path.replace('.pth', '_teamA.pth'))
  +            # Save best for team B separately
  +            if avg_return_B > (globals().get('_best_return_B', -1e9)):
  +                _best_return_B = avg_return_B
  +                os.makedirs(os.path.dirname(best_ckpt_path), exist_ok=True)
  +                agent_teamB.save(best_ckpt_path.replace('.pth', '_teamB.pth'))
  +        elif dual_team_nets and opp_frozen_env:
  +            if train_A_now:
  +                avg_return_A = float(batch_A['ret'].mean().detach().cpu().item()) if len(batch_A['ret']) else 0.0
  +                avg_return = avg_return_A
  +                if avg_return_A > best_return:
  +                    best_return = avg_return_A
  +                    os.makedirs(os.path.dirname(best_ckpt_path), exist_ok=True)
  +                    agent.save(best_ckpt_path.replace('.pth', '_teamA.pth'))
  +            else:
  +                avg_return_B = float(batch_B['ret'].mean().detach().cpu().item()) if len(batch_B['ret']) else 0.0
  +                avg_return = avg_return_B
  +                if avg_return_B > (globals().get('_best_return_B', -1e9)):
  +                    _best_return_B = avg_return_B
  +                    os.makedirs(os.path.dirname(best_ckpt_path), exist_ok=True)
  +                    agent_teamB.save(best_ckpt_path.replace('.pth', '_teamB.pth'))
  +        else:
  +            if len(batch['ret']):
  +                avg_return = float(batch['ret'].mean().detach().cpu().item())
  +            else:
  +                avg_return = 0.0
  +            if avg_return > best_return:
  +                best_return = avg_return
  +                os.makedirs(os.path.dirname(best_ckpt_path), exist_ok=True)
  +                agent.save(best_ckpt_path)
  +
  +        # mini-eval periodica e Elo update
  +        if eval_every and (it + 1) % eval_every == 0 and len(league.history) >= 1:
  +            cur_tmp = ckpt_path.replace('.pth', f'_tmp_it{it+1}.pth')
  +            agent.save(cur_tmp)
  +            league.register(cur_tmp)
  +            if dual_team_nets and (not opp_frozen_env):
  +                cur_tmp_B = ckpt_path.replace('.pth', f'_tmpB_it{it+1}.pth')
  +                agent_teamB.save(cur_tmp_B)
  +                league.register(cur_tmp_B)
  +            prev_ckpt = league.history[-2] if len(league.history) >= 2 else None
  +            if prev_ckpt is not None:
  +                mcts_cfg = None
  +                if mcts_in_eval:
  +                    mcts_cfg = {
  +                        'sims': int(mcts_sims_eval if mcts_sims_eval is not None else mcts_sims),
  +                        'dets': mcts_dets,
  +                        'c_puct': mcts_c_puct,
  +                        'root_temp': mcts_root_temp,
  +                        'prior_smooth_eps': mcts_prior_smooth_eps,
  +                        'root_dirichlet_alpha': mcts_dirichlet_alpha,
  +                        'root_dirichlet_eps': mcts_dirichlet_eps,
  +                        'robust_child': True,
  +                    }
  +                # Ensure deterministic MCTS during evaluation/post-game analysis
  +                mcts_eval_seed = int(os.environ.get('MCTS_EVAL_SEED', resolve_seed(0)))
  +                with temporary_seed(mcts_eval_seed):
  +                    diff, bd = evaluate_pair_actors(cur_tmp, prev_ckpt, games=eval_games, k_history=k_history,
  +                                                 mcts=_make_mcts_cfg_for_eval(),
  +                                                 belief_particles=(belief_particles if mcts_in_eval else 0),
  +                                                 belief_ess_frac=belief_ess_frac,
  +                                                 tqdm_desc=f"Mini-eval: it{it+1}",
  +                                                 tqdm_position=2)
  +                league.update_elo_from_diff(cur_tmp, prev_ckpt, diff)
  +                if writer is not None:
  +                    wr = float((bd.get('meta') or {}).get('win_rate', 0.0))
  +                    writer.add_scalar('league/mini_eval_diff', diff, it)
  +                    writer.add_scalar('league/mini_eval_wr', wr, it)
  +                    writer.add_scalar('league/elo_current', league.elo.get(cur_tmp, 1000.0), it)
  +                    writer.add_scalar('league/elo_previous', league.elo.get(prev_ckpt, 1000.0), it)
  +                # salva checkpoint best wr con soglia/CI (Wilson interval)
  +                import math
  +                n = max(1, eval_games)
  +                p = wr
  +                z = 1.96
  +                denom = 1 + z*z/n
  +                center = p + z*z/(2*n)
  +                margin = z*math.sqrt((p*(1-p)/n) + (z*z/(4*n*n)))
  +                lower = (center - margin) / denom
  +                improved = wr > best_wr
  +                meets_threshold = lower >= 0.5  # lower bound sopra il 50%
  +                if improved or meets_threshold:
  +                    best_wr = max(best_wr, wr)
  +                    os.makedirs(os.path.dirname(best_wr_ckpt_path), exist_ok=True)
  +                    agent.save(best_wr_ckpt_path)
  +                if wr > best_wr:
  +                    best_wr = wr
  +                    os.makedirs(os.path.dirname(best_wr_ckpt_path), exist_ok=True)
  +                    agent.save(best_wr_ckpt_path)
  +
  +        # Aggiorna output terminale ad ogni iterazione
  +        def _to_float(x):
  +            return float(x.detach().cpu().item()) if torch.is_tensor(x) else float(x)
  +        preview_keys = ['loss_pi', 'loss_v', 'approx_kl', 'clip_frac', 'avg_clip_frac', 'avg_kl']
  +        if dual_team_nets and (not opp_frozen_env):
  +            preview_A = {k: round(_to_float(info_A[k]), 4) for k in preview_keys if k in info_A}
  +            preview_A.update({'avg_ret': round(avg_return_A, 4)})
  +            preview_B = {k: round(_to_float(info_B[k]), 4) for k in preview_keys if k in info_B}
  +            preview_B.update({'avg_ret_B': round(avg_return_B, 4)})
  +            preview = {**{f"A_{k}": v for k, v in preview_A.items()}, **{f"B_{k}": v for k, v in preview_B.items()}, 't_s': round(dt, 2)}
  +        elif dual_team_nets and opp_frozen_env:
  +            if train_A_now:
  +                preview_A = {k: round(_to_float(info_A[k]), 4) for k in preview_keys if k in info_A}
  +                preview_A.update({'avg_ret': round(avg_return, 4)})
  +                preview = {**{f"A_{k}": v for k, v in preview_A.items()}, 't_s': round(dt, 2)}
  +            else:
  +                preview_B = {k: round(_to_float(info_B[k]), 4) for k in preview_keys if k in info_B}
  +                preview_B.update({'avg_ret_B': round(avg_return, 4)})
  +                preview = {**{f"B_{k}": v for k, v in preview_B.items()}, 't_s': round(dt, 2)}
  +        else:
  +            preview = {k: round(_to_float(info[k]), 4) for k in preview_keys if k in info}
  +            preview.update({'avg_ret': round(avg_return, 4), 't_s': round(dt, 2)})
  +        metrics_body = " ".join([f"{k}:{v}" for k, v in preview.items()]) if preview else ""
  +        metrics_prefix = f"it {it + 1}/{num_iterations}"
  +        metrics_bar.set_description_str(
  +            f"{metrics_prefix} | {metrics_body}" if metrics_body else metrics_prefix,
  +            refresh=False
  +        )
  +        # Iteration-level timing print (sums to dt)
  +        if _PAR_TIMING:
  +            _post = max(0.0, dt - (_iter_t_collect + _iter_t_preproc + _iter_t_update))
  +            tqdm.write(f"[iter-timing] total={dt:.3f}s collect={_iter_t_collect:.3f} preproc={_iter_t_preproc:.3f} update={_iter_t_update:.3f} post={_post:.3f}")
  +
  +        if writer is not None:
  +            def _to_float(x):
  +                return float(x.detach().cpu().item()) if torch.is_tensor(x) else float(x)
  +            if dual_team_nets and (not opp_frozen_env):
  +                for k, v in info_A.items():
  +                    writer.add_scalar(f'train/A_{k}', _to_float(v), it)
  +                for k, v in info_B.items():
  +                    writer.add_scalar(f'train/B_{k}', _to_float(v), it)
  +                writer.add_scalar('train/A_episode_time_s', dt, it)
  +                writer.add_scalar('train/A_avg_return', avg_return_A, it)
  +                writer.add_scalar('train/B_avg_return', avg_return_B, it)
  +            elif dual_team_nets and opp_frozen_env:
  +                if train_A_now:
  +                    for k, v in info_A.items():
  +                        writer.add_scalar(f'train/A_{k}', _to_float(v), it)
  +                    writer.add_scalar('train/A_episode_time_s', dt, it)
  +                    writer.add_scalar('train/A_avg_return', avg_return, it)
  +                else:
  +                    for k, v in info_B.items():
  +                        writer.add_scalar(f'train/B_{k}', _to_float(v), it)
  +                    writer.add_scalar('train/B_episode_time_s', dt, it)
  +                    writer.add_scalar('train/B_avg_return', avg_return, it)
  +            else:
  +                for k, v in info.items():
  +                    writer.add_scalar(f'train/{k}', _to_float(v), it)
  +                writer.add_scalar('train/episode_time_s', dt, it)
  +                writer.add_scalar('train/avg_return', avg_return, it)
  +            writer.add_text('train/main_seats', str(main_seats), it)
  +            # Log by seat completamente su GPU; singola sync per tutto il blocco
  +            if dual_team_nets and opp_frozen_env:
  +                _b_used = (batch_A if train_A_now else batch_B)
  +            elif dual_team_nets and (not opp_frozen_env):
  +                _b_used = batch_A  # primary reference
  +            else:
  +                _b_used = batch
  +            seats_t = torch.argmax(_b_used['seat_team'][:, :4], dim=1)
  +            ret_t = _b_used['ret']
  +            mask_02_t = (seats_t == 0) | (seats_t == 2)
  +            mask_13_t = (seats_t == 1) | (seats_t == 3)
  +            cnt_02 = mask_02_t.float().sum()
  +            cnt_13 = mask_13_t.float().sum()
  +            sum_ret_02 = (ret_t * mask_02_t.float()).sum()
  +            sum_ret_13 = (ret_t * mask_13_t.float()).sum()
  +            mean_ret_02 = torch.where(cnt_02 > 0, sum_ret_02 / cnt_02, torch.zeros((), device=device, dtype=torch.float32))
  +            mean_ret_13 = torch.where(cnt_13 > 0, sum_ret_13 / cnt_13, torch.zeros((), device=device, dtype=torch.float32))
  +            diag = _compute_per_seat_diagnostics(agent, _b_used)
  +            # Prepara chiavi e valori da sincronizzare in una volta
  +            keys = [
  +                'by_seat/ret_02', 'by_seat/ret_13',
  +                'by_seat/kl_02', 'by_seat/kl_13',
  +                'by_seat/entropy_02', 'by_seat/entropy_13',
  +                'by_seat/clip_frac_02', 'by_seat/clip_frac_13'
  +            ]
  +            vals = [
  +                mean_ret_02.to(torch.float32),
  +                mean_ret_13.to(torch.float32),
  +                diag['by_seat/kl_02'].to(torch.float32),
  +                diag['by_seat/kl_13'].to(torch.float32),
  +                diag['by_seat/entropy_02'].to(torch.float32),
  +                diag['by_seat/entropy_13'].to(torch.float32),
  +                diag['by_seat/clip_frac_02'].to(torch.float32),
  +                diag['by_seat/clip_frac_13'].to(torch.float32),
  +            ]
  +            stacked = torch.stack(vals)
  +            numbers = stacked.detach().cpu().tolist()  # unica sincronizzazione CPU
  +            for key, num in zip(keys, numbers):
  +                writer.add_scalar(key, float(num), it)
  +        # Save periodically every 'save_every' iters. Additionally, save on the last
  +        # iteration to avoid runs with zero checkpoints.
  +        if ((it + 1) % save_every == 0) or ((it + 1) == num_iterations):
  +            os.makedirs(os.path.dirname(ckpt_path), exist_ok=True)
  +            # Timestamped filenames, with suffixes by mode
  +            from datetime import datetime as _dt
  +            ts = _dt.now().strftime('%Y%m%d_%H%M%S')
  +            if use_selfplay:
  +                out_A = ckpt_path.replace('.pth', f'_selfplay_{ts}.pth')
  +                agent.save(out_A)
  +                # Register only non-bootstrap
  +                league.register(out_A)
  +                # Elo evaluation: current vs best historical reference (top Elo)
  +                if len(league.history) >= 2:
  +                    # pick highest-Elo historical different from current
  +                    hist = [p for p in league.history if p != out_A]
  +                    if hist:
  +                        ref = max(hist, key=lambda p: float(league.elo.get(p, 1000.0)))
  +                        diff, _ = evaluate_pair_actors(out_A, ref, games=eval_games, k_history=k_history,
  +                                                    mcts=_make_mcts_cfg_for_eval(),
  +                                                    belief_particles=(belief_particles if mcts_in_eval else 0),
  +                                                    belief_ess_frac=belief_ess_frac,
  +                                                    tqdm_desc=f"Eval: A vs ref",
  +                                                    tqdm_position=2)
  +                        league.update_elo_from_diff(out_A, ref, diff)
  +            else:
  +                out_A = ckpt_path.replace('.pth', f'_teamA_{ts}.pth')
  +                agent.save(out_A)
  +                league.register(out_A)
  +                out_B = ckpt_path.replace('.pth', f'_teamB_{ts}.pth') if dual_team_nets else None
  +                if out_B:
  +                    agent_teamB.save(out_B)
  +                    league.register(out_B)
  +                # Elo evaluation: A vs B (if both) and vs league top1
  +                if out_B:
  +                    diffAB, bdAB = evaluate_pair_actors(out_A, out_B, games=eval_games, k_history=k_history,
  +                                                   mcts=_make_mcts_cfg_for_eval(),
  +                                                   belief_particles=(belief_particles if mcts_in_eval else 0),
  +                                                   belief_ess_frac=belief_ess_frac,
  +                                                   tqdm_desc=f"Eval: A vs B",
  +                                                   tqdm_position=2)
  +                    league.update_elo_from_diff(out_A, out_B, diffAB)
  +                # pick highest-Elo historical reference different from the new ones
  +                hist = [p for p in league.history if p not in ([out_A] + ([out_B] if out_B else []))]
  +                if hist:
  +                    ref = max(hist, key=lambda p: float(league.elo.get(p, 1000.0)))
  +                    diffA, bdA = evaluate_pair_actors(out_A, ref, games=eval_games, k_history=k_history,
  +                                                  mcts=_make_mcts_cfg_for_eval(),
  +                                                  belief_particles=(belief_particles if mcts_in_eval else 0),
  +                                                  belief_ess_frac=belief_ess_frac,
  +                                                  tqdm_desc=f"Eval: A vs ref",
  +                                                  tqdm_position=2)
  +                    league.update_elo_from_diff(out_A, ref, diffA)
  +                    if out_B:
  +                        diffB, bdB = evaluate_pair_actors(out_B, ref, games=eval_games, k_history=k_history,
  +                                                      mcts=_make_mcts_cfg_for_eval(),
  +                                                      belief_particles=(belief_particles if mcts_in_eval else 0),
  +                                                      belief_ess_frac=belief_ess_frac,
  +                                                      tqdm_desc=f"Eval: B vs ref",
  +                                                      tqdm_position=2)
  +                        league.update_elo_from_diff(out_B, ref, diffB)
  +            # After saving a real checkpoint, delete any bootstrap files present
  +            _bootstrap_path = os.path.join('checkpoints', 'bootstrap_random.pth')
  +            if os.path.isfile(_bootstrap_path):
  +                os.remove(_bootstrap_path)
  +                tqdm.write("[bootstrap] Removed bootstrap_random.pth after saving real checkpoint")
  +        # Optional profiler or external hook per-iteration
  +        if on_iter_end is not None:
  +            on_iter_end(it)
  +    if writer is not None:
  +        writer.close()
  +    metrics_bar.close()
  +    pbar.close()
  +
  +
  +if __name__ == '__main__':
  +    import argparse
  +    parser = argparse.ArgumentParser(description='Train PPO Action-Conditioned for Scopone')
  +    parser.add_argument('--iters', type=int, default=2000, help='Number of PPO iterations')
  +    parser.add_argument('--horizon', type=int, default=256, help='Rollout horizon (steps) per iteration (minimo 40); con solo reward finale raccoglie ~horizon//40 episodi')
  +    parser.add_argument('--save-every', type=int, default=200, help='Save checkpoint every N iterations')
  +    parser.add_argument('--ckpt', type=str, default='checkpoints/ppo_ac.pth', help='Checkpoint path')
  +    parser.add_argument('--compact', action='store_true', help='Use compact observation (recommended)')
  +    parser.add_argument('--k-history', type=int, default=39, help='Number of recent moves in compact history')
  +    parser.add_argument('--seed', type=int, default=0, help='Random seed')
  +    parser.add_argument('--entropy-schedule', type=str, default='linear', choices=['linear','cosine'], help='Entropy schedule type')
  +    parser.add_argument('--eval-every', type=int, default=0, help='Run mini-eval every N iters (0=off)')
  +    parser.add_argument('--eval-games', type=int, default=10, help='Games per mini-eval')
  +    parser.add_argument('--belief-particles', type=int, default=512, help='Belief particles for trainer')
  +    parser.add_argument('--belief-ess-frac', type=float, default=0.5, help='Belief ESS fraction for trainer')
  +    parser.add_argument('--mcts-eval', action='store_true', help='Use MCTS in mini-eval')
  +    parser.add_argument('--mcts-train', action='store_true', default=True, help='Use MCTS during training action selection for main seats')
  +    parser.add_argument('--train-both-teams', action='store_true', help='Train both teams simultaneously (all seats are main)')
  +    parser.add_argument('--mcts-sims', type=int, default=128)
  +    parser.add_argument('--mcts-sims-eval', type=int, default=None, help='Override eval MCTS sims (default: same as --mcts-sims)')
  +    parser.add_argument('--mcts-dets', type=int, default=4)
  +    parser.add_argument('--mcts-c-puct', type=float, default=1.0)
  +    parser.add_argument('--mcts-root-temp', type=float, default=0.0)
  +    parser.add_argument('--mcts-prior-smooth-eps', type=float, default=0.0)
  +    parser.add_argument('--mcts-dirichlet-alpha', type=float, default=0.25)
  +    parser.add_argument('--mcts-dirichlet-eps', type=float, default=0.25)
  +    parser.add_argument('--num-envs', type=int, default=32, help='Number of parallel env workers (>=1). 1 disables parallel mode')
  +    args = parser.parse_args()
  +    train_ppo(num_iterations=args.iters, horizon=max(40, int(args.horizon)), save_every=args.save_every, ckpt_path=args.ckpt,
  +              k_history=args.k_history, seed=args.seed,
  +              entropy_schedule_type=args.entropy_schedule, eval_every=args.eval_every, eval_games=args.eval_games,
  +              belief_particles=args.belief_particles, belief_ess_frac=args.belief_ess_frac,
  +              mcts_in_eval=args.mcts_eval, mcts_train=args.mcts_train, mcts_sims=args.mcts_sims, mcts_sims_eval=args.mcts_sims_eval, mcts_dets=args.mcts_dets, mcts_c_puct=args.mcts_c_puct,
  +              mcts_root_temp=args.mcts_root_temp, mcts_prior_smooth_eps=args.mcts_prior_smooth_eps,
  +              mcts_dirichlet_alpha=args.mcts_dirichlet_alpha, mcts_dirichlet_eps=args.mcts_dirichlet_eps,
  +              num_envs=args.num_envs)
